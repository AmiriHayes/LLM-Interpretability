{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6443588e",
      "metadata": {
        "id": "6443588e"
      },
      "source": [
        "Author: Amiri Hayes \\\n",
        "Date Updated: 7/15/25 \\\n",
        "Title: ViewLLM\n",
        "\n",
        "Starter code !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99",
      "metadata": {
        "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99"
      },
      "outputs": [],
      "source": [
        "# package installs if necessary\n",
        "!pip install matplotlib torch spacy nltk tqdm transformers datasets scikit-learn\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3e36de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3e36de",
        "outputId": "5c233452-f8b9-4137-f8b9-cd2881649f94"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import Optional, Tuple, Callable\n",
        "from matplotlib.colors import PowerNorm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
        "print(\"Imports completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012daf28",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Data & Pattern Initialization Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbe22fd",
      "metadata": {
        "id": "7cbe22fd"
      },
      "outputs": [],
      "source": [
        "# POSITIONAL FILTERING PATTERNS:\n",
        "\n",
        "def next_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i+1] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Next Head Attention Pattern\", out\n",
        "\n",
        "def previous_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i-1] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Previous Head Attention Pattern\", out\n",
        "\n",
        "def same_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Same Token Attention Pattern\", out\n",
        "\n",
        "def punctuation_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = tokenizer.convert_ids_to_tokens(toks.input_ids[0])\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    punctuation_indices = [i for i, tok in enumerate(words) if any(p in tok for p in punctuation_set)]\n",
        "    for i in range(len_seq):\n",
        "        future_punct = [j for j in punctuation_indices if j > i]\n",
        "        if future_punct:\n",
        "            for j in future_punct:\n",
        "                out[i, j] = 1.0\n",
        "            out[i] /= out[i].sum()\n",
        "        else:\n",
        "            out[i, i] = 1.0\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Punctuation Pattern\", out\n",
        "\n",
        "def repeated_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    input_ids = toks.input_ids[0].tolist()\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        token_id = input_ids[i]\n",
        "        for j in range(1, len_seq-1):\n",
        "            if input_ids[j] == token_id:\n",
        "                out[i, j] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Repitition Pattern\", out\n",
        "\n",
        "# LINGUISTIC ROLE ALIGNMENT PATTERNS:\n",
        "\n",
        "def pos_alignment(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True, add_special_tokens=True)\n",
        "    input_ids = toks.input_ids[0].tolist()\n",
        "    word_ids = toks.word_ids(0)\n",
        "    len_seq = len(input_ids)\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    doc = nlp(sentence)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    pos_to_token_indices = {}\n",
        "    for token_idx, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None or word_idx >= len(pos_tags):\n",
        "            continue\n",
        "        pos = pos_tags[word_idx]\n",
        "        pos_to_token_indices.setdefault(pos, []).append(token_idx)\n",
        "    for token_indices in pos_to_token_indices.values():\n",
        "        if len(token_indices) > 1:\n",
        "            for i in token_indices:\n",
        "                for j in token_indices:\n",
        "                    out[i, j] = 1\n",
        "        else:\n",
        "            i = token_indices[0]\n",
        "            out[i, i] = 1\n",
        "    out[0, 0] = 1\n",
        "    out[-1, -1] = 1\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Part of Speech Pattern\", out\n",
        "\n",
        "def dependencies(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = sentence.split()\n",
        "    doc = nlp(\" \".join(words))\n",
        "    check_errors = False\n",
        "    if check_errors:\n",
        "        if len(doc) == 0: print(\"problem, doc empty\")\n",
        "        if len(doc) != (len_seq-2): print(\"problem, doc length mismatch\", len(doc), len(toks)-2)\n",
        "    for stok in doc:\n",
        "        parent_index = stok.i\n",
        "        for child_stok in stok.children:\n",
        "            child_index = child_stok.i\n",
        "            out[parent_index+1, child_index+1] = 1\n",
        "            out[child_index+1, parent_index+1] = 1\n",
        "    out[0, 0] = 1\n",
        "    out[-1, 0] = 1\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Dependency Parsing Pattern\", out\n",
        "\n",
        "# SEMI-STRUCTURED EVALUATION PATTERN:\n",
        "\n",
        "def chainofthought_pattern(sentence: str, tokenizer: PreTrainedTokenizerBase, att: np.ndarray, hint: bool) -> Tuple[str, str, np.ndarray]:\n",
        "    out = []\n",
        "    output = False\n",
        "\n",
        "    prefix = \"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nuser\\n\\n\"\n",
        "    if hint == False:\n",
        "        i = sentence.find(\"assistant\")\n",
        "        prompt = sentence[:i].strip()\n",
        "        prompt = prompt[len(prefix):]\n",
        "    elif hint == True:\n",
        "        i = sentence.find(\" [ Note:\")\n",
        "        prompt = sentence[:i].strip()\n",
        "        prompt = prompt[len(prefix):]\n",
        "    len_toks = len(tokenizer([prompt], return_tensors=\"pt\").input_ids[0])\n",
        "    start_token_idx = len(tokenizer([prefix], return_tensors=\"pt\").input_ids[0])\n",
        "    prompt_matrix = att[start_token_idx:len_toks, start_token_idx:len_toks]\n",
        "    vector_1 = np.mean(prompt_matrix, axis=0)\n",
        "    if output: print(f\"Prompt shape: {prompt_matrix.shape}, vector shape: {vector_1.shape}\")\n",
        "\n",
        "    answer = str(sentence.split(\".\")[-2]).strip()\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    decoded_tokens = tokenizer.convert_ids_to_tokens(toks.input_ids[0].tolist())\n",
        "    period_indices = [i for i, token in enumerate(decoded_tokens) if '.' in token]\n",
        "    start_idx = 0\n",
        "    len_toks = len(tokenizer([sentence], return_tensors=\"pt\").input_ids[0])\n",
        "    if len(period_indices) >= 2:\n",
        "        second_to_last_period_idx = period_indices[-2]\n",
        "        start_idx = second_to_last_period_idx\n",
        "    answer_matrix = att[start_idx:len_toks, start_idx:len_toks]\n",
        "    vector_2 = np.mean(answer_matrix, axis=0)\n",
        "    max_token = 20\n",
        "    if len(vector_2) > max_token:\n",
        "        vector_2 = vector_2[-max_token:]\n",
        "    elif len(vector_2) < max_token:\n",
        "        padding_length = max_token - len(vector_2)\n",
        "        vector_2 = np.pad(vector_2, (0, padding_length), 'constant', constant_values=0)\n",
        "    if output: print(f\"Answer shape: {answer_matrix.shape}, vector shape: {vector_2.shape}\")\n",
        "\n",
        "    if output: print(f\"prompt: {prompt}\\nanswer: {answer}\\n\")\n",
        "    out = np.concatenate((vector_1, vector_2))\n",
        "    return prompt, answer, out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d51a2fa",
      "metadata": {
        "id": "9d51a2fa"
      },
      "outputs": [],
      "source": [
        "# GENERATE & VISUALIZE ATTENTION PATTERN SCORES\n",
        "\n",
        "def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    q = np.clip(q, 1e-12, 1.0)\n",
        "    p /= p.sum()\n",
        "    q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
        "\n",
        "def score_prediction(torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: Tuple[int, int], pattern: Callable, sentence_1: str, sentence_2: Optional[str] = None, distance=\"jsd\", output=False):\n",
        "    layer, head = head_loc\n",
        "    tokens = torch_tokenizer(sentence_1, return_tensors=\"pt\")\n",
        "\n",
        "    if torch_model.config.is_encoder_decoder:\n",
        "        decoder_input_ids = tokens[\"input_ids\"]\n",
        "        outputs = torch_model(input_ids=tokens[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "        att = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "    else:\n",
        "        if sentence_2 and pattern.__name__ == \"chainofthought_pattern\":\n",
        "            name = \"Chain of Thought Pattern\"\n",
        "            tokens_2 = torch_tokenizer(sentence_2, return_tensors=\"pt\")\n",
        "\n",
        "            att = torch_model(**tokens_2, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            pred_att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "            if output: print(\"RUNNING FIRST WITH NO HINT\")\n",
        "            question, answer, vector_att = chainofthought_pattern(sentence_1, torch_tokenizer, pred_att, hint=False)\n",
        "            if output: print(\"RUNNING AFTER WITH A HINT\")\n",
        "            question, answer, vector_pred_att = chainofthought_pattern(sentence_2, torch_tokenizer, att, hint=True)\n",
        "\n",
        "            att, pred_att = vector_att.copy(), vector_pred_att.copy()\n",
        "        else:\n",
        "            att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    if distance == \"raw\":\n",
        "        score = np.abs(att - pred_att).sum()\n",
        "    elif distance == \"jsd\":\n",
        "        jensonshannon_distances = []\n",
        "        for row_att, row_out in zip(att, pred_att):\n",
        "            jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "        score = np.mean(jensonshannon_distances)\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\":\n",
        "        score = np.sqrt(js_divergence(att, pred_att))\n",
        "\n",
        "    if output == \"cot\":\n",
        "        colors = \"inferno\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 9))\n",
        "        axes[0].plot(att, color=plt.get_cmap(colors)(0.6))\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        axes[1].plot(pred_att, color=plt.get_cmap(colors)(0.9))\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        bound_axes = False\n",
        "        for i in range(2):\n",
        "            axes[i].set_xlabel(\"Token Index\")\n",
        "            axes[i].set_ylabel(\"Attention Weight\")\n",
        "            axes[i].grid(True)\n",
        "            if bound_axes:\n",
        "                axes[i].set_ylim(0, 1)\n",
        "                axes[i].set_xlim(0, len(att) - 1)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        question_chart = question.replace(\".\", \".\\n\")\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nQuestion: \\\"{question_chart}\\n\\nAnswer: \\\"{answer}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "    \n",
        "    toks = torch_tokenizer([sentence_1], return_tensors=\"pt\")\n",
        "    token_ids = toks[\"input_ids\"][0]\n",
        "    tokens = torch_tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    if output == True:\n",
        "        colors=\"Greens\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        im1 = axes[0].imshow(att, cmap=colors, aspect='auto')\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        fig.colorbar(im1, ax=axes[0])\n",
        "        im2 = axes[1].imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        fig.colorbar(im2, ax=axes[1])\n",
        "        for i in range(2):\n",
        "            axes[i].set_xticks(range(len(tokens)))\n",
        "            axes[i].set_yticks(range(len(tokens)))\n",
        "            axes[i].set_xticklabels(tokens, rotation=90)\n",
        "            axes[i].set_yticklabels(tokens)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nSentence: \\\"{sentence_1}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"optimal\":\n",
        "        colors = \"Oranges\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        ax.set_axis_off()\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"actual\":\n",
        "        colors = \"Reds\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(att, cmap=colors, aspect='auto')\n",
        "        ax.set_title(\"Example Head Attention for Pattern\")\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n899pxRzSWRe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n899pxRzSWRe",
        "outputId": "470ad98d-f61e-4b36-bde9-e6c2bb80882d"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (1):\n",
        "\n",
        "file = 'data/small_text.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "sentences = []\n",
        "for paragraph in df['text']:\n",
        "    sentences.extend(sent_tokenize(paragraph))\n",
        "\n",
        "sentences = sentences[:10_000]\n",
        "print(\"Sentences from Tiny Stories Dataset:\")\n",
        "for sentence in sentences[20:30]:\n",
        "  print(f\"\\t{sentence}\")\n",
        "\n",
        "df_json = pd.read_json('data/generic_sentences.json')\n",
        "generic_sentences = df_json[0].tolist()\n",
        "print(\"\\nGeneric Sentences:\")\n",
        "for sentence in generic_sentences[:10]:\n",
        "  print(f\"\\t{sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0adb7458",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (2):\n",
        "\n",
        "math_data = pd.read_json('data/math_problems_results.jsonl', lines=True)\n",
        "\n",
        "filtered_results = math_data[\n",
        "    (math_data['consistency'] == \"False\") &\n",
        "    (math_data['evaluated_answer_nohint'] != \"DNF: llm did not finish\") &\n",
        "    (math_data['evaluated_answer_hint'] != \"DNF: llm did not finish\")\n",
        "]\n",
        "\n",
        "answers_nohint = filtered_results['answer_nohint'].tolist()\n",
        "answers_hint = filtered_results['answer_hint'].tolist()\n",
        "prefix = \"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nuser\\n\\n\"\n",
        "\n",
        "prompts = []\n",
        "for s1, s2 in zip(answers_nohint, answers_hint):\n",
        "    if s1.startswith(prefix): s1 = s1[len(prefix):]\n",
        "    if s2.startswith(prefix): s2 = s2[len(prefix):]\n",
        "\n",
        "    i_suffix_s1 = s1.find(\"assistant\")\n",
        "    if i_suffix_s1 != -1: s1 = s1[:i_suffix_s1].strip()\n",
        "\n",
        "    i_suffix_s2 = s2.find(\"assistant\")\n",
        "    if i_suffix_s2 != -1: s2 = s2[:i_suffix_s2].strip()\n",
        "\n",
        "    if s1 and s2: prompts.append((s1, s2))\n",
        "\n",
        "print(len(prompts), \"relevant prompts loaded from math problems dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1015c0ec",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "  Analysis Helper Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff49f93",
      "metadata": {
        "id": "1ff49f93",
        "outputId": "d8f00b5c-d395-4564-885c-d1f65e5c815d"
      },
      "outputs": [],
      "source": [
        "# ANALYZE PATTERN AT LAYER AND HEAD\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "sentence = \"I like apples and I like bananas. I like apples more though.\"\n",
        "\n",
        "models = [\"bert-base-uncased\", \"openai-community/gpt2\", \"google-t5/t5-small\", \"Meta-Llama-3.1-8B-Instruct\"]\n",
        "\n",
        "i = 0\n",
        "name = models[i]\n",
        "\n",
        "if i in [0,1,2]: # small models ( <400M Params )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    model = AutoModel.from_pretrained(name, output_attentions=True)\n",
        "    model.eval()\n",
        "\n",
        "elif i == 3: # big model & cot ( 8B Params, Load from Compute Node )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt_num = 0\n",
        "    sentence = prompts[prompt_num][0]  # Use the prompt's first sentence (no hint)\n",
        "    sentence_with_hint = prompts[prompt_num][1]  # Use prompt's second sentence (hint)\n",
        "\n",
        "layer, head = 2, 6\n",
        "score_prediction(model, tokenizer, (layer, head), previous_attention, sentence, distance=\"jsd\", output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd29726",
      "metadata": {
        "id": "9fd29726",
        "outputId": "feeb18a4-11de-4928-9582-fdf4b4c816df"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (1 SENTENCE)\n",
        "\n",
        "def visualize_full_model(sentence: str, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9, sentence_2: Optional[str] = None) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    num_heads = model.config.num_attention_heads\n",
        "    model_viz = np.zeros((num_layers, num_heads))\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_heads):\n",
        "            score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "            model_viz[i, j] = score\n",
        "    print(f\"Best Score: {min(map(min, model_viz)):.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=model_viz.min(), vmax=model_viz.max())\n",
        "    plt.imshow(model_viz, cmap='Greens_r', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_three = list(zip(*np.unravel_index(np.argsort(model_viz, axis=None)[:3], model_viz.shape)))\n",
        "    for tuple_val in top_three:\n",
        "        layer, head = tuple_val\n",
        "        score = model_viz[layer, head]\n",
        "        print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return model_viz, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "pattern_name = \"Repeated Attention Pattern\"\n",
        "underlined_name_unicode = \"\".join([char + '\\u0332' for char in pattern_name])\n",
        "visualize_full_model(sentence, model, tokenizer, repeated_attention, title=f\"Top Heads: {underlined_name_unicode} | {model_name}\\nSentence: \\\"{sentence}\\\"\\n\", bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G90Rgk6fVKIZ",
      "metadata": {
        "id": "G90Rgk6fVKIZ",
        "outputId": "56ce27bd-6ea5-4f3d-cbf7-f6dc1b69f015"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (MULTIPLE SENTENCES)\n",
        "\n",
        "def visualize_full_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    header = [\"Layer\", \"Head\", \"Score\"]\n",
        "    csv_file_name = \"scores.csv\"\n",
        "    file_exists = os.path.exists(csv_file_name)\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "    \n",
        "        average_score = np.zeros((num_layers, num_heads))\n",
        "        for sentence in sentences:\n",
        "            sentence_1 = sentence[0]  # first sentence (no hint)\n",
        "            sentence_2 = sentence[1]  # second sentence (hint)\n",
        "            model_score = np.zeros((num_layers, num_heads))\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    score = score_prediction(model, tokenizer, (i, j), chainofthought_pattern, sentence_1, sentence_2, distance=\"jsd\", output=False)\n",
        "                    writer.writerow([i, j, f\"{score:.2f}\"])\n",
        "                    print(f\"Layer {i}, Head {j} - Score: {score:.2f}\")\n",
        "                    model_score[i, j] = score\n",
        "            average_score += model_score\n",
        "        average_score /= len(sentences)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=average_score.min(), vmax=average_score.max())\n",
        "    plt.imshow(average_score, cmap='Reds', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\": \n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score.ravel())[::-1][:3], average_score.shape))) # highest scores\n",
        "    else:\n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score, axis=None)[3:], average_score.shape))) # lowest scores\n",
        "        top_three = np.sort(average_score)\n",
        "\n",
        "    for tuple_val in top_three:\n",
        "            layer, head = tuple_val\n",
        "            score = average_score[layer, head]\n",
        "            print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return average_score, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentences_zipped = list(zip(answers_nohint[:5], answers_hint[:5]))\n",
        "visualize_full_model(sentences_zipped, model, tokenizer, chainofthought_pattern, title=\"Top Heads: Chain-of_Thought Evaluation [AVERAGE]\\n\"+model_name, bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eTFYV1Gpzv",
      "metadata": {
        "id": "b8eTFYV1Gpzv"
      },
      "outputs": [],
      "source": [
        "# FILTER SENTENCES TO TARGET SPECIFIC PATTERNS\n",
        "\n",
        "def filterer(sentences: list[str], length_matters: bool=False, punctuation_matters: bool=False, duplicates: bool=False) -> list[str]:\n",
        "    filtered_sentences = []\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    for sentence in sentences:\n",
        "        passes_all_active_filters = True\n",
        "        if length_matters:\n",
        "            if len(sentence.split()) < 5:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if punctuation_matters:\n",
        "            punctuation_count = sum(1 for char in sentence if char in punctuation_set)\n",
        "            if punctuation_count < 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if duplicates:\n",
        "            set_l = len(set(sentence.split()))\n",
        "            sent_1 = len(sentence.split())\n",
        "            if set_l >= sent_1 - 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if passes_all_active_filters: filtered_sentences.append(sentence)\n",
        "    return filtered_sentences\n",
        "\n",
        "filtered_sentences = filterer(generic_sentences, length_matters = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mR56LNrRd37k",
      "metadata": {
        "id": "mR56LNrRd37k",
        "outputId": "4b42cff5-d843-439d-8409-d5c47e9b87a7"
      },
      "outputs": [],
      "source": [
        "# ANALYZE HEAD PATTERN ON SELECTED SENTENCES\n",
        "\n",
        "def visualize_highest_head(sentences: list[str], top_n:  int, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: tuple[int, int], pattern: Callable):\n",
        "    layer, head = head_loc\n",
        "    scores = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        score = score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, distance=\"jsd\", output=False)\n",
        "        scores.append(score)\n",
        "\n",
        "    indexed_scores = list(enumerate(scores))\n",
        "    sorted_scores = sorted(indexed_scores, key= lambda x: x[1])\n",
        "    top_scores = sorted_scores[:top_n]\n",
        "\n",
        "    for idx, score in top_scores:\n",
        "        print(f\"Sentence #{idx} Score: {score}\")\n",
        "        sentence = sentences[idx]\n",
        "        score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, output=True)\n",
        "    return 0\n",
        "\n",
        "layer, head = 7, 8\n",
        "visualize_highest_head(filtered_sentences, 3, model, tokenizer, (layer, head), punctuation_attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be980bdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS\n",
        "\n",
        "def classify_whole_model(sentences, torch_model, torch_tokenizer, patterns):\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "    activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "    \n",
        "    csv_file_name = \"data/best_fit.csv\"\n",
        "    file_exists = os.path.exists(csv_file_name)\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        for pattern in patterns:\n",
        "            print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "            avg_score = []\n",
        "            for idx, sentence in enumerate(sentences):\n",
        "                if idx % 20 == 0: print(f\"\\tProcessing sentence {idx}/{len(sentences)}\")\n",
        "                for i in range(num_layers):\n",
        "                    for j in range(num_heads):\n",
        "                        score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                        avg_score.append(score)\n",
        "                        \n",
        "            avg_score = np.mean(avg_score)\n",
        "            if avg_score > 0.5: continue\n",
        "            print(i, j, avg_score)\n",
        "            key = (i, j)\n",
        "\n",
        "            if key not in activations or avg_score < activations[key][1]:\n",
        "                activations[key] = (pattern.__name__, avg_score)\n",
        "            writer.writerow([i, j, pattern.__name__, avg_score])\n",
        "\n",
        "    return activations\n",
        "\n",
        "patterns = [next_attention] #, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "activations = classify_whole_model(generic_sentences, model, tokenizer, patterns)\n",
        "print(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8374c077",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANALYZE EFFECT OF LINEAR WEIGHTS ON ATTENTION ACTIVATION ACCURACY\n",
        "\n",
        "def generate_dataset(patterns: list[Callable], model: PreTrainedModel, tokenizer: PreTrainedTokenizerBase, sentences: list[str], layer_head: tuple[int, int]):\n",
        "    layer, head = layer_head\n",
        "    X_data, y_data = [], []\n",
        "    print(\"Generating dataset for Layer\", layer, \", Head\", head)\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            attn = outputs.attentions[layer][0, head]\n",
        "        X_i_list = []\n",
        "        for pattern in patterns:\n",
        "            _, X_i = pattern(sentence, tokenizer)\n",
        "            X_i = torch.tensor(X_i, dtype=torch.float32)\n",
        "            X_i_list.append(X_i)\n",
        "        X_data.append(X_i_list)\n",
        "        y_data.append(attn)\n",
        "\n",
        "    torch.save({'X': X_data, 'y': y_data}, \"data/attention_dataset.pt\")\n",
        "    print(\"Dataset generated and saved to 'data/attention_dataset.pt'.\")\n",
        "\n",
        "def train_linearregression() -> pd.DataFrame:\n",
        "    data = torch.load(\"data/attention_dataset.pt\")\n",
        "    X, y = data['X'], data['y']\n",
        "    X, y = data['X'], data['y']\n",
        "    output = []\n",
        "\n",
        "    for i, (xb, yb) in enumerate(zip(X, y)):\n",
        "        xb = torch.stack(xb)\n",
        "        X_flat = (xb.reshape(len(xb), -1).T).numpy()\n",
        "        y_flat = yb.flatten().numpy()\n",
        "        reg = LinearRegression().fit(X_flat, y_flat)\n",
        "        if i % 100 == 0: print(f\"Sentence #{i} - Coeffs: {[float(f\"{coef:.2f}\") for coef in reg.coef_]}, Intercept: {reg.intercept_:.2f}\")\n",
        "        output.append([reg.coef_.tolist(), float(reg.intercept_)])\n",
        "\n",
        "    output = pd.DataFrame(output, columns=[\"Coefficients\", \"Intercept\"]).to_csv(\"data/linear_regression_results.csv\", index=False)\n",
        "    return output\n",
        "\n",
        "head_loc = (3, 9)\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "generate_dataset(patterns, model, tokenizer, sentences, head_loc)\n",
        "output = train_linearregression()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fdf02a",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Filter Automation Efforts\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5510c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# AUTOMATICALLY GENERATE PATTERNS FOR A HEAD\n",
        "\n",
        "def generate_prompt(sentences, model, tokenizer, head_loc, top_k_ratio=0.1):\n",
        "    layer, head = head_loc\n",
        "    name = model.config.architectures[0]\n",
        "    data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model_name,\n",
        "        \"examples\": []\n",
        "    }\n",
        "    def scrape_head(att, tokens, ignore_special=True, top_k_ratio=0.1):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append({\n",
        "                f\"from_token_{i}\": tokens[i],\n",
        "                f\"to_token_{j}\": tokens[j],\n",
        "                \"weight\": float(score)\n",
        "            })\n",
        "        return top_activations\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=top_k_ratio)\n",
        "        item = {\n",
        "            \"sentence\": sentence,\n",
        "            \"attention\": top_activations\n",
        "        }\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "    data = json.dumps(data, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    Using the following pieces of data based on {len(sentences[0])} sentences, generate three\n",
        "    hypothesises about the linguistic role the following head is responsible for based on patterns\n",
        "    in the activations. These patterns can be simple or complex. Here are some examples: {data} Then, choose the most fitting hypothesis for the head responsibility using a few examples from the data.\n",
        "    Finally, using the linguistic hypothesis you determine, write a python function which takes in a sentence and tokenizer as parameters and\n",
        "    outputs the name of the pattern you hypothesize along with a 'predicted_matrix' (size: token_len * token_len),\n",
        "    which is the rule-encoded matrix that mirroring attention patterns you'd predict for any given sentence for\n",
        "    Layer {layer}, Head {head}. Feel free to use the capacbilities of proved libraries like spacey and nltk for describing linguistic concepts. Feel free to encode complex functions. Make sure you generalize your hypothesis pattern to any sentence. As examples:,\n",
        "    Layer 3, Head 9 has been found to be responsible for dependency parsing. It's predicted pseudocode would look like:\n",
        "    def dependencies(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = sentence.split()\n",
        "    # use spacey nlp to split word into doc dependency tree\n",
        "    # loop through each node in tree and assign directional attention\n",
        "    # to the matrix 'out' by adding one when there is an outgoing edge.\n",
        "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention\n",
        "    # Normalize out matrix by row (results in uniform attention) and return out\n",
        "    return 'Dependency Parsing Pattern', out\n",
        "    Here is another pseudocode example for one method to implement part-of-speech:\n",
        "    def pos_alignment(sentence, tokenizer):\n",
        "    # assign toks, input_ids, word_ids, len_seq, out, doc\n",
        "    # use spacey to get pos_tags for tokens in docs [token.pos_ for token in doc]\n",
        "    # for token in pos_tags:\n",
        "    # loop through pos_tags and increment out[i,j] when pos_tags match\n",
        "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention\n",
        "    # Normalize out matrix by row (results in uniform attention) and return out\n",
        "    # return 'Part of Speech Implementation 1', out\n",
        "    \"\"\"\n",
        "    return ' '.join(prompt.strip().split())\n",
        "\n",
        "pd = pd.read_csv('data/small_text.csv')\n",
        "model_name = models[0]\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "generate_prompt(generic_sentences, model, tokenizer, (7, 1), 0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62fdd9bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_idea(llm_idea):\n",
        "    fn_name = \"get llm idea name\"\n",
        "    llm_function = \"get llm function\"\n",
        "    return fn_name, llm_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c8a036",
      "metadata": {},
      "outputs": [],
      "source": [
        "def automation_pipeline(model, tokenizer, head_loc, sentences, evaluate=False):\n",
        "    layer, head = head_loc\n",
        "    prompt = generate_prompt(sentences, model, tokenizer, head_loc, 0.05)\n",
        "    print(\"Step 1... Generated Prompt\")\n",
        "    llm_idea = hypothesize_function(prompt)\n",
        "    print(\"Step 2... Coded Function\")\n",
        "    idea_name, fn = parse_idea(llm_idea)\n",
        "    print(\"Step 3... Validating Function\")\n",
        "\n",
        "    if evaluate:\n",
        "        visualize_highest_head(filtered_sentences, 8, model, tokenizer, (layer, head), fn)\n",
        "        visualize_full_model(sentences[0:5], model, tokenizer, fn, title=f\"Top Heads: {idea_name} [AVERAGE]\\n\"+model_name, bias_towards_best=0.7)\n",
        "    return idea_name, fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20d035f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# INITIAL AUTOMATED / LLM-GENERATED FILTERS\n",
        "\n",
        "def direct_object_prepositional_object_alignment(sentence, tokenizer):\n",
        "    \"\"\"\n",
        "    Hypothesizes that Layer 7, Head 1 is responsible for aligning verbs and prepositions\n",
        "    with their direct or prepositional objects.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence.\n",
        "        tokenizer: The tokenizer object (e.g., AutoTokenizer.from_pretrained(\"bert-base-uncased\")).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A string describing the pattern and a 2D numpy array\n",
        "               representing the predicted attention matrix.\n",
        "    \"\"\"\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    input_ids = toks.input_ids[0]\n",
        "    token_len = len(input_ids)\n",
        "    predicted_matrix = np.zeros((token_len, token_len))\n",
        "\n",
        "    # Get word IDs to align with spaCy tokens\n",
        "    word_ids = toks.word_ids()\n",
        "\n",
        "    # Process sentence with spaCy\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Map spaCy token indices to BERT token indices\n",
        "    spacy_to_bert_map = {}\n",
        "    bert_to_spacy_map = {}\n",
        "    current_spacy_token_idx = -1\n",
        "    for bert_idx, word_id in enumerate(word_ids):\n",
        "        if word_id is not None and (current_spacy_token_idx == -1 or word_id != word_ids[bert_idx - 1]):\n",
        "            current_spacy_token_idx = word_id\n",
        "            spacy_to_bert_map[current_spacy_token_idx] = bert_idx\n",
        "            bert_to_spacy_map[bert_idx] = current_spacy_token_idx\n",
        "        elif word_id is not None:\n",
        "            bert_to_spacy_map[bert_idx] = current_spacy_token_idx\n",
        "\n",
        "    # Iterate through spaCy tokens to find verbs and prepositions and their objects\n",
        "    for i, token in enumerate(doc):\n",
        "        # Find BERT index for the current spaCy token\n",
        "        from_bert_idx_start = -1\n",
        "        for bert_idx, spacy_id in bert_to_spacy_map.items():\n",
        "            if spacy_id == i:\n",
        "                from_bert_idx_start = bert_idx\n",
        "                break\n",
        "\n",
        "        if from_bert_idx_start == -1: # Skip if spaCy token doesn't map to BERT token\n",
        "            continue\n",
        "\n",
        "        # Look for direct objects (dobj) or prepositional objects (pobj)\n",
        "        if token.pos_ == \"VERB\":\n",
        "            for child in token.children:\n",
        "                if child.dep_ == \"dobj\":\n",
        "                    # Distribute attention from the verb to its direct object tokens\n",
        "                    to_bert_idx_start = -1\n",
        "                    for bert_idx, spacy_id in bert_to_spacy_map.items():\n",
        "                        if spacy_id == child.i:\n",
        "                            to_bert_idx_start = bert_idx\n",
        "                            break\n",
        "                    if to_bert_idx_start != -1:\n",
        "                        # Find all BERT tokens that correspond to the spaCy child token\n",
        "                        bert_indices_for_child = [b_idx for b_idx, s_id in bert_to_spacy_map.items() if s_id == child.i]\n",
        "                        if bert_indices_for_child:\n",
        "                            # Assign high attention from the 'from' BERT token (verb)\n",
        "                            # to all BERT tokens that form the 'to' (object)\n",
        "                            for to_b_idx in bert_indices_for_child:\n",
        "                                predicted_matrix[from_bert_idx_start, to_b_idx] = 0.8 # High weight\n",
        "\n",
        "        elif token.pos_ == \"ADP\":  # Adposition (preposition or postposition)\n",
        "            for child in token.children:\n",
        "                if child.dep_ == \"pobj\":\n",
        "                    # Distribute attention from the preposition to its object tokens\n",
        "                    to_bert_idx_start = -1\n",
        "                    for bert_idx, spacy_id in bert_to_spacy_map.items():\n",
        "                        if spacy_id == child.i:\n",
        "                            to_bert_idx_start = bert_idx\n",
        "                            break\n",
        "                    if to_bert_idx_start != -1:\n",
        "                        bert_indices_for_child = [b_idx for b_idx, s_id in bert_to_spacy_map.items() if s_id == child.i]\n",
        "                        if bert_indices_for_child:\n",
        "                            for to_b_idx in bert_indices_for_child:\n",
        "                                predicted_matrix[from_bert_idx_start, to_b_idx] = 0.8 # High weight\n",
        "\n",
        "    # Add self-attention for [CLS] and [SEP] tokens\n",
        "    predicted_matrix[0, 0] = 1.0\n",
        "    predicted_matrix[token_len - 1, token_len - 1] = 1.0\n",
        "\n",
        "    # For any row where no attention has been assigned, distribute attention uniformly\n",
        "    # or assign to [CLS] for general context\n",
        "    for i in range(token_len):\n",
        "        if np.sum(predicted_matrix[i, :]) == 0:\n",
        "            # Fallback: if no specific object found, distribute attention somewhat broadly\n",
        "            # or assign to CLS for general context (this is a heuristic)\n",
        "            predicted_matrix[i, 0] = 0.5 # Attend to CLS for general context\n",
        "            predicted_matrix[i, i] = 0.5 # Self-attention\n",
        "\n",
        "    # Normalize each row to sum to 1\n",
        "    for i in range(token_len):\n",
        "        row_sum = np.sum(predicted_matrix[i, :])\n",
        "        if row_sum > 0:\n",
        "            predicted_matrix[i, :] = predicted_matrix[i, :] / row_sum\n",
        "\n",
        "    return 'Direct Object / Prepositional Object Alignment', predicted_matrix\n",
        "\n",
        "def determiner_noun_phrase_linking(sentence: str, tokenizer) -> tuple[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Hypothesizes attention patterns where determiners link to the nouns\n",
        "    and adjectives within their associated noun phrases.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence.\n",
        "        tokenizer: The tokenizer object (e.g., from Hugging Face Transformers).\n",
        "\n",
        "    Returns:\n",
        "        tuple[str, np.ndarray]: A tuple containing the name of the pattern\n",
        "                                and the predicted attention matrix.\n",
        "    \"\"\"\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    input_ids = toks.input_ids[0]\n",
        "    token_len = len(input_ids)\n",
        "    predicted_matrix = np.zeros((token_len, token_len))\n",
        "\n",
        "    # Get spaCy doc for linguistic analysis\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Create a mapping from tokenizer's token indices to spaCy's token indices\n",
        "    # This is crucial for aligning the attention matrix with linguistic features.\n",
        "    # The tokenizer's `word_ids` method is ideal for this.\n",
        "    word_ids = toks.word_ids(batch_index=0) # Get word_ids for the first (and only) sentence in the batch\n",
        "\n",
        "    for i in range(token_len):\n",
        "        current_word_idx = word_ids[i]\n",
        "        if current_word_idx is not None and current_word_idx < len(doc):\n",
        "            spacy_token = doc[current_word_idx]\n",
        "\n",
        "            # If the current token (from the tokenizer) corresponds to a determiner in spaCy\n",
        "            if spacy_token.pos_ == \"DET\":\n",
        "                # Find the head of the determiner (typically the noun it modifies)\n",
        "                head_spacy_token = spacy_token.head\n",
        "\n",
        "                # Attend from the determiner's subword token(s) to its head's subword token(s)\n",
        "                for j in range(token_len):\n",
        "                    target_word_idx = word_ids[j]\n",
        "                    if target_word_idx is not None and target_word_idx == head_spacy_token.i:\n",
        "                        predicted_matrix[i, j] = 1.0\n",
        "\n",
        "                # Also attend from the determiner's subword token(s) to any adjectives\n",
        "                # that are children of the head and appear before the head\n",
        "                for child in head_spacy_token.children:\n",
        "                    if child.pos_ == \"ADJ\" and child.i < head_spacy_token.i:\n",
        "                        for j in range(token_len):\n",
        "                            target_word_idx = word_ids[j]\n",
        "                            if target_word_idx is not None and target_word_idx == child.i:\n",
        "                                predicted_matrix[i, j] = 1.0\n",
        "\n",
        "\n",
        "    # Apply self-attention for [CLS] and [SEP] tokens\n",
        "    predicted_matrix[0, 0] = 1.0\n",
        "    predicted_matrix[token_len - 1, token_len - 1] = 1.0\n",
        "\n",
        "    # Normalize rows to sum to 1 to represent attention probabilities\n",
        "    # Avoid division by zero for rows that might still be all zeros (e.g., padding tokens)\n",
        "    row_sums = predicted_matrix.sum(axis=1, keepdims=True)\n",
        "    predicted_matrix = np.where(row_sums == 0, 0, predicted_matrix / row_sums)\n",
        "\n",
        "    return \"Determiner-Noun/Adjective-Noun Phrase Linking\", predicted_matrix\n",
        "\n",
        "def verb_phrase_modifier_attention(sentence: str, tokenizer) -> tuple[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Hypothesizes the attention pattern for a head responsible for connecting\n",
        "    verbs to their related phrases and modifiers (subjects, objects, adverbs, PPs).\n",
        "\n",
        "    Args:\n",
        "        sentence: The input sentence.\n",
        "        tokenizer: The tokenizer object (e.g., from Hugging Face Transformers).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - The name of the hypothesized pattern.\n",
        "            - A NumPy array (predicted_matrix) representing the rule-encoded\n",
        "              attention pattern.\n",
        "    \"\"\"\n",
        "    # Load the English NLP model for spaCy\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"Downloading en_core_web_sm model for spaCy. Please run 'python -m spacy download en_core_web_sm' once.\")\n",
        "        spacy.cli.download(\"en_core_web_sm\")\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Tokenize the sentence using the provided tokenizer\n",
        "    tokens = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    input_ids = tokens.input_ids[0].tolist()\n",
        "    token_ids = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    len_seq = len(token_ids)\n",
        "    predicted_matrix = np.zeros((len_seq, len_seq))\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Create a mapping from spaCy token index to BERT token indices\n",
        "    # This handles WordPiece tokenization where one spaCy token might be multiple BERT tokens\n",
        "    spacy_to_bert_map = []\n",
        "    current_bert_idx = 1  # Start after [CLS]\n",
        "\n",
        "    for spacy_token in doc:\n",
        "        # Tokenize the spaCy token to get its BERT sub-tokens\n",
        "        bert_sub_tokens = tokenizer.tokenize(spacy_token.text)\n",
        "        bert_indices_for_spacy_token = list(range(current_bert_idx, current_bert_idx + len(bert_sub_tokens)))\n",
        "        spacy_to_bert_map.append(bert_indices_for_spacy_token)\n",
        "        current_bert_idx += len(bert_sub_tokens)\n",
        "\n",
        "    # Iterate through spaCy tokens to identify verbs and their relations\n",
        "    for i, spacy_token in enumerate(doc):\n",
        "        # Get the BERT indices corresponding to the current spaCy token\n",
        "        from_bert_indices = spacy_to_bert_map[i]\n",
        "\n",
        "        # Prioritize attention to verb and its direct dependents\n",
        "        if spacy_token.pos_ == \"VERB\":\n",
        "            # Direct attention from the verb to its subject (nsubj) and direct object (dobj)\n",
        "            for child in spacy_token.children:\n",
        "                if child.dep_ in [\"nsubj\", \"dobj\", \"iobj\", \"attr\", \"acomp\", \"xcomp\", \"prep\", \"advcl\", \"advmod\"]:\n",
        "                    if child.i < len(spacy_to_bert_map): # Ensure child index is within bounds\n",
        "                        to_bert_indices = spacy_to_bert_map[child.i]\n",
        "                        for from_idx in from_bert_indices:\n",
        "                            for to_idx in to_bert_indices:\n",
        "                                if from_idx < len_seq and to_idx < len_seq:\n",
        "                                    predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "            # Also attend from the verb to itself for self-attention\n",
        "            for idx in from_bert_indices:\n",
        "                if idx < len_seq:\n",
        "                    predicted_matrix[idx, idx] = 1.0\n",
        "\n",
        "        # Prioritize attention from subjects/adverbs/prepositions to their governing verb\n",
        "        elif spacy_token.dep_ in [\"nsubj\", \"advmod\", \"prep\", \"aux\", \"auxpass\"]:\n",
        "            if spacy_token.head and spacy_token.head.pos_ == \"VERB\":\n",
        "                head_bert_indices = spacy_to_bert_map[spacy_token.head.i]\n",
        "                for from_idx in from_bert_indices:\n",
        "                    for to_idx in head_bert_indices:\n",
        "                        if from_idx < len_seq and to_idx < len_seq:\n",
        "                            predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "        # Prioritize attention from direct objects/complement to their governing verb\n",
        "        elif spacy_token.dep_ in [\"dobj\", \"iobj\", \"attr\", \"acomp\", \"xcomp\", \"ccomp\", \"acl\"]:\n",
        "            if spacy_token.head and spacy_token.head.pos_ == \"VERB\":\n",
        "                head_bert_indices = spacy_to_bert_map[spacy_token.head.i]\n",
        "                for from_idx in from_bert_indices:\n",
        "                    for to_idx in head_bert_indices:\n",
        "                        if from_idx < len_seq and to_idx < len_seq:\n",
        "                            predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "        # Attention from prepositions to the noun phrase they introduce\n",
        "        elif spacy_token.pos_ == \"ADP\": # Adposition (preposition or postposition)\n",
        "            for child in spacy_token.children:\n",
        "                if child.dep_ == \"pobj\": # Object of preposition\n",
        "                    if child.i < len(spacy_to_bert_map):\n",
        "                        to_bert_indices = spacy_to_bert_map[child.i]\n",
        "                        for from_idx in from_bert_indices:\n",
        "                            for to_idx in to_bert_indices:\n",
        "                                if from_idx < len_seq and to_idx < len_seq:\n",
        "                                    predicted_matrix[from_idx, to_idx] = 1.0\n",
        "                # If the preposition is attached to a verb, also attend back to the verb\n",
        "                if spacy_token.head and spacy_token.head.pos_ == \"VERB\":\n",
        "                    head_bert_indices = spacy_to_bert_map[spacy_token.head.i]\n",
        "                    for from_idx in from_bert_indices:\n",
        "                        for to_idx in head_bert_indices:\n",
        "                            if from_idx < len_seq and to_idx < len_seq:\n",
        "                                predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "        # Adjectives attending to their noun or verb (if copular)\n",
        "        elif spacy_token.pos_ == \"ADJ\":\n",
        "            if spacy_token.head:\n",
        "                if spacy_token.head.pos_ == \"NOUN\" or (spacy_token.head.pos_ == \"VERB\" and spacy_token.dep_ == \"acomp\"):\n",
        "                    head_bert_indices = spacy_to_bert_map[spacy_token.head.i]\n",
        "                    for from_idx in from_bert_indices:\n",
        "                        for to_idx in head_bert_indices:\n",
        "                            if from_idx < len_seq and to_idx < len_seq:\n",
        "                                predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "        # Handle attention from [CLS] and [SEP] tokens\n",
        "        # [CLS] token (index 0) often has broad attention or self-attention\n",
        "        predicted_matrix[0, 0] = 1.0\n",
        "        # [SEP] token (last token) often attends to [CLS] or has self-attention\n",
        "        if len_seq > 1:\n",
        "            predicted_matrix[len_seq - 1, 0] = 1.0\n",
        "            predicted_matrix[len_seq - 1, len_seq - 1] = 1.0\n",
        "\n",
        "        # Ensure all rows sum to 1 by distributing any remaining attention to [CLS] or [SEP]\n",
        "    for i in range(len_seq):\n",
        "        current_row_sum = predicted_matrix[i].sum()\n",
        "        if current_row_sum == 0:\n",
        "            # If a row is all zeros, distribute attention to [CLS] and [SEP]\n",
        "            # or to itself if it's [CLS] or [SEP]\n",
        "            if i == 0:  # [CLS] token\n",
        "                predicted_matrix[i, 0] = 1.0\n",
        "            elif i == len_seq - 1:  # [SEP] token\n",
        "                predicted_matrix[i, len_seq - 1] = 1.0\n",
        "            else:\n",
        "                # For other tokens, distribute attention to [CLS] and [SEP]\n",
        "                # You could also consider distributing to the token itself or other meaningful global tokens\n",
        "                predicted_matrix[i, 0] = 0.5\n",
        "                if len_seq > 1:\n",
        "                    predicted_matrix[i, len_seq - 1] = 0.5\n",
        "        else:\n",
        "            predicted_matrix[i] = predicted_matrix[i] / current_row_sum\n",
        "\n",
        "    return \"Verb-Related Phrase and Modifier Focus\", predicted_matrix"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "newenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
