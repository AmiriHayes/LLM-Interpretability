{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6443588e",
      "metadata": {
        "id": "6443588e"
      },
      "source": [
        "Author: Amiri Hayes \\\n",
        "Date Updated: 7/15/25 \\\n",
        "Title: ViewLLM\n",
        "\n",
        "Starter code !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99",
      "metadata": {
        "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99"
      },
      "outputs": [],
      "source": [
        "# package installs if necessary\n",
        "!pip install torch spacy nltk tqdm transformers datasets scikit-learn openai\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3e36de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3e36de",
        "outputId": "5c233452-f8b9-4137-f8b9-cd2881649f94"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import Optional, Tuple, Callable\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import PowerNorm, ListedColormap\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
        "from openai import OpenAI\n",
        "load_dotenv(find_dotenv())\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)\n",
        "print(\"Imports completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012daf28",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Data & Pattern Initialization Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbe22fd",
      "metadata": {
        "id": "7cbe22fd"
      },
      "outputs": [],
      "source": [
        "# IMPORT THE PROGRAM DATABASE:\n",
        "\n",
        "from programs import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d51a2fa",
      "metadata": {
        "id": "9d51a2fa"
      },
      "outputs": [],
      "source": [
        "# GENERATE & VISUALIZE ATTENTION PATTERN SCORES FROM PROGRAMS\n",
        "\n",
        "def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    q = np.clip(q, 1e-12, 1.0)\n",
        "    p /= p.sum()\n",
        "    q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
        "\n",
        "def score_prediction(torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: Tuple[int, int], pattern: Callable, sentence_1: str, sentence_2: Optional[str] = None, distance=\"jsd\", output=False):\n",
        "    layer, head = head_loc\n",
        "    tokens = torch_tokenizer(sentence_1, return_tensors=\"pt\")\n",
        "\n",
        "    if torch_model.config.is_encoder_decoder:\n",
        "        decoder_input_ids = tokens[\"input_ids\"]\n",
        "        outputs = torch_model(input_ids=tokens[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "        att = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "        name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    else:\n",
        "        if sentence_2 and pattern.__name__ == \"chainofthought_pattern\":\n",
        "            name = \"Chain of Thought Pattern\"\n",
        "            tokens_2 = torch_tokenizer(sentence_2, return_tensors=\"pt\")\n",
        "\n",
        "            att = torch_model(**tokens_2, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            pred_att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "            if output: print(\"RUNNING FIRST WITH NO HINT\")\n",
        "            question, answer, vector_att = chainofthought_pattern(sentence_1, torch_tokenizer, pred_att, hint=False)\n",
        "            if output: print(\"RUNNING AFTER WITH A HINT\")\n",
        "            question, answer, vector_pred_att = chainofthought_pattern(sentence_2, torch_tokenizer, att, hint=True)\n",
        "\n",
        "            att, pred_att = vector_att.copy(), vector_pred_att.copy()\n",
        "        else:\n",
        "            att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            if pattern.__name__ == \"linear_fit\":\n",
        "                name, pred_att = pattern(sentence_1, torch_tokenizer, idx=0)\n",
        "            else: name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    if distance == \"raw\":\n",
        "        score = np.abs(att - pred_att).sum()\n",
        "    elif distance == \"jsd\":\n",
        "        jensonshannon_distances = []\n",
        "        for row_att, row_out in zip(att, pred_att):\n",
        "            jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "        score = np.mean(jensonshannon_distances)\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\":\n",
        "        score = np.sqrt(js_divergence(att, pred_att))\n",
        "\n",
        "    if output == \"cot\":\n",
        "        colors = \"inferno\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 9))\n",
        "        axes[0].plot(att, color=plt.get_cmap(colors)(0.6))\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        axes[1].plot(pred_att, color=plt.get_cmap(colors)(0.9))\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        bound_axes = False\n",
        "        for i in range(2):\n",
        "            axes[i].set_xlabel(\"Token Index\")\n",
        "            axes[i].set_ylabel(\"Attention Weight\")\n",
        "            axes[i].grid(True)\n",
        "            if bound_axes:\n",
        "                axes[i].set_ylim(0, 1)\n",
        "                axes[i].set_xlim(0, len(att) - 1)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        question_chart = question.replace(\".\", \".\\n\")\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nQuestion: \\\"{question_chart}\\n\\nAnswer: \\\"{answer}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "    \n",
        "    toks = torch_tokenizer([sentence_1], return_tensors=\"pt\")\n",
        "    token_ids = toks[\"input_ids\"][0]\n",
        "    tokens = torch_tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    if output == True:\n",
        "        colors=\"Greens\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        im1 = axes[0].imshow(att, cmap=colors, aspect='auto')\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        fig.colorbar(im1, ax=axes[0])\n",
        "        im2 = axes[1].imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        fig.colorbar(im2, ax=axes[1])\n",
        "        for i in range(2):\n",
        "            axes[i].set_xticks(range(len(tokens)))\n",
        "            axes[i].set_yticks(range(len(tokens)))\n",
        "            # get rid of the weird special characters in each token in tokens\n",
        "            for token in tokens:\n",
        "                if token.startswith(\"Ä \"):\n",
        "                    tokens[tokens.index(token)] = token[1:]\n",
        "            axes[i].set_xticklabels(tokens, rotation=90)\n",
        "            axes[i].set_yticklabels(tokens)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nSentence: \\\"{sentence_1}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"optimal\":\n",
        "        colors = \"Oranges\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        ax.set_axis_off()\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"actual\":\n",
        "        colors = \"Reds\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(att, cmap=colors, aspect='auto')\n",
        "        ax.set_title(\"Example Head Attention for Pattern\")\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n899pxRzSWRe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n899pxRzSWRe",
        "outputId": "470ad98d-f61e-4b36-bde9-e6c2bb80882d"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (1):\n",
        "\n",
        "file = 'data/small_text.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "sentences = []\n",
        "for paragraph in df['text']:\n",
        "    sentences.extend(sent_tokenize(paragraph))\n",
        "\n",
        "sentences = sentences[:10_000]\n",
        "print(\"Sentences from Tiny Stories Dataset:\")\n",
        "for sentence in sentences[20:30]:\n",
        "  print(f\"\\t{sentence}\")\n",
        "\n",
        "df_json = pd.read_json('data/generic_sentences.json')\n",
        "generic_sentences = df_json[0].tolist()\n",
        "print(\"\\nGeneric Sentences:\")\n",
        "for sentence in generic_sentences[:10]:\n",
        "  print(f\"\\t{sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0adb7458",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (2):\n",
        "\n",
        "math_data = pd.read_json('data/math_problems_results.jsonl', lines=True)\n",
        "\n",
        "filtered_results = math_data[\n",
        "    (math_data['consistency'] == \"False\") &\n",
        "    (math_data['evaluated_answer_nohint'] != \"DNF: llm did not finish\") &\n",
        "    (math_data['evaluated_answer_hint'] != \"DNF: llm did not finish\")\n",
        "]\n",
        "\n",
        "answers_nohint = filtered_results['answer_nohint'].tolist()\n",
        "answers_hint = filtered_results['answer_hint'].tolist()\n",
        "prefix = \"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nuser\\n\\n\"\n",
        "\n",
        "prompts = []\n",
        "for s1, s2 in zip(answers_nohint, answers_hint):\n",
        "    if s1.startswith(prefix): s1 = s1[len(prefix):]\n",
        "    if s2.startswith(prefix): s2 = s2[len(prefix):]\n",
        "\n",
        "    i_suffix_s1 = s1.find(\"assistant\")\n",
        "    if i_suffix_s1 != -1: s1 = s1[:i_suffix_s1].strip()\n",
        "\n",
        "    i_suffix_s2 = s2.find(\"assistant\")\n",
        "    if i_suffix_s2 != -1: s2 = s2[:i_suffix_s2].strip()\n",
        "\n",
        "    if s1 and s2: prompts.append((s1, s2))\n",
        "\n",
        "print(len(prompts), \"relevant prompts loaded from math problems dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1015c0ec",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "  Analysis Helper Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff49f93",
      "metadata": {
        "id": "1ff49f93",
        "outputId": "d8f00b5c-d395-4564-885c-d1f65e5c815d"
      },
      "outputs": [],
      "source": [
        "# ANALYZE PATTERN AT LAYER AND HEAD\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog. That he does.\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "# sentence = \"I like apples and I like bananas. I like apples more though.\"\n",
        "\n",
        "models = [\"bert-base-uncased\", \"openai-community/gpt2\", \"google-t5/t5-small\", \"Meta-Llama-3.1-8B-Instruct\"]\n",
        "\n",
        "i = 0\n",
        "name = models[i]\n",
        "\n",
        "if i in [0,1,2]: # small models ( <400M Params )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    model = AutoModel.from_pretrained(name, output_attentions=True)\n",
        "    model.eval()\n",
        "\n",
        "elif i == 3: # big model & cot ( 8B Params, Load from Compute Node )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt_num = 0\n",
        "    sentence = prompts[prompt_num][0]  # Use the prompt's first sentence (no hint)\n",
        "    sentence_with_hint = prompts[prompt_num][1]  # Use prompt's second sentence (hint)\n",
        "\n",
        "layer, head = 1,1\n",
        "# sentence = \"he said no, he did not, he will not, he feels weirdly good about it\"_\n",
        "score_prediction(model, tokenizer, (layer, head), next_attention, sentence, distance=\"jsd\", output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd29726",
      "metadata": {
        "id": "9fd29726",
        "outputId": "feeb18a4-11de-4928-9582-fdf4b4c816df"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (1 SENTENCE)\n",
        "\n",
        "def visualize_full_model(sentence: str, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9, sentence_2: Optional[str] = None) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    num_heads = model.config.num_attention_heads\n",
        "    model_viz = np.zeros((num_layers, num_heads))\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_heads):\n",
        "            score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "            model_viz[i, j] = score\n",
        "    print(f\"Best Score: {min(map(min, model_viz)):.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=model_viz.min(), vmax=model_viz.max())\n",
        "    plt.imshow(model_viz, cmap='Greens_r', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_three = list(zip(*np.unravel_index(np.argsort(model_viz, axis=None)[:3], model_viz.shape)))\n",
        "    for tuple_val in top_three:\n",
        "        layer, head = tuple_val\n",
        "        score = model_viz[layer, head]\n",
        "        print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return model_viz, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "pattern_name = \"Pattern Name Here\"\n",
        "underlined_name_unicode = \"\".join([char + '\\u0332' for char in pattern_name])\n",
        "visualize_full_model(sentence, model, tokenizer, adverbial_modulation, title=f\"Top Heads: {underlined_name_unicode} | {model_name}\\nSentence: \\\"{sentence}\\\"\\n\", bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G90Rgk6fVKIZ",
      "metadata": {
        "id": "G90Rgk6fVKIZ",
        "outputId": "56ce27bd-6ea5-4f3d-cbf7-f6dc1b69f015"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (MULTIPLE SENTENCES)\n",
        "\n",
        "def visualize_full_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    header = [\"Layer\", \"Head\", \"Score\"]\n",
        "    csv_file_name = \"scores.csv\"\n",
        "    file_exists = os.path.exists(csv_file_name)\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "    \n",
        "        average_score = np.zeros((num_layers, num_heads))\n",
        "        for sentence in sentences:\n",
        "            sentence_1 = sentence[0]  # first sentence (no hint)\n",
        "            sentence_2 = sentence[1]  # second sentence (hint)\n",
        "            model_score = np.zeros((num_layers, num_heads))\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    score = score_prediction(model, tokenizer, (i, j), chainofthought_pattern, sentence_1, sentence_2, distance=\"jsd\", output=False)\n",
        "                    writer.writerow([i, j, f\"{score:.2f}\"])\n",
        "                    print(f\"Layer {i}, Head {j} - Score: {score:.2f}\")\n",
        "                    model_score[i, j] = score\n",
        "            average_score += model_score\n",
        "        average_score /= len(sentences)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=average_score.min(), vmax=average_score.max())\n",
        "    plt.imshow(average_score, cmap='Reds', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\": \n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score.ravel())[::-1][:3], average_score.shape))) # highest scores\n",
        "    else:\n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score, axis=None)[3:], average_score.shape))) # lowest scores\n",
        "        top_three = np.sort(average_score)\n",
        "\n",
        "    for tuple_val in top_three:\n",
        "            layer, head = tuple_val\n",
        "            score = average_score[layer, head]\n",
        "            print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return average_score, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentences_zipped = list(zip(answers_nohint[:5], answers_hint[:5]))\n",
        "visualize_full_model(sentences_zipped, model, tokenizer, relative_position_attention, title=\"Top Heads: Chain-of_Thought Evaluation [AVERAGE]\\n\"+model_name, bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eTFYV1Gpzv",
      "metadata": {
        "id": "b8eTFYV1Gpzv"
      },
      "outputs": [],
      "source": [
        "# FILTER SENTENCES TO TARGET SPECIFIC PATTERNS\n",
        "\n",
        "def filterer(sentences: list[str], length_matters: bool=False, punctuation_matters: bool=False, duplicates: bool=False) -> list[str]:\n",
        "    filtered_sentences = []\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    for sentence in sentences:\n",
        "        passes_all_active_filters = True\n",
        "        if length_matters:\n",
        "            if len(sentence.split()) < 5:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if punctuation_matters:\n",
        "            punctuation_count = sum(1 for char in sentence if char in punctuation_set)\n",
        "            if punctuation_count < 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if duplicates:\n",
        "            set_l = len(set(sentence.split()))\n",
        "            sent_1 = len(sentence.split())\n",
        "            if set_l >= sent_1 - 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if passes_all_active_filters: filtered_sentences.append(sentence)\n",
        "    return filtered_sentences\n",
        "\n",
        "filtered_sentences = filterer(generic_sentences, length_matters = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mR56LNrRd37k",
      "metadata": {
        "id": "mR56LNrRd37k",
        "outputId": "4b42cff5-d843-439d-8409-d5c47e9b87a7"
      },
      "outputs": [],
      "source": [
        "# ANALYZE HEAD PATTERN ON SELECTED SENTENCES\n",
        "\n",
        "def visualize_highest_head(sentences: list[str], top_n:  int, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: tuple[int, int], pattern: Callable):\n",
        "    layer, head = head_loc\n",
        "    scores = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        score = score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, distance=\"jsd\", output=False)\n",
        "        scores.append(score)\n",
        "\n",
        "    indexed_scores = list(enumerate(scores))\n",
        "    sorted_scores = sorted(indexed_scores, key= lambda x: x[1])\n",
        "    top_scores = sorted_scores[:top_n]\n",
        "\n",
        "    for idx, score in top_scores:\n",
        "        print(f\"Sentence #{idx} Score: {score}\")\n",
        "        sentence = sentences[idx]\n",
        "        score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, output=True)\n",
        "    return 0\n",
        "\n",
        "layer, head = 7, 8\n",
        "visualize_highest_head(filtered_sentences, 3, model, tokenizer, (layer, head), punctuation_attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be980bdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS\n",
        "\n",
        "def classify_whole_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, patterns: list[Callable]) -> dict[Tuple[int, int], Tuple[str, float]]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "    activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "    header = [\"i\", \"j\", \"Pattern\", \"Score\"]\n",
        "    \n",
        "    csv_file_name = \"data/best_fit_refinement_gpt2_new.csv\"\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        file_exists = os.path.exists(csv_file_name)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "\n",
        "        for pattern in patterns:\n",
        "            try:\n",
        "                print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "                all_scores = []\n",
        "                for idx, sentence in enumerate(sentences):\n",
        "                    if idx % 20 == 0: print(f\"\\tProcessing sentence {idx}/{len(sentences)}\")\n",
        "                    for i in range(num_layers):\n",
        "                        for j in range(num_heads):\n",
        "                            score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                            if score < 0.55: print(f\"sentence #{idx}|\", i, j, score)\n",
        "                            all_scores.append(score)\n",
        "\n",
        "                average_scores = np.array(all_scores).reshape(len(sentences), num_layers * num_heads).mean(axis=0)\n",
        "                head_performance = average_scores.reshape(num_layers, num_heads)\n",
        "                print(head_performance)\n",
        "\n",
        "                ix, jx = np.where(head_performance < 0.45)\n",
        "                pairs = list(zip(ix, jx))\n",
        "\n",
        "                for (ix, jx) in pairs:\n",
        "                    print(ix, jx, head_performance[ix, jx])\n",
        "                    # if key not in activations or pattern_score < activations[key][1]:\n",
        "                    #     activations[key] = (pattern.__name__, pattern_score)\n",
        "                    writer.writerow([ix, jx, pattern.__name__, head_performance[ix, jx]])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing pattern {pattern.__name__}: {e}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            # print(len(avg_scores))\n",
        "            # pattern_score = np.mean(avg_scores)\n",
        "            # print(i, j, pattern_score)\n",
        "\n",
        "            # if pattern_score > 0.5: continue\n",
        "            # key = (i, j)\n",
        "\n",
        "            # if key not in activations or pattern_score < activations[key][1]:\n",
        "            #     activations[key] = (pattern.__name__, pattern_score)\n",
        "            # writer.writerow([i, j, pattern.__name__, pattern_score])\n",
        "\n",
        "    return activations\n",
        "\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "# patterns = [next_attention, previous_attention, same_attention, punctuation_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "activations = classify_whole_model(generic_sentences[:5], model, tokenizer, patterns)\n",
        "print(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6a8ddd",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# NEVERMIND, loop through all heads find the corresponsing programs from llm_code in automation_results_bert and bert2 and save to best_fit_refinement_bert.csv if score < 0.4\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    for head in range(num_heads):\n",
        "        # print(f\"Layer {layer}, Head {head}\")\n",
        "        # look for layer, head in automation_results_bert/scores (e.g. C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\automation_results_bert\\scores\\layer0_head0_score.txt)\n",
        "        with open(f\"automation_results_gpt2/scores/layer{layer}_head{head}_score.txt\", 'r') as f:\n",
        "            score_1 = float(f.read().strip())\n",
        "\n",
        "        if score_1 < 0.25:\n",
        "            # copy the name of the corresponding program from automation_results_bert/llm_code (e.g. C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\automation_results_bert\\llm_code\\layer0_head0_code.py)\n",
        "            with open(f\"automation_results_bert/llm_code/layer{layer}_head{head}_code.py\", 'r') as f:\n",
        "                # program name is after \"def \" and before \"(\", isn't necessarily the first line, read all lines and join\n",
        "                all_lines = f.readlines()\n",
        "\n",
        "                # strip all_lines of stuff before def and after ( \n",
        "                program_name = \" \".join([line.strip().split(\" \")[1] for line in all_lines if line.startswith(\"def \")])\n",
        "                program_name = program_name.split(\"(\")[0]\n",
        "\n",
        "            # save to best_fit_refinement_bert.csv\n",
        "            # with open('best_fit_refinement_bert.csv', 'a') as f:\n",
        "            #     f.write(f\"{layer},{head},{program_name},{score_1}\\n\")\n",
        "            print(f\"{layer},{head},{program_name},{score_1}\")\n",
        "\n",
        "            # for som reason the append isn't working\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ecc7a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from programs import *\n",
        "import importlib.util\n",
        "import types\n",
        "patterns = [next_attention, previous_attention, same_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "master_list_dir = \"automation_refinement_bert/master_list\"\n",
        "for filename in os.listdir(master_list_dir):\n",
        "    if filename.endswith(\".py\"):\n",
        "        module_name = filename[:-3]\n",
        "        if module_name not in [p.__name__ for p in patterns]:\n",
        "            try:\n",
        "                program_path = os.path.join(master_list_dir, filename)\n",
        "                spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                module.__dict__['np'] = np\n",
        "                # module.__dict__['torch'] = torch\n",
        "                module.__dict__['spacy'] = spacy\n",
        "                module.__dict__['nlp'] = nlp\n",
        "                module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "                module.__dict__['Optional'] = Optional\n",
        "                module.__dict__['Tuple'] = Tuple\n",
        "                module.__dict__['Callable'] = Callable\n",
        "                spec.loader.exec_module(module)\n",
        "            except Exception as e:\n",
        "                print(f\"Program loading failed: {str(e)}\")\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    program_to_test = attr\n",
        "                    break\n",
        "\n",
        "            patterns.append(program_to_test)\n",
        "\n",
        "master_list_dir = \"automation_refinement/master_list\"\n",
        "for filename in os.listdir(master_list_dir):\n",
        "    if filename.endswith(\".py\"):\n",
        "        module_name = filename[:-3]\n",
        "        if module_name not in [p.__name__ for p in patterns]:\n",
        "            try:\n",
        "                program_path = os.path.join(master_list_dir, filename)\n",
        "                spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                module.__dict__['np'] = np\n",
        "                # module.__dict__['torch'] = torch\n",
        "                module.__dict__['spacy'] = spacy\n",
        "                module.__dict__['nlp'] = nlp\n",
        "                module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "                module.__dict__['Optional'] = Optional\n",
        "                module.__dict__['Tuple'] = Tuple\n",
        "                module.__dict__['Callable'] = Callable\n",
        "                spec.loader.exec_module(module)\n",
        "            except Exception as e:\n",
        "                print(f\"Program loading failed: {str(e)}\")\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    program_to_test = attr\n",
        "                    break\n",
        "\n",
        "            patterns.append(program_to_test)\n",
        "\n",
        "print(f\"Total patterns to analyze: {len(patterns)}\")\n",
        "for pattern in patterns: print(f\"\\tPattern: {pattern.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08432fb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS (CONTINUED)\n",
        "\n",
        "model = AutoModel.from_pretrained(\"google-t5/t5-small\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "\n",
        "torch_model = model\n",
        "torch_tokenizer = tokenizer\n",
        "\n",
        "num_layers = torch_model.config.num_hidden_layers\n",
        "num_heads = torch_model.config.num_attention_heads\n",
        "activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "\n",
        "from programs import *\n",
        "patterns = [next_attention, previous_attention, same_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "master_list_dir = \"automation_refinement_gpt2/master_list\"\n",
        "for filename in os.listdir(master_list_dir):\n",
        "    if filename.endswith(\".py\"):\n",
        "        module_name = filename[:-3]\n",
        "        if module_name not in [p.__name__ for p in patterns]:\n",
        "            try:\n",
        "                program_path = os.path.join(master_list_dir, filename)\n",
        "                spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                module.__dict__['np'] = np\n",
        "                # module.__dict__['torch'] = torch\n",
        "                module.__dict__['spacy'] = spacy\n",
        "                module.__dict__['nlp'] = nlp\n",
        "                module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "                module.__dict__['Optional'] = Optional\n",
        "                module.__dict__['Tuple'] = Tuple\n",
        "                module.__dict__['Callable'] = Callable\n",
        "                spec.loader.exec_module(module)\n",
        "            except Exception as e:\n",
        "                print(f\"Program loading failed: {str(e)}\")\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    program_to_test = attr\n",
        "                    break\n",
        "\n",
        "            patterns.append(program_to_test)\n",
        "\n",
        "print(f\"Total patterns to analyze: {len(patterns)}\")\n",
        "for pattern in patterns: print(f\"\\tPattern: {pattern.__name__}\")\n",
        "\n",
        "short = sentences[:8]\n",
        "csv_file_name = \"data/best_fit_refinement.csv\"\n",
        "with open(csv_file_name, 'a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for pattern in patterns:\n",
        "        print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "        avg_score = []\n",
        "        for idx, sentence in enumerate(short):\n",
        "            print(f\"\\tProcessing sentence {idx}/{len(short)}\")\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    if i != 3 or j != 9: continue\n",
        "                    score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                    if score < 0.55:\n",
        "                        avg_score.append((idx, pattern.__name__, i, j, score))\n",
        "        \n",
        "        score_dict = {}\n",
        "        for idx, pattern_name, i, j, score in avg_score:\n",
        "            score_dict.setdefault((i, j), []).append((pattern_name, score))\n",
        "        for (i, j), values in score_dict.items():\n",
        "            scores = [score for _, score in values]\n",
        "            avg_score_val = sum(scores) / len(scores)\n",
        "            pattern_name = values[0][0]\n",
        "            activations[(i, j)] = (pattern_name, avg_score_val)\n",
        "            print(f\"Layer {i}, Head {j} - Score: {avg_score_val:.2f}\")\n",
        "            writer.writerow([i, j, pattern.__name__, avg_score_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8374c077",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANALYZE EFFECT OF LINEAR WEIGHTS ON ATTENTION ACTIVATION ACCURACY\n",
        "\n",
        "def generate_dataset(patterns: list[Callable], model: PreTrainedModel, tokenizer: PreTrainedTokenizerBase, sentences: list[str], layer_head: tuple[int, int]):\n",
        "    layer, head = layer_head\n",
        "    X_data, y_data = [], []\n",
        "    print(\"Generating dataset for Layer\", layer, \", Head\", head)\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            attn = outputs.attentions[layer][0, head]\n",
        "        X_i_list = []\n",
        "        for pattern in patterns:\n",
        "            _, X_i = pattern(sentence, tokenizer)\n",
        "            X_i = torch.tensor(X_i, dtype=torch.float32)\n",
        "            X_i_list.append(X_i)\n",
        "        X_data.append(X_i_list)\n",
        "        y_data.append(attn)\n",
        "\n",
        "    torch.save({'X': X_data, 'y': y_data}, \"data/attention_dataset.pt\")\n",
        "    print(\"Dataset generated and saved to 'data/attention_dataset.pt'.\")\n",
        "\n",
        "def train_linearregression() -> pd.DataFrame:\n",
        "    data = torch.load(\"data/attention_dataset.pt\")\n",
        "    X, y = data['X'], data['y']\n",
        "    X, y = data['X'], data['y']\n",
        "    output = []\n",
        "\n",
        "    for i, (xb, yb) in enumerate(zip(X, y)):\n",
        "        xb = torch.stack(xb)\n",
        "        X_flat = (xb.reshape(len(xb), -1).T).numpy()\n",
        "        y_flat = yb.flatten().numpy()\n",
        "        reg = LinearRegression().fit(X_flat, y_flat)\n",
        "        if i % 100 == 0: print(f\"Sentence #{i} - Coeffs: {[float(f\"{coef:.2f}\") for coef in reg.coef_]}, Intercept: {reg.intercept_:.2f}\")\n",
        "        output.append([reg.coef_.tolist(), float(reg.intercept_)])\n",
        "\n",
        "    output = pd.DataFrame(output, columns=[\"Coefficients\", \"Intercept\"]).to_csv(\"data/linear_regression_results.csv\", index=False)\n",
        "    return output\n",
        "\n",
        "head_loc = (3, 9)\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "generate_dataset(patterns, model, tokenizer, sentences, head_loc)\n",
        "output = train_linearregression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1612da99",
      "metadata": {},
      "outputs": [],
      "source": [
        "# VISUALIZE BEST FIT PATTERNS ACROSS LAYERS AND HEADS\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"roberta-base\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "threshold = 0.45\n",
        "\n",
        "df = pd.read_csv('data/best_fit_refinement_gpt2_new.csv')\n",
        "# ignore columns where scores >= threshold\n",
        "df = df[df['Score'] < threshold]\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "mat = np.zeros((num_layers, num_heads), dtype=object)\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        mat[r, c] = []\n",
        "\n",
        "for (i, j), group in df.groupby(['i', 'j']):\n",
        "    sorted_group = group.sort_values(by='Score', ascending=False)\n",
        "    mat[i, j] = [(row['Pattern'], row['Score']) for idx, row in sorted_group.iterrows()]\n",
        "\n",
        "unique_patterns = df['Pattern'].unique()\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import ListedColormap, to_rgb\n",
        "\n",
        "colors_40 = plt.cm.get_cmap('tab20').colors + plt.cm.get_cmap('tab20b').colors\n",
        "custom_cmap_40 = ListedColormap(colors_40, name='tab40')\n",
        "cmap_patterns = plt.get_cmap(custom_cmap_40, len(unique_patterns))\n",
        "\n",
        "pattern_colors = {pattern: cmap_patterns(i) for i, pattern in enumerate(unique_patterns)}\n",
        "white_color = (1, 1, 1, 1)\n",
        "plotting_matrix_rgb = np.zeros((num_layers, num_heads, 3))\n",
        "\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        cell_data = mat[r, c]\n",
        "        \n",
        "        if not cell_data:\n",
        "            plotting_matrix_rgb[r, c] = white_color[:3]\n",
        "        elif len(cell_data) == 1: \n",
        "            pattern_name = cell_data[0][0]\n",
        "            plotting_matrix_rgb[r, c] = pattern_colors[pattern_name][:3]\n",
        "        else:\n",
        "            top_pattern_1 = cell_data[0][0]\n",
        "            top_pattern_2 = cell_data[1][0]\n",
        "            color_1 = pattern_colors[top_pattern_1][:3]\n",
        "            color_2 = pattern_colors[top_pattern_2][:3]\n",
        "            plotting_matrix_rgb[r, c] = color_1 \n",
        "\n",
        "custom_draw_mask = np.zeros((num_layers, num_heads), dtype=bool)\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        if len(mat[r, c]) > 1:\n",
        "            custom_draw_mask[r, c] = True\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 18))\n",
        "img = ax.imshow(plotting_matrix_rgb, origin='lower', extent=[-0.5, num_heads - 0.5, -0.5, num_layers - 0.5])\n",
        "\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        if custom_draw_mask[r, c]:\n",
        "            cell_data = mat[r, c]\n",
        "            top_pattern_1 = cell_data[0][0]\n",
        "            top_pattern_2 = cell_data[1][0]\n",
        "            \n",
        "            color_1 = pattern_colors[top_pattern_1]\n",
        "            color_2 = pattern_colors[top_pattern_2]\n",
        "            triangle1 = mpatches.Polygon(\n",
        "                [[c - 0.5, r - 0.5], [c + 0.5, r - 0.5], [c + 0.5, r + 0.5]],\n",
        "                facecolor=color_1, edgecolor='none'\n",
        "            )\n",
        "            ax.add_patch(triangle1)\n",
        "            triangle2 = mpatches.Polygon(\n",
        "                [[c - 0.5, r - 0.5], [c - 0.5, r + 0.5], [c + 0.5, r + 0.5]],\n",
        "                facecolor=color_2, edgecolor='none'\n",
        "            )\n",
        "            ax.add_patch(triangle2)\n",
        "\n",
        "ax.set_xticks(np.arange(num_heads))\n",
        "ax.set_yticks(np.arange(num_layers))\n",
        "ax.set_xticks(np.arange(-0.5, num_heads, 1), minor=True)\n",
        "ax.set_yticks(np.arange(-0.5, num_layers, 1), minor=True)\n",
        "ax.set_xlabel(f'{model.config.architectures[0]} - Heads', fontsize=14)\n",
        "ax.set_ylabel(f'{model.config.architectures[0]} - Layers', fontsize=14)\n",
        "# center title across whole figure\n",
        "# ax.set_title('What is GPT doing? - Best Fit Patterns Across All Heads and Layers', fontsize=18, pad=20, loc='center')\n",
        "ax.set_aspect('equal')\n",
        "# ax.grid(color='black', linestyle='-', linewidth=0.5)\n",
        "ax.grid(which='minor', color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "legend_handles = []\n",
        "for pattern, color in pattern_colors.items():\n",
        "    patch = mpatches.Patch(color=color, label=pattern)\n",
        "    legend_handles.append(patch)\n",
        "\n",
        "# center title\n",
        "title_value_text = 'What is BERT doing? - Best Fit Patterns Across Attention Heads'\n",
        "underlined_title_unicode = \"\".join([char + '\\u0332' for char in title_value_text])\n",
        "# plt.suptitle(underlined_title_unicode, fontsize=20, y=0.725, x=0.4)\n",
        "# sort handles alphabetically by label\n",
        "# legend_handles = sorted(legend_handles, key=lambda x: x.get_label())\n",
        "\n",
        "white_patch = mpatches.Patch(facecolor=white_color, label='No Hypotheses Validated', edgecolor='black', linewidth=0.2)\n",
        "legend_handles.append(white_patch)\n",
        "\n",
        "name = 'Confidence'\n",
        "underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "\n",
        "ax.legend(\n",
        "    handles=legend_handles, \n",
        "    loc='center left', \n",
        "    bbox_to_anchor=(1.05, 0.5),\n",
        "    ncol=3, \n",
        "    fancybox=True, \n",
        "    shadow=True, \n",
        "    title=f\"HIGH SCORING HYPOTHESES - {model.config.architectures[0]} | {underlined_name_unicode}: avg_score < {threshold}\\n\",\n",
        "    title_fontsize=16,\n",
        "    fontsize='large', # Make legend text bigger. Can use 'medium', 'x-large', 'xx-large' or a numerical value (e.g., 12)\n",
        "    labelspacing=1.5, # Adjust vertical spacing between legend entries (default is 0.5)\n",
        "    handlelength=2.5, # Adjust length of the color patch/line in the legend\n",
        "    handletextpad=0.8, # Adjust space between the handle (color patch) and the text label\n",
        "    borderpad=0.5 # Adjust padding between the legend content and its border \n",
        ") \n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1]) # Adjust layout to make space for the legend\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42abc485",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    instructions=\"You are a coding assistant that talks like a pirate.\",\n",
        "    input=\"How do I check if a Python object is an instance of a class?\",\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45650d28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt pieces (sequential)\n",
        "# 1 look at attention matrix and try to predict pattern\n",
        "# 2. come up with five candidate hypotheses and pick the top one\n",
        "# 3. explain why you picked that hypothesis\n",
        "# 4. write code that generates that pattern according to the hypothesis\n",
        "\n",
        "from automation_helper import example_one, example_two, example_three\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "layer = 3\n",
        "head = 9\n",
        "sentences = sentences[:10]\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model.config.architectures[0],\n",
        "        \"examples\": []}\n",
        "\n",
        "def handle_score(score):\n",
        "        # convert to percentage with 0 decimal places\n",
        "        return \"{:.0f}\".format(score * 100)\n",
        "\n",
        "def scrape_head(att, tokens, top_k_ratio, ignore_special=True):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append(f\"[{str(tokens[i])}|{str(tokens[j])}:{handle_score(score)}]\")\n",
        "        #make top activations str and delete brackets\n",
        "        top_activations_str = \" \".join(top_activations).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "        # print(top_activations_str)\n",
        "        return top_activations_str\n",
        "\n",
        "for idx, sentence in enumerate(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=0.05)\n",
        "        item = {f\"sentence {idx}\": \" \".join(tokens), \"sentence attention\": top_activations}\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "prompt_1 = f\"\"\"\n",
        "        You are given the following attention matrices sourced from {model.config.architectures[0]} based on {len(sentences)} sentences\n",
        "        from Layer {layer}, Head {head}. Look at the attenion matrix subset below and try to predict the most fitting three hypotheses\n",
        "        for the head function. Then choose what you think is the best hypothesis and explain why you picked that one. DATA: {data}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_2 = f\"\"\"\n",
        "        Now use your explanation to write a Python function that generates an attention matrix for any given sentence according to your hypothesis  in less than 50 lines. The input to your function is def fn(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: and \n",
        "        the output is a tuple of a string (the name of the pattern) and a numpy array (the attention matrix). Your response should be a single code block with no extra text, and must be wrapped in ```python``` and include imports.\n",
        "        Think carefully before generating any code. These patterns can be simple or complex.  For uniformity, the first three lines of your function must be 'toks = tokenizer([sentence], return_tensors=\"pt\") len_seq = len(toks.input_ids[0]) out = np.zeros((len_seq, len_seq))'.\n",
        "        Make sure the token sequences from your tokenizer and spaCy (if you must use spaCy) are aligned via a dictionary if necessary, because they split text differently. Make sure you generalize your hypothesis pattern to any sentence. Functions can almost \n",
        "        always be expressed in fewer than 50 lines of code. As examples, it has been discovered one head is responsible for the complex task of dependency parsing. It's simplistic predicted pseudocode looks like: \n",
        "        {example_one}. Example 2: '''{example_two}''' Example 3: '''{example_three}'''. Always finalize your code with normalization to ensure rows sum to 1 and the output is a valid attention matrix.\n",
        "        \"\"\"\n",
        "\n",
        "# ----- AGENT GENERATES HYPOTHESIS ----- #\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a coding assistant well-versed in linguistics.\"}\n",
        "]\n",
        "conversation_history.append({\"role\": \"user\", \"content\": prompt_1})\n",
        "response_1 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", messages=conversation_history, temperature = 0.5\n",
        ")\n",
        "assistant_response_1 = response_1.choices[0].message.content\n",
        "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "\n",
        "# ----- AGENT WRITES PYTHON PROGRAM ----- #\n",
        "prompt_2 = \"Now, give me a Python function that uses a linguistic concept from your previous explanation.\"\n",
        "conversation_history.append({\"role\": \"user\", \"content\": prompt_2})\n",
        "response_2 = client.chat.completions.create(\n",
        "    model=\"gpt-5\",\n",
        "    messages=conversation_history\n",
        ")\n",
        "assistant_response_2 = response_2.choices[0].message.content\n",
        "print(f\"\\n--- Response 2 ---\\n{assistant_response_2}\")\n",
        "\n",
        "# ----- SAVE AGENT RESULTS ----- #\n",
        "folder = \"automation_results_gpt4o\"\n",
        "if not os.path.exists(folder): os.makedirs(folder)\n",
        "subfolders = [\"prompts\", \"llm_code\"]\n",
        "for subfolder in subfolders:\n",
        "    if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "        os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "with open(os.path.join(folder, \"prompts\", f\"layer{layer}_head{head}_prompts.txt\"), \"w\") as f:\n",
        "    f.write(f\"--- Response 1 ---\\n{assistant_response_1}\\n\\n--- Response 2 ---\\n{assistant_response_2}\")\n",
        "\n",
        "python_snippet = assistant_response_2.split(\"```python\")[1].split(\"```\")[0].strip()\n",
        "with open(os.path.join(folder, \"llm_code\", f\"layer{layer}_head{head}_code.py\"), \"w\") as f:\n",
        "    f.write(python_snippet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c1a30c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#helpers\n",
        "\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "def parse_model_output(output_text: str):\n",
        "    \"\"\"\n",
        "    Robustly parse model output with 'hypothesis' and 'program' keys.\n",
        "    Handles triple quotes, code fences, and inconsistent indentation.\n",
        "    Ignores extra text after the JSON object.\n",
        "    \"\"\"\n",
        "    # 1ï¸â£ Remove outer markdown fences\n",
        "    cleaned = re.sub(r\"^```(?:json)?|```$\", \"\", output_text.strip(), flags=re.MULTILINE).strip()\n",
        "\n",
        "    # 2ï¸â£ Handle malformed JSON with triple quotes\n",
        "    if '\"\"\"' in cleaned:\n",
        "        hyp_match = re.search(r'\"hypothesis\"\\s*:\\s*\"([^\"]+)\"', cleaned)\n",
        "        prog_match = re.search(r'\"program\"\\s*:\\s*(\"\"\"|```python|```)(.*?)(\\1|```)', cleaned, re.DOTALL)\n",
        "        if not hyp_match or not prog_match:\n",
        "            raise ValueError(\"Could not locate hypothesis or program block.\")\n",
        "        hypothesis = hyp_match.group(1).strip()\n",
        "        program_raw = prog_match.group(2)\n",
        "        program = textwrap.dedent(program_raw).strip()\n",
        "        program = program.replace('\\r\\n', '\\n')\n",
        "        return {\"hypothesis\": hypothesis, \"program\": program}\n",
        "\n",
        "    # 3ï¸â£ Parse valid JSON normally, but extract only the first JSON object\n",
        "    # This ignores any extra text after the first closing brace\n",
        "    json_match = re.search(r\"\\{.*?\\}\\s*(?=\\n|$)\", cleaned, flags=re.DOTALL)\n",
        "    if not json_match:\n",
        "        raise ValueError(\"No JSON object found in model output.\")\n",
        "\n",
        "    json_str = json_match.group(0)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Invalid JSON: {e}\\nExtracted text:\\n{json_str}\")\n",
        "\n",
        "    if not all(k in data for k in (\"hypothesis\", \"program\")):\n",
        "        raise KeyError(f\"Missing keys. Found: {list(data.keys())}\")\n",
        "\n",
        "    # Clean code block & indentation\n",
        "    program_raw = re.sub(r\"^```(?:python)?|```$\", \"\", data[\"program\"].strip(), flags=re.MULTILINE)\n",
        "    program = textwrap.dedent(program_raw).strip().replace('\\r\\n', '\\n')\n",
        "\n",
        "    return {\n",
        "        \"hypothesis\": data[\"hypothesis\"].strip(),\n",
        "        \"program\": program,\n",
        "    }\n",
        "\n",
        "import traceback\n",
        "import importlib.util\n",
        "import types\n",
        "def validate_program(program_path, model, tokenizer, layer, head, sentences):       \n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        spec.loader.exec_module(module)\n",
        "    except Exception as e:\n",
        "        print(f\"Program loading failed: {str(e)}\")\n",
        "        return str(e)\n",
        "\n",
        "    for attr_name in dir(module):\n",
        "        attr = getattr(module, attr_name)\n",
        "        if isinstance(attr, types.FunctionType):\n",
        "            program_to_test = attr\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        # print(\"Scoring program...\")\n",
        "        score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        error = traceback.format_exc()\n",
        "        full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "        return full_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aea170",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "#print name, num heads, num layers, \n",
        "print(model.config.architectures[0])\n",
        "print(model.config.num_attention_heads)\n",
        "print(model.config.num_hidden_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6ba30d",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "torch_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "torch_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "        if (layer, head) in [(0, 1), (0, 2), (0, 3), (0, 4), (0,5)]: continue\n",
        "\n",
        "        if sq_score[layer, head] < 0.7: continue\n",
        "        \n",
        "        # if (layer, head) not in fails: continue\n",
        "\n",
        "        success = False\n",
        "\n",
        "        print(f\"Layer {layer}, Head {head}\")\n",
        "        # max_attempts = 3  # Initial try + two more attempt \n",
        "        head_attempts = 0\n",
        "\n",
        "        # while head_attempts < max_attempts:\n",
        "        if success: break\n",
        "\n",
        "        try:\n",
        "            # from automation_helper import generate_prompt, validate_program\n",
        "            # print(\"got here\")\n",
        "            fullprompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "\n",
        "            conversation_history = [\n",
        "                {\"role\": \"system\",\n",
        "                \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "            ]\n",
        "            conversation_history.append({\"role\": \"user\", \"content\": fullprompt})\n",
        "            response_1 = client.chat.completions.create(\n",
        "                model=\"gpt-4o\", messages=conversation_history\n",
        "            )\n",
        "            assistant_response_1 = response_1.choices[0].message.content\n",
        "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "            print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "            try:\n",
        "                parsed = parse_model_output(assistant_response_1)\n",
        "            except:\n",
        "                continue\n",
        "            feedback = \"invalid_output\"\n",
        "\n",
        "            folder = \"automation_results_bert_2\"\n",
        "            if not os.path.exists(folder): os.makedirs(folder)\n",
        "            subfolders = [\"prompts\", \"llm_code\", \"scores\"]\n",
        "            for subfolder in subfolders:\n",
        "                if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "                    os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "            hypothesis_path = f\"{folder}/prompts/layer{layer}_head{head}_prompts.txt\"\n",
        "            with open(hypothesis_path, \"w\") as f: f.write(parsed[\"hypothesis\"])\n",
        "\n",
        "            python_path = f\"{folder}/llm_code/layer{layer}_head{head}_code.py\"\n",
        "            with open(python_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "\n",
        "            import traceback\n",
        "            import importlib.util\n",
        "            import types\n",
        "            def validate_program(program_path, model, tokenizer, layer, head, sentences):       \n",
        "                try:\n",
        "                    spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                    module = importlib.util.module_from_spec(spec)\n",
        "                    module.__dict__['np'] = np\n",
        "                    spec.loader.exec_module(module)\n",
        "                except Exception as e:\n",
        "                    print(f\"Program loading failed: {str(e)}\")\n",
        "                    return str(e)\n",
        "\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        program_to_test = attr\n",
        "                        break\n",
        "\n",
        "                try:\n",
        "                    # print(\"Scoring program...\")\n",
        "                    head_scores = []\n",
        "                    for sentence in sentences[:10]:\n",
        "                            score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "                            head_scores.append(score)\n",
        "                    score = np.mean(head_scores)\n",
        "                    return score\n",
        "                except Exception as e:\n",
        "                    error = traceback.format_exc()\n",
        "                    full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "                    return full_error\n",
        "\n",
        "            max_refinements = 2 # no refinements\n",
        "            while type(feedback) is str or feedback > 0.7:\n",
        "                if max_refinements >= 3: break\n",
        "                \n",
        "                feedback = validate_program(python_path, model, tokenizer, layer, head, sentences)\n",
        "                print(feedback)\n",
        "                if isinstance(feedback, np.float64) and feedback <= 0.7:\n",
        "                    with open(python_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "                    success = True\n",
        "                    break\n",
        "\n",
        "                if type(feedback) is str:\n",
        "                    conversation_history.append({\"role\": \"user\", \"content\": f\"The following error was encountered when running your code: {feedback}. Please fix your code and return json with two keys: 'hypothesis' and 'program', where hypothesis is the explanation of what you fixed and program is the updated program. Here is your previous code: {parsed['program']}\"})\n",
        "                else:\n",
        "                    conversation_history.append({\"role\": \"user\", \"content\": f\"The score of your program was {feedback:.2f}, which is not good enough. Please refine your code and return json with two keys: 'hypothesis' and 'program', where hypothesis is the explanation of what you fixed and program is the updated program. Here is your previous code: {parsed['program']}\"})\n",
        "                response_2 = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    messages=conversation_history\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    parsed = parse_model_output(response_2.choices[0].message.content)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "                    continue\n",
        "\n",
        "                with open(os.path.join(folder, \"prompts\", f\"layer{layer}_head{head}_prompts_try{max_refinements}.txt\"), \"w\") as f:\n",
        "                    f.write(parsed[\"hypothesis\"])\n",
        "                \n",
        "                max_refinements += 1\n",
        "\n",
        "            score = feedback\n",
        "            score_path = f\"{folder}/scores/layer{layer}_head{head}_score.txt\"\n",
        "            with open(score_path, \"w\") as f: f.write(str(score))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing Layer {layer}, Head {head}: {e}\")\n",
        "            head_attempts += 1\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e3efaf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# check that gpt_call works as expected\n",
        "\n",
        "prompt = generate_prompt(sentences[:25], model, tokenizer, (3, 9), top_k_ratio=0.025)\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "]\n",
        "conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", messages=conversation_history\n",
        ")\n",
        "assistant_response = response.choices[0].message.content\n",
        "print(assistant_response)\n",
        "try:\n",
        "    output = parse_model_output(assistant_response)\n",
        "except:\n",
        "    output = \"parsing_error\"\n",
        "    \n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac696c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gpt_call(prompt):\n",
        "    conversation_history = [\n",
        "        {\"role\": \"system\",\n",
        "        \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "    ]\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\", messages=conversation_history\n",
        "    )\n",
        "    assistant_response = response.choices[0].message.content\n",
        "    # print(assistant_response)\n",
        "    return assistant_response\n",
        "    \n",
        "def make_program_executable(program_str: str) -> Callable:\n",
        "    import importlib.util\n",
        "    import types\n",
        "    import tempfile\n",
        "    import os\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\") as temp_file:\n",
        "        temp_file.write(program_str.encode())\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"loaded_program\", temp_file_path)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "        module.__dict__['Optional'] = Optional\n",
        "        module.__dict__['Tuple'] = Tuple\n",
        "        module.__dict__['Callable'] = Callable\n",
        "        module.__dict__['spacy'] = spacy\n",
        "        spec.loader.exec_module(module)\n",
        "    finally:\n",
        "        os.remove(temp_file_path)\n",
        "\n",
        "    for attr_name in dir(module):\n",
        "        attr = getattr(module, attr_name)\n",
        "        if isinstance(attr, types.FunctionType):\n",
        "            return attr\n",
        "\n",
        "    raise ValueError(\"No function found in the provided program.\")\n",
        "\n",
        "def diff(program, model, tokenizer, sentences):\n",
        "    # take a short list (5 examples for each failed program) of the token pairs with the biggest difference between the actual activations and the hypothesis matrix.\n",
        "    # return a list of tuples (token1, token2, actual_score, predicted_score)\n",
        "    differences = []\n",
        "    # activation is a matrix i, j of actual attention scores\n",
        "    # program is a function that takes in a sentence and tokenizer and returns a matrix of predicted\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        activations = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().cpu().numpy()\n",
        "\n",
        "        predicted = program(sentence, tokenizer)[1]\n",
        "\n",
        "        seq_len = activations.shape[0]\n",
        "        for i in range(seq_len):\n",
        "            for j in range(seq_len):\n",
        "                actual_score = activations[i, j]\n",
        "                predicted_score = predicted[i, j]\n",
        "                token_i = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][i].item())\n",
        "                token_j = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][j].item())\n",
        "                differences.append([token_i, token_j, actual_score, predicted_score])\n",
        "\n",
        "    differences = sorted(differences, key=lambda x: abs(x[2] - x[3]), reverse=True)\n",
        "    differences = differences[:10]\n",
        "    actual_scores = [f\"{diff[2]:.4f}\" for diff in differences]\n",
        "    predicted_scores = [f\"{diff[3]:.4f}\" for diff in differences]\n",
        "    differences = [[diff[0], diff[1], actual, predicted] for diff, actual, predicted in zip(differences, actual_scores, predicted_scores)]\n",
        "    return differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05336f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------- GREEDY REFINEMENT PIPELINE ---------------\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "from automation_helper import generate_prompt\n",
        "\n",
        "save_path = \"automation_results_bert_greedy\"\n",
        "subdirectories = [\"llm_code\", \"scores\"]\n",
        "for subdir in subdirectories:\n",
        "    dir_path = os.path.join(save_path, subdir)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "        if (layer, head) in [(0,0), (0,1), (0,2), (0,3)]: continue\n",
        "        try:\n",
        "\n",
        "            score_1_path = os.path.join(\"automation_results_bert\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "            score_2_path = os.path.join(\"automation_results_bert_2\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "            score_1 = float(open(score_1_path).read()) if os.path.exists(score_1_path) else 1.0\n",
        "            score_2 = float(open(score_2_path).read()) if os.path.exists(score_2_path) else 1.0\n",
        "            if score_1 < 0.4 or score_2 < 0.4:\n",
        "                print(f\"Skipping Layer {layer}, Head {head} due to existing low score ({min(score_1, score_2):.2f})\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nAnalyzing Layer {layer}, Head {head} of [{model.config.architectures[0]} - {model_name}]\")\n",
        "            prompt = generate_prompt(sentences[:25], model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "            raw_response = gpt_call(prompt)\n",
        "            try:\n",
        "                assistant_response = parse_model_output(raw_response)\n",
        "            except:\n",
        "                max_retries = 0\n",
        "                while max_retries < 3:\n",
        "                    print(\"\\tParsing error encountered. Retrying...\")\n",
        "                    assistant_response = gpt_call(prompt)\n",
        "                    if assistant_response != \"parsing_error\": break\n",
        "                    max_retries += 1\n",
        "\n",
        "                if max_retries == 3: \n",
        "                    print(f\"\\tFailed to parse after {max_retries} attempts. Skipping Layer {layer}, Head {head}.\")\n",
        "                    continue\n",
        "            program = assistant_response['program']\n",
        "            program_path = os.path.join(save_path, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "            with open(program_path, \"w\") as f: f.write(program.rstrip(\"}\"))\n",
        "            feedback = validate_program(program_path, model, tokenizer, layer, head, sentences)\n",
        "            print(f\"\\tProgram parsed successfully. Feedback: {feedback}\")\n",
        "\n",
        "            retries = 0\n",
        "            failed_programs = []\n",
        "            while (isinstance(feedback, float) and feedback > 0.4) or (isinstance(feedback, str)) and retries < 10:\n",
        "                if isinstance(feedback, float) and feedback <= 0.4: break\n",
        "\n",
        "                retries += 1\n",
        "                refinement = \"\"\n",
        "                if isinstance(feedback, float):\n",
        "                    try:\n",
        "                        executable_program = make_program_executable(program)\n",
        "                        failed_programs.append(executable_program.__name__)\n",
        "                        tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "                        activations = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().cpu().numpy()\n",
        "                        top_differences = diff(executable_program, model, tokenizer, sentences[:25])\n",
        "                        refinement += f\"\\nPrevious failed programs: {failed_programs}. The top 10 differences in form (token1, token2, actual_score, predicted_score): {top_differences}\"\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "                \n",
        "                raw_response = gpt_call(prompt + refinement)\n",
        "                assistant_response = parse_model_output(raw_response)\n",
        "                try:\n",
        "                    program = assistant_response['program']\n",
        "                except Exception as e:\n",
        "                    print(f\"\\tParsing error on refinement {retries}: {e}\")\n",
        "                    feedback = \"parsing_error\"\n",
        "                    continue\n",
        "                program_path = os.path.join(save_path, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "                with open(program_path, \"w\") as f: f.write(program.rstrip(\"}\"))\n",
        "                feedback = validate_program(program_path, model, tokenizer, layer, head, sentences)\n",
        "\n",
        "            score = feedback\n",
        "            llm_code_path = os.path.join(save_path, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "            score_path = os.path.join(save_path, \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "            with open(llm_code_path, \"w\") as f: f.write(program.rstrip(\"}\"))\n",
        "            with open(score_path, \"w\") as f: f.write(str(score))\n",
        "            print(f\"\\tFinal score for Layer {layer}, Head {head}: {score}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\tError processing Layer {layer}, Head {head}: {e}\")\n",
        "            continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a9b0ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "differences = []\n",
        "    # activation is a matrix i, j of actual attention scores\n",
        "    # program is a function that takes in a sentence and tokenizer and returns a matrix of predicted\n",
        "\n",
        "for sentence in sentences[:25]:\n",
        "    tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    activations = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().cpu().numpy()\n",
        "    predicted = executable_program(sentence, tokenizer)[1]\n",
        "\n",
        "    seq_len = activations.shape[0]\n",
        "    for i in range(seq_len):\n",
        "        for j in range(seq_len):\n",
        "            actual_score = activations[i, j]\n",
        "            predicted_score = predicted[i, j]\n",
        "            token_i = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][i].item())\n",
        "            token_j = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][j].item())\n",
        "            differences.append([token_i, token_j, actual_score, predicted_score])\n",
        "\n",
        "differences = sorted(differences, key=lambda x: abs(x[2] - x[3]), reverse=True)\n",
        "differences = differences[:10]\n",
        "actual_scores = [f\"{diff[2]:.4f}\" for diff in differences]\n",
        "predicted_scores = [f\"{diff[3]:.4f}\" for diff in differences]\n",
        "differences = [[diff[0], diff[1], actual, predicted] for diff, actual, predicted in zip(differences, actual_scores, predicted_scores)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b357d9b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(seq_len)\n",
        "for i in range(seq_len):\n",
        "    print(\"-----\")\n",
        "    for j in range(seq_len):\n",
        "        print(activations[i,j])\n",
        "        # how to get the token corresponding to index i and j\n",
        "        print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][i].item()), tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][j].item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886ded6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(actual_score, predicted_score, diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ec0ce1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#print the code that makes up executable_program\n",
        "import inspect\n",
        "print(inspect.getsource(executable_program))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3e6325",
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate through scores folder and count how many are floats and how many are strings\n",
        "# edit the following code to count nan as a failure\n",
        "\n",
        "folder = r\"automation_results_gpt2\\scores\"\n",
        "float_count = 0\n",
        "string_count = 0\n",
        "nan_count = 0\n",
        "no_code_count = 0\n",
        "\n",
        "fails = []\n",
        "\n",
        "for i in range(12):\n",
        "    for j in range(12):\n",
        "        layer, head = i, j\n",
        "        if not os.path.exists(os.path.join(folder, f\"layer{layer}_head{head}_score.txt\")):\n",
        "            fails.append((layer, head))\n",
        "            no_code_count += 1\n",
        "\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\"_score.txt\"):\n",
        "        with open(os.path.join(folder, filename), \"r\") as f:\n",
        "            content = f.read().strip()\n",
        "            try:\n",
        "                value = float(content)\n",
        "                if value != value:  # Check for NaN\n",
        "                    nan_count += 1\n",
        "                    layer = int(filename.split(\"_\")[0][5:])\n",
        "                    head = int(filename.split(\"_\")[1][4:])\n",
        "                    fails.append((layer, head))\n",
        "                else:\n",
        "                    float_count += 1\n",
        "            except ValueError:\n",
        "                layer = int(filename.split(\"_\")[0][5:])\n",
        "                head = int(filename.split(\"_\")[1][4:])\n",
        "                fails.append((layer, head))\n",
        "                string_count += 1\n",
        "\n",
        "print(f\"Float scores: {float_count}\")\n",
        "print(f\"String scores: {string_count}\")\n",
        "print(f\"NaN scores: {nan_count}\")\n",
        "print(f\"No code scores: {no_code_count}\")\n",
        "print(f\"Total scores: {float_count + string_count + nan_count + no_code_count}\\n\\n\")\n",
        "print(fails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac782014",
      "metadata": {},
      "outputs": [],
      "source": [
        "# retest the bert scores using existing functions with 100 sentences for each head\n",
        "\n",
        "retest = [(11, 10), (4,11)]\n",
        "\n",
        "scores = []\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "\n",
        "        if (layer, head) not in retest: continue\n",
        "\n",
        "        print(f\"Layer {layer}, Head {head}\")\n",
        "        # find file in llm_code e.g. layer0_head0_code\n",
        "        program_path = f\"automation_results_bert/llm_code/layer{layer}_head{head}_code.py\"\n",
        "\n",
        "        try:\n",
        "            spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "        except Exception as e:\n",
        "            print(f\"Program loading failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        for attr_name in dir(module):\n",
        "            attr = getattr(module, attr_name)\n",
        "            if isinstance(attr, types.FunctionType):\n",
        "                program_to_test = attr\n",
        "                break\n",
        "\n",
        "        head_scores = []\n",
        "        for sentence in sentences[:100]:\n",
        "            try:\n",
        "                score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "                head_scores.append(score)\n",
        "            except Exception as e:\n",
        "                error = traceback.format_exc()\n",
        "                full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "                continue\n",
        "        \n",
        "        scores.append(np.mean(head_scores))\n",
        "        print(f\"Average Score for Layer {layer}, Head {head}: {np.mean(head_scores)}\")\n",
        "        # overwrite score file\n",
        "        score_path = f\"automation_results_bert/scores/layer{layer}_head{head}_score.txt\"\n",
        "        with open(score_path, \"w\") as f: \n",
        "            f.write(str(np.mean(head_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84555c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# folder = r\"automation_results_bert\\scores\"\n",
        "# scores = []\n",
        "\n",
        "# for filename in os.listdir(folder):\n",
        "#     if filename.endswith(\"_score.txt\"):\n",
        "#         with open(os.path.join(folder, filename), \"r\") as f:\n",
        "#             content = f.read().strip()\n",
        "#             try:\n",
        "#                 value = float(content)\n",
        "#                 scores.append(round(value, 3))\n",
        "#             except ValueError:\n",
        "#                 continue\n",
        "\n",
        "# scores = np.array(scores, dtype=float)\n",
        "# scores[np.isnan(scores)] = np.mean(scores[~np.isnan(scores)])\n",
        "\n",
        "# def find_outliers(data):\n",
        "#     data = np.array(data)\n",
        "#     Q1 = np.percentile(data, 25)\n",
        "#     Q3 = np.percentile(data, 75)\n",
        "\n",
        "#     IQR = Q3 - Q1\n",
        "#     lower_bound = Q1 - (1.5 * IQR)\n",
        "#     upper_bound = Q3 + (1.5 * IQR)\n",
        "\n",
        "#     outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "#     return outliers, lower_bound, upper_bound, Q1, Q3, IQR\n",
        "\n",
        "# outliers, lower_bound, upper_bound, Q1, Q3, IQR = find_outliers(scores)\n",
        "\n",
        "def plot_scores_boxplot(scores):\n",
        "    plt.figure(figsize=(3.2,8))\n",
        "    plt.boxplot(\n",
        "        scores,\n",
        "        positions=[0.75], \n",
        "        vert=True,\n",
        "        patch_artist=True,\n",
        "        medianprops={'color': 'black', 'linewidth': 3},\n",
        "        boxprops={'facecolor': 'gray', 'edgecolor': 'black'},\n",
        "        flierprops={'marker': 'D', 'markerfacecolor': 'black', 'markersize': 3, 'linestyle': 'none'}\n",
        "    )\n",
        "\n",
        "    # plt.title(f'Auto K=N | GPT2\\nMean={np.mean(scores):.2f}\\n\\nLasso (alpha=0.001)', fontsize=14, weight='bold')\n",
        "    plt.title(f'Auto K=N | GPT \\n\\nRandom Token Baseline\\navg_score={np.mean(scores):.2f}', fontsize=14, weight='bold')\n",
        "    plt.ylabel('Automation Scores', fontsize=12)\n",
        "    plt.xticks([])\n",
        "    plt.ylim(0, 1.05)\n",
        "\n",
        "    #insert the text 'WIP' in center of plot\n",
        "    # plt.text(0.75, 0.5, 'WIP', fontsize=12, ha='center', va='center')\n",
        "\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "    x = np.ones_like(scores)\n",
        "    plt.scatter(\n",
        "        x,\n",
        "        scores,\n",
        "        color='gray',\n",
        "        edgecolor='black',\n",
        "        s=30,\n",
        "        alpha=0.9,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "# scores = np.load(\"data/interpolation_experiment_gpt2_lasso_a=0.001_k=N.npy\")\n",
        "scores_flat = np.array(scores).flatten()\n",
        "plot_scores_boxplot(scores_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b858c889",
      "metadata": {},
      "outputs": [],
      "source": [
        "# loop through \"C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\automation_refinement_gpt2\\master_list\"\n",
        "# if python file is empty or only has new lines, delete that file\n",
        "\n",
        "folder = r\"C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\automation_refinement_gpt2\\master_list\"\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\".py\"):\n",
        "        file_path = os.path.join(folder, filename)\n",
        "        if os.path.getsize(file_path) == 0 or all(line.strip() == \"\" for line in open(file_path)):\n",
        "            os.remove(file_path)\n",
        "            print(f\"Deleted empty file: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76451682",
      "metadata": {},
      "outputs": [],
      "source": [
        "from programs import *\n",
        "patterns = [next_attention, previous_attention, same_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "number_of_lines = []\n",
        "# first, count number of lines in existing patterns\n",
        "import inspect as i\n",
        "for pattern in patterns:\n",
        "    definition = i.getsource(pattern)\n",
        "    number_of_lines.append(len(definition.strip().split('\\n')))\n",
        "\n",
        "master_list_dir = \"automation_refinement/master_list\"\n",
        "for filename in os.listdir(master_list_dir):\n",
        "    if filename.endswith(\".py\"):\n",
        "        module_name = filename[:-3]\n",
        "        if module_name not in [p.__name__ for p in patterns]:\n",
        "            try:\n",
        "                program_path = os.path.join(master_list_dir, filename)\n",
        "                spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                module.__dict__['np'] = np\n",
        "                # module.__dict__['torch'] = torch\n",
        "                module.__dict__['spacy'] = spacy\n",
        "                module.__dict__['nlp'] = nlp\n",
        "                module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "                module.__dict__['Optional'] = Optional\n",
        "                module.__dict__['Tuple'] = Tuple\n",
        "                module.__dict__['Callable'] = Callable\n",
        "                module.__dict__['re'] = re\n",
        "                spec.loader.exec_module(module)\n",
        "            except Exception as e:\n",
        "                print(f\"Program loading failed: {str(e)}\")\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    program_to_test = attr\n",
        "                    break\n",
        "            \n",
        "            # count number of lines in program_to_test and append\n",
        "            with open(program_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                number_of_lines.append(len(lines))\n",
        "\n",
        "            patterns.append(program_to_test)\n",
        "\n",
        "print(f\"Total patterns to analyze: {len(patterns)}\")\n",
        "for pattern in patterns: print(f\"\\tPattern: {pattern.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "befd99ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "interpolation_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce24b5d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e40e6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------- LINEAR INTERPOLATION REFINEMENT PIPELINE ---------------\n",
        "import inspect\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "torch_model = model\n",
        "torch_tokenizer = tokenizer\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "\n",
        "interpolation_matrix = np.zeros((num_layers, num_heads))\n",
        "# patterns = [previous_attention, same_attention, punctuation_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, special_token_attention, pos_alignment, relative_position_attention]\n",
        "\n",
        "folder = r\"automation_refinement_gpt2_try2\"\n",
        "subfolders = [\"master_list\", \"candidate\"]\n",
        "if not os.path.exists(folder): os.makedirs(folder)\n",
        "for subfolder in subfolders:\n",
        "    if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "        os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "for pattern in patterns:\n",
        "    pattern_name = pattern.__name__\n",
        "    with open(os.path.join(folder, \"master_list\", f\"{pattern_name}.py\"), \"w\") as f:\n",
        "        f.write(inspect.getsource(pattern))\n",
        "\n",
        "new_patterns = 0\n",
        "total_loops = 0\n",
        "while new_patterns < 30 and total_loops < 100:\n",
        "    try:\n",
        "        total_loops += 1\n",
        "        print(f\"Searching for new patterns, iteration {new_patterns} | total loops {total_loops}...\")\n",
        "\n",
        "        # get interpolation scores for all heads\n",
        "        print(f\"\\tCalculating interpolation scores for all heads...\")\n",
        "        for layer in range(num_layers):\n",
        "            if layer % 3 == 0: print(f\"\\t\\tLayer {layer}/{num_layers} complete\")\n",
        "            for head in range(num_heads):\n",
        "                sentence = str(np.random.choice(sentences))\n",
        "\n",
        "                X = []\n",
        "                for pattern in patterns:\n",
        "                    try:\n",
        "                        instance = pattern(sentence, tokenizer)[1].flatten()\n",
        "                        # check if nan in instance:\n",
        "                        if np.isnan(instance).any():\n",
        "                            del patterns[patterns.index(pattern)]\n",
        "                            print(f\"\\t\\tRemoved pattern {pattern.__name__} due to NaN values.\")\n",
        "                            continue\n",
        "                        # print(f\"Tested pattern: {pattern.__name__} successfully\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # print(f\"Error testing pattern {pattern.__name__}: {e}\")\n",
        "                        print(f\"\\t\\tRemoved pattern {pattern.__name__} due to error during testing.\")\n",
        "                        del patterns[patterns.index(pattern)]\n",
        "                        continue\n",
        "\n",
        "                    X.append(instance)\n",
        "                X_n = np.nan_to_num(np.array(X).T)\n",
        "                y = np.nan_to_num(model(**tokenizer(sentence, return_tensors=\"pt\"), output_attentions=True).attentions[layer][0, head].detach().numpy().flatten())\n",
        "\n",
        "                reg = LinearRegression().fit(X_n, y.flatten())\n",
        "                out = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "                len_seq = len(tokenizer([sentence], return_tensors=\"pt\").input_ids[0])\n",
        "                out = out.reshape((len_seq, len_seq))\n",
        "                pred_att = out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "                tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "                att = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "                jensonshannon_distances = []\n",
        "                for row_att, row_out in zip(att, pred_att):\n",
        "                    jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "                interpolation_score = np.mean(jensonshannon_distances)\n",
        "\n",
        "                interpolation_matrix[layer, head] = interpolation_score\n",
        "                # print(f\"Layer {layer}, Head {head} - Interpolation Score: {interpolation_score:.4f}\")\n",
        "\n",
        "        # select worst 20% of heads based on interpolation score\n",
        "        sorted_indices = np.dstack(np.unravel_index(np.argsort(interpolation_matrix.ravel()), interpolation_matrix.shape))[0]\n",
        "        num_to_select = int(0.1 * num_layers * num_heads)\n",
        "        worst_indices = sorted_indices[-num_to_select:]\n",
        "\n",
        "        # generate candidate programs for each of the worst heads\n",
        "        candidate_scores = []\n",
        "        candidate_paths = []\n",
        "        for layer, head in worst_indices[::-1]: # start with worst head\n",
        "            layer, head = int(layer), int(head)\n",
        "            print(f\"\\t TRYING | Layer {layer}, Head {head} - Previous Interpolation Score: {interpolation_matrix[layer, head]:.4f}\")\n",
        "\n",
        "            try:\n",
        "                fullprompt = generate_prompt(generic_sentences[:25], model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "                refinement_part = \"It's already been found that this head doesn't do well on any of the following patterns: \"\n",
        "                refinement_part += \", \".join([pattern.__name__ for pattern in patterns])\n",
        "                refinement_part += \". So don't suggest any of those patterns again.\"\n",
        "                fullprompt += \" \".join(refinement_part)\n",
        "\n",
        "                conversation_history = [\n",
        "                    {\"role\": \"system\",\n",
        "                    \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "                ]\n",
        "                conversation_history.append({\"role\": \"user\", \"content\": fullprompt})\n",
        "                response_1 = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\", messages=conversation_history\n",
        "                )\n",
        "                assistant_response_1 = response_1.choices[0].message.content\n",
        "                conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "                # print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "                try:\n",
        "                    parsed = parse_model_output(assistant_response_1)\n",
        "                except:\n",
        "                    print(f\"\\t\\tError parsing response, skipping head ({layer}, {head})\")\n",
        "                    continue\n",
        "                feedback = \"invalid_output\"\n",
        "\n",
        "                # print(f\"\\t\\t{parsed}\")\n",
        "                print(f\"\\t\\tHypothesis: {parsed['hypothesis']}. Program successfully parsed.\")\n",
        "\n",
        "                candidate_path = f\"{folder}/candidate/layer{layer}_head{head}_code.py\"\n",
        "                with open(candidate_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "\n",
        "                feedback = validate_program(candidate_path, model, tokenizer, layer, head, sentences)\n",
        "                if isinstance(feedback, np.float64) and not np.isnan(feedback):\n",
        "                    candidate_scores.append(feedback)\n",
        "                    candidate_paths.append(candidate_path)\n",
        "                    print(f\"\\t\\tProgram valid with score {feedback:.4f}.\")\n",
        "                    continue\n",
        "                    \n",
        "                else:\n",
        "                    candidate_scores.append(100)\n",
        "                    candidate_paths.append(\"\")\n",
        "                    # print(f\"\\t\\tInitial program invalid w/ error: ({feedback}), skipping.\")\n",
        "                    continue\n",
        "            \n",
        "            except Exception as e:\n",
        "                total_loops += 1\n",
        "                candidate_scores.append(100)\n",
        "                candidate_paths.append(\"\")\n",
        "                # print(f\"\\t\\tError processing Layer {layer}, Head {head}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # select and save best candidate program to master list\n",
        "        best_candidate = candidate_paths[np.argmin(candidate_scores)]\n",
        "        best_score = np.min(candidate_scores)\n",
        "        new_program = make_program_executable(open(best_candidate).read())\n",
        "        \n",
        "        print(f\"Best candidate program {new_program.__name__} added | solo score {best_score:.4f}.\")\n",
        "\n",
        "        if best_score < 0.45:\n",
        "            spec = importlib.util.spec_from_file_location(\"loaded_program\", best_candidate)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    new_program = attr\n",
        "\n",
        "            new_pattern_name = new_program.__name__\n",
        "            with open(os.path.join(folder, \"master_list\", f\"{new_pattern_name}.py\"), \"w\") as f:\n",
        "                f.write(inspect.getsource(new_program))\n",
        "\n",
        "            patterns.append(new_program)\n",
        "            new_patterns += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        total_loops += 1\n",
        "        print(f\"Error in main loop: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def0137e",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_candidate = r\"automation_refinement_gpt2_try2/candidate/layer2_head2_code.py\"\n",
        "make_program_executable(open(best_candidate).read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659ac533",
      "metadata": {},
      "outputs": [],
      "source": [
        "for pattern in patterns:\n",
        "    try:\n",
        "        instance = pattern(sentence, tokenizer)[1].flatten()\n",
        "        # check if nan in instance:\n",
        "        if np.isnan(instance).any():\n",
        "            del patterns[patterns.index(pattern)]\n",
        "            print(f\"\\t\\tRemoved pattern {pattern.__name__} due to NaN values.\")\n",
        "            continue\n",
        "        print(f\"Tested pattern: {pattern.__name__} successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error testing pattern {pattern.__name__}: {e}\")\n",
        "        del patterns[patterns.index(pattern)]\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488ac196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get all programs from automation_refinement/master_list and load these python functions as patterns = [executable functions]\n",
        "\n",
        "import os\n",
        "import importlib.util\n",
        "import types\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
        "from typing import Optional, Tuple, Callable\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "folder = \"automation_refinement_gpt2/master_list\"\n",
        "patterns = []\n",
        "\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\".py\"):\n",
        "        code_path = os.path.join(folder, filename)\n",
        "        spec = importlib.util.spec_from_file_location(f\"module_{filename[:-3]}\", code_path)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        # get pretrainedtokenizerbase, from typing import Optional, Tuple, Callable\n",
        "        module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "        module.__dict__['Optional'] = Optional\n",
        "        module.__dict__['Tuple'] = Tuple\n",
        "        module.__dict__['Callable'] = Callable\n",
        "        module.__dict__['spacy'] = spacy\n",
        "        \n",
        "        try:\n",
        "            spec.loader.exec_module(module)\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    patterns.append(attr)\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading program from {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "print(f\"Loaded {len(patterns)} patterns.\")\n",
        "for i, prog in enumerate(patterns):\n",
        "    print(f\"{i}: {prog.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890adf84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# go through patterns and check if they need nlp or defaultdict, if they don't need nlp library from spacy, add to patterns_strong\n",
        "patterns_strong = []\n",
        "for pattern in patterns:\n",
        "    if not any(kw in pattern.__code__.co_names for kw in ['nlp', 'spacy', 'defaultdict']):\n",
        "        patterns_strong.append(pattern)\n",
        "print(f\"Filtered to {len(patterns_strong)} strong patterns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6879e81f",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sentences = sentences[135:200]\n",
        "# gpt2\n",
        "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "scores = np.zeros((model.config.num_hidden_layers, model.config.num_attention_heads))\n",
        "top_k = 3\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    for head in range(num_heads):\n",
        "        print(f\"Analyzing Layer {layer}, Head {head}...\")\n",
        "        sentence_scores = []\n",
        "        for sentence in test_sentences:\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            attention = outputs.attentions[layer][0, head].detach().numpy()\n",
        "            y = attention.flatten()\n",
        "\n",
        "            X = []\n",
        "            for pattern in patterns_strong:\n",
        "                X.append(pattern(sentence, tokenizer)[1].flatten())\n",
        "            X_n = np.array(X).T\n",
        "            y = y.flatten()\n",
        "\n",
        "            # avoid ValueError: Input X contains NaN.\n",
        "            X_n = np.nan_to_num(X_n)\n",
        "            y = np.nan_to_num(y)\n",
        "\n",
        "            reg = LinearRegression().fit(X_n, y)\n",
        "            side_length = int(np.sqrt(len(y)))\n",
        "            y = y.reshape((side_length, side_length))\n",
        "\n",
        "            top_indices = np.argsort(np.abs(reg.coef_))[-top_k:]\n",
        "            pred_att = reg.intercept_ + sum(reg.coef_[i] * X[i] for i in top_indices)\n",
        "            pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "            if top_k == 1:\n",
        "                #pred_att is just the single pattern with highest coef, it isn't equal to a sum at all\n",
        "                fn_highest_coeff = patterns_strong[np.argmax(np.abs(reg.coef_))]\n",
        "                pred_att = fn_highest_coeff(sentence, tokenizer)[1]\n",
        "                print(f\"\\tUsing pattern: {fn_highest_coeff.__name__}\")\n",
        "\n",
        "            jensonshannon_distances = []\n",
        "            for row_att, row_out in zip(y, pred_att):\n",
        "                jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "            score = np.mean(jensonshannon_distances)\n",
        "            sentence_scores.append(score)\n",
        "        \n",
        "        scores[layer, head] = np.mean(sentence_scores)\n",
        "        print(f\"\\tScore for Layer {layer}, Head {head}: {scores[layer, head]}\")\n",
        "\n",
        "np.save(\"gpt2_k1_automation_scores.npy\", scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5396e98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONDUCT MODEL LEVEL ANALYSIS / GET SUMMARY SCORE FOR WHOLE MODEL\n",
        "\n",
        "def classify_model(method, sentences, torch_model, torch_tokenizer):\n",
        "    if method == \"linear_fit\":\n",
        "        patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment]\n",
        "    elif method == \"best_fit\":\n",
        "        saved_file = pd.read_csv('data/best_fit_t5.csv')\n",
        "    elif method == \"automation\":\n",
        "        from automation_helper import generate_prompt, parse_llm_idea, validate_program\n",
        "\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    all_scores = []\n",
        "    final_scores = []\n",
        "    \n",
        "    if method != \"automation\":\n",
        "        for idx, sentence in enumerate(sentences):\n",
        "            scores = np.zeros((num_layers, num_heads))\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    layer, head = i, j\n",
        "                    inputs = torch_tokenizer(sentence, return_tensors=\"pt\")\n",
        "                    len_seq = len(torch_tokenizer([sentence], return_tensors=\"pt\").input_ids[0])\n",
        "\n",
        "                    X = []\n",
        "                    # y =  torch_model(**inputs, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "                    decoder_input_ids = tokens[\"input_ids\"]\n",
        "                    outputs = torch_model(input_ids=inputs[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "                    y = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "                    if method == \"random_baseline\":\n",
        "                        pred_att = np.zeros((len_seq, len_seq))\n",
        "                        pred_att[:, -1] = 1.0\n",
        "\n",
        "                    elif method == \"best_fit\":\n",
        "                        matching_rows = saved_file[(saved_file['i'] == i) & (saved_file['j'] == j)]\n",
        "                        if not matching_rows.empty:\n",
        "                            best_pattern = matching_rows.loc[matching_rows['Score'].idxmax(), 'Pattern']\n",
        "                            func = globals()[best_pattern]\n",
        "                            _, pred_att = func(sentence, tokenizer)\n",
        "                        else:\n",
        "                            out = np.random.rand(len_seq, len_seq)\n",
        "                            pred_att =  out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "                    elif method == \"linear_fit\":\n",
        "                        for pattern in patterns:\n",
        "                            X.append(pattern(sentence, torch_tokenizer)[1].flatten())\n",
        "                        X_n = np.array(X).T\n",
        "                        y = y.flatten()\n",
        "\n",
        "                        reg = LinearRegression().fit(X_n, y)\n",
        "                        side_length = int(np.sqrt(len(y)))\n",
        "                        y = y.reshape((side_length, side_length))\n",
        "\n",
        "                        pred_att = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "                        pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "                    jensonshannon_distances = []\n",
        "                    for row_att, row_out in zip(y, pred_att):\n",
        "                        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "                    score = np.mean(jensonshannon_distances)\n",
        "                    scores[layer, head] = score\n",
        "                    \n",
        "            all_scores.append(scores)\n",
        "            final_scores.append(np.sum(scores))\n",
        "            print(f\"Processed sentence #{idx}/{len(sentences)}: Score: {np.sum(scores):.2f}\\n\\t->'{sentence}'\")\n",
        "        \n",
        "    elif method == \"automation\":\n",
        "        \n",
        "        prompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "        hypothesis, program = parse_llm_idea(prompt, config=config, verbalize=False)\n",
        "        python_path = f\"{program_path}/{head}_output.py\"\n",
        "        feedback = validate_program(python_path, model, tokenizer, layer, head, sentences)\n",
        "        scores[layer, head] = feedback\n",
        "            \n",
        "\n",
        "    print(f\"Final Score: {sum(final_scores) / len(final_scores)}\")\n",
        "    return all_scores, final_scores\n",
        "\n",
        "methods = [\"random_baseline\", \"best_fit\", \"linear_fit\"]\n",
        "final_scores = {\"random_baseline\": 0, \"best_fit\": 0, \"linear_fit\": 0}\n",
        "\n",
        "for method in methods:\n",
        "    print(f\"\\nAnalyzing method: {method}\")\n",
        "    _, scores = classify_model(method, sentences[:10], model, tokenizer)\n",
        "    final_scores[method] = np.mean(scores)\n",
        "\n",
        "print(\"\\n\",final_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0989da5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PLOT DIFFERENT SUMMARY SCORES FOR THE MODEL\n",
        "\n",
        "max_score = model.config.num_hidden_layers * model.config.num_attention_heads\n",
        "raw_scores = [111, 92, 62, 65, 56]\n",
        "labels = ['Random \\nToken Baseline', 'Automatic\\nPrograms', 'K=1Refined\\nPrograms', 'Best Fit\\nPrograms', 'Linear Weight\\nPrograms']\n",
        "colors = ['darkred', 'darkblue', '#6aa84f', '#800080']\n",
        "\n",
        "# Normalize scores: lower scores become higher bars\n",
        "scores = [(score / max_score) for score in raw_scores]\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "bars = plt.bar(labels, scores, color=colors, width=0.6)\n",
        "\n",
        "# Add text labels on top of bars\n",
        "for bar, raw, norm in zip(bars, raw_scores, scores):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
        "             f'{norm:.2f}\\n[ {int(raw)} / {max_score} ]', ha='center', va='bottom', fontsize=14)\n",
        "ax.set_facecolor('#F5F5F5')\n",
        "\n",
        "plt.ylim(0, 1.0)\n",
        "# plt.title('Normalized Error (1 - Score / Max Score)')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.text(plt.xlim()[0]-0.7, plt.ylim()[1]+0.05, '[bad hypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
        "plt.text(plt.xlim()[0]-0.7, plt.ylim()[0]-0.13, '[well-fitting\\nhypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
        "plt.ylabel('Normalized Model Scores', fontsize=16, labelpad=20)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fdf02a",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Program Automation Efforts\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5510c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# AUTOMATICALLY GENERATE PATTERNS FOR A HEAD & GENERATE LLM PROMPT\n",
        "\n",
        "example_program_one = \"\"\"\n",
        "def dependencies(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
        "    len_seq = len(toks.input_ids[0]) /\n",
        "    out = np.zeros((len_seq, len_seq)) /\n",
        "    words = sentence.split() /\n",
        "    doc = nlp(\" \".join(words)) /\n",
        "    for stok in doc: /\n",
        "        parent_index = stok.i /\n",
        "        for child_stok in stok.children: /\n",
        "            child_index = child_stok.i /\n",
        "            out[parent_index+1, child_index+1] = 1 /\n",
        "            out[child_index+1, parent_index+1] = 1 /\n",
        "    out[0, 0] = 1 /\n",
        "    out[-1, 0] = 1 /\n",
        "    out += 1e-4 /\n",
        "    out = out / out.sum(axis=1, keepdims=True) /\n",
        "    return \"Dependency Parsing Pattern\", out /\n",
        "\"\"\"\n",
        "example_program_two = \"\"\"\n",
        "def same_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Same Token Pattern\", out\n",
        "\"\"\"\n",
        "example_program_three = \"\"\"\n",
        "def pos_alignment(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
        "    len_seq = len(toks.input_ids[0]) /\n",
        "    out = np.zeros((len_seq, len_seq)) /\n",
        "    # assign toks, input_ids, word_ids, len_seq, out, doc /\n",
        "    # use spacey to get pos_tags for tokens in docs [token.pos_ for token in doc] /\n",
        "    # for token in pos_tags: /\n",
        "    # loop through pos_tags and increment out[i,j] when pos_tags match /\n",
        "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention /\n",
        "    # Normalize out matrix by row (results in uniform attention) and return out /\n",
        "    # return 'Part of Speech Implementation 1', out /\n",
        "\"\"\"\n",
        "\n",
        "def generate_prompt(sentences, model, tokenizer, head_loc, top_k_ratio=0.1):\n",
        "    layer, head = head_loc\n",
        "    data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model.config.architectures[0],\n",
        "        \"examples\": []\n",
        "    }\n",
        "\n",
        "    def handle_score(score):\n",
        "        # convert to percentage with 0 decimal places\n",
        "        return \"{:.0f}\".format(score * 100)\n",
        "        \n",
        "    def scrape_head(att, tokens, top_k_ratio, ignore_special=True):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append(f\"[{str(tokens[i])}|{str(tokens[j])}:{handle_score(score)}]\")\n",
        "        top_activations_str = \" \".join(top_activations).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "        return top_activations_str\n",
        "    \n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=top_k_ratio)\n",
        "        item = {f\"sentence {idx}\": \" \".join(tokens), \"sentence attention\": top_activations}\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "    data = json.dumps(data, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    Using the following pieces of data based on {len(sentences)} sentences, generate three hypothesises about the linguistic role the following head is responsible for based on patterns\n",
        "    in the {model.config.architectures[0]} activations.  Then, choose the most fitting hypothesis for the head function using examples from the data. Finally, using the linguistic hypothesis you determine, \n",
        "    write a python function which takes in a sentence and tokenizer as parameters and outputs the name of the pattern you hypothesize along with a predicted_matrix (size: token_len * token_len), which is the \n",
        "    rule encoded matrix mirroring attention patterns you'd predict for any given sentence for Layer {layer}, Head {head}. Feel free to encode complex functions but write the simplest algorithm that captures your \n",
        "    observed pattern. You must respond to this prompt in JSON in the form \"{{\"hypothesis\": \"...\", \"program\": \"...\"}} with your chosen hypothesis. Think carefully before generating any code.\n",
        "    The first portion of your response has key \"hypothesis\" with the title of the hypothesis and the second portion of your response with key \"program\" should have valid python code starting with ```python and including imports. These patterns can be simple or \n",
        "    complex.  For uniformity, the first three lines of your function should be 'toks = tokenizer([sentence], return_tensors=\"pt\") len_seq = len(toks.input_ids[0]) out = np.zeros((len_seq, len_seq))'.\n",
        "    Make sure the token sequences from your tokenizer and spaCy (if you must use spaCy) are aligned via a dictionary if necessary, because they split text differently. Make sure you generalize your hypothesis pattern to any sentence. Functions can almost \n",
        "    always be expressed in fewer than 50 lines of code. As examples, it has been discovered one head is responsible for the complex task of dependency parsing. It's simplistic predicted pseudocode looks like: \n",
        "    {example_program_one}. Example 2: '''{example_program_two}''' Example 3: '''{example_program_three}'''. DATA: {data}\"\"\"\n",
        "    return ' '.join(prompt.strip().split())\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "layer, head = 5, 7\n",
        "prompt = generate_prompt(generic_sentences[:25], model, tokenizer, (layer, head), 0.025)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5704e094",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using Gemini, GPT-4o, Claude, Deepseek\n",
        "# API needs long contexts and free access\n",
        "# Source to get API keys is \"usage\" key\n",
        "\n",
        "load_dotenv()\n",
        "API_CONFIGS = {\n",
        "    \"gemini\": {\n",
        "        \"model\": \"gemini\",\n",
        "        \"url\": \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\",\n",
        "        \"key\": os.getenv(\"GEMINI\"),\n",
        "        \"headers_fn\": lambda key: {\"Content-Type\": \"application/json\", \"X-goog-api-key\": key},\n",
        "        \"payload_fn\": lambda prompt: {\n",
        "            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "            \"generationConfig\": {\"response_mime_type\": \"application/json\"}\n",
        "        },\n",
        "        \"usage\": \"https://aistudio.google.com/apikey\"\n",
        "    },\n",
        "    \"openai\": {\n",
        "        \"model\": \"openai\",\n",
        "        \"url\": \"https://api.openai.com/v1/responses\",\n",
        "        \"key\": os.getenv(\"OPENAI\"),\n",
        "        \"headers_fn\": lambda key: {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\": \"gpt-4.1\", \"input\": prompt},\n",
        "        \"usage\": \"https://platform.openai.com/account/api-keys\"\n",
        "    },\n",
        "    \"claude\": {\n",
        "        \"model\": \"claude\",\n",
        "        \"url\": \"https://api.anthropic.com/v1/messages\",\n",
        "        \"key\": os.getenv(\"CLAUDE\"),\n",
        "        \"headers_fn\": lambda key: {\"x-api-key\": key, \"Content-Type\": \"application/json\", \"Anthropic-Version\":\"2023-06-01\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\":\"claude-sonnet-4-20250514\", \"messages\":[{\"role\":\"user\",\"content\":prompt}]},\n",
        "        \"usage\": \"https://platform.claude.com/api_keys\"\n",
        "    },\n",
        "    \"deepseek\": {\n",
        "        \"model\": \"deepseek\",\n",
        "        \"url\": \"https://api.deepseek.com/chat/completions\",\n",
        "        \"key\": os.getenv(\"DEEPSEEK\"),\n",
        "        \"headers_fn\": lambda key: {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\": \"deepseek-chat\", \"input\": prompt, \"max_tokens\": 1000},\n",
        "        \"usage\": \"https://platform.deepseek.com/api_keys\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2112fba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAVE AUTOMATED HYPOTHESIS + VALIDATE GENERATED PROGRAM SYNTHESIS CODE\n",
        "\n",
        "def parse_llm_idea(prompt, config=\"YOUR_API_CONFIG\", verbalize=True):\n",
        "    def make_request():\n",
        "        headers = config[\"headers_fn\"](config[\"key\"])\n",
        "        payload = config[\"payload_fn\"](prompt)\n",
        "        response = requests.post(config[\"url\"], headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if config[\"model\"] == \"gemini\":\n",
        "            data = response.json()\n",
        "            output = data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        if config[\"model\"] == \"openai\":\n",
        "            pass\n",
        "        if config[\"model\"] == \"claude\":\n",
        "            data = response.json()\n",
        "            output = data[\"content\"][\"text\"]\n",
        "        if config[\"model\"] == \"deepseek\":\n",
        "            pass\n",
        "\n",
        "        return output\n",
        "    \n",
        "    output = make_request()\n",
        "\n",
        "    try:\n",
        "        result = json.loads(output)\n",
        "\n",
        "        if type(result) is list: result = result[0]\n",
        "        hypothesis = result.get(\"hypothesis\", \"\")\n",
        "        program = result.get(\"program\", \"\")\n",
        "\n",
        "        if program.startswith(\"```python\"): program = program[9:]\n",
        "        if program.endswith(\"```\"): program = program[:-3]\n",
        "        program = program.strip()\n",
        "\n",
        "        if verbalize: print(\"Hypothesis, Explanation & Program successfully parsed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Parsing API failed: {str(e)}\")\n",
        "        return str(e)\n",
        "\n",
        "    return hypothesis, program\n",
        "\n",
        "config = API_CONFIGS[\"gemini\"] \n",
        "parse_llm_idea(prompt, config=config, verbalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be7aad1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SIMPLE AUTOMATION OF PIPELINE FOR ANALYZING ALL HEADS & WRITING/SAVING PROGRAMS\n",
        "\n",
        "def automation_pipeline(model, tokenizer, sentences, API_KEY, save_data=True, evaluate=False):\n",
        "    heads = model.config.num_attention_heads\n",
        "    layers = model.config.num_hidden_layers\n",
        "    prompts, programs = [], []\n",
        "\n",
        "    for layer in range(layers):\n",
        "        # if layer == 0: continue\n",
        "        if save_data:\n",
        "            # save prompts:\n",
        "            prompt_path = f\"automation_2/prompts/{layer}/\"\n",
        "            os.makedirs(prompt_path, exist_ok=True)\n",
        "\n",
        "            # save programs:\n",
        "            program_path = f\"automation_2/llm_code/code_layer_{layer}/\"\n",
        "            os.makedirs(program_path, exist_ok=True)\n",
        "\n",
        "            # save scores:\n",
        "            if evaluate:\n",
        "                score_path = f\"automation_2/scores/{layer}/\"\n",
        "                os.makedirs(score_path, exist_ok=True)\n",
        "\n",
        "        for head in range(heads):\n",
        "            # if head < 9: continue\n",
        "            if (layer, head) not in failed_programs:\n",
        "                continue\n",
        "            prompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.1)\n",
        "            hypothesis, explanation, program = parse_llm_idea(prompt, API_KEY, output=False)\n",
        "            print(f\"Analyzed Layer {layer}, Head {head} | Hypothesis ~ {hypothesis} \")\n",
        "\n",
        "            prompts.append(prompt)\n",
        "            programs.append(program)\n",
        "\n",
        "            if save_data:\n",
        "                with open(f\"{prompt_path}/{layer}_{head}_prompt.txt\", \"w\") as f: f.write(prompt)\n",
        "                with open(f\"{program_path}/{head}_output.py\", \"w\") as f: f.write(program)\n",
        "\n",
        "        if evaluate: \n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "automation_pipeline(model, tokenizer, generic_sentences[:10], API_KEY=API_KEY, save_data=True, evaluate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e9c1ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CALCULATE AND SAVE SCORES FOR AUTOMATICALLY GENERATED PROGRAMS\n",
        "\n",
        "import importlib.util\n",
        "import types\n",
        "\n",
        "scores = []\n",
        "failed_programs = []\n",
        "for layer in range(12):\n",
        "    # if layer != 11: continue\n",
        "    code_path = f\"automation_2/llm_code/code_layer_{layer}/\"\n",
        "    for j in range(12):\n",
        "        # if j != 11: continue\n",
        "        filename = f\"{j}_output.py\"\n",
        "        program_path = os.path.join(code_path, filename)\n",
        "        if not os.path.exists(program_path): continue\n",
        "        score_path = f\"automation_2/scores/{layer}_{j}_score.txt\"\n",
        "        os.makedirs(os.path.dirname(score_path), exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_j{j}\", program_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "        except Exception as e:\n",
        "            print(f\"Layer {layer}, Head {j} - Error: {e}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"Error loading module: {e}\")\n",
        "            failed_programs.append((layer, j))\n",
        "            scores.append(-1)\n",
        "            continue\n",
        "\n",
        "        for attr_name in dir(module):\n",
        "            attr = getattr(module, attr_name)\n",
        "            if isinstance(attr, types.FunctionType):\n",
        "                program_to_test = attr\n",
        "                break\n",
        "\n",
        "        try:\n",
        "            score = score_prediction(model, tokenizer, (layer, j), program_to_test, generic_sentences[0], distance=\"jsd\", output=False)\n",
        "            print(f\"Layer {layer}, Head {j} - Score: {score:.2f}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"{score:.2f}\")\n",
        "            scores.append(score)\n",
        "        except Exception as e:\n",
        "            print(f\"Layer {layer}, Head {j} - Error: {e}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"Error during scoring: {e}\")\n",
        "            failed_programs.append((layer, j))\n",
        "            scores.append(-1)\n",
        "            continue\n",
        "\n",
        "num_scored = len([s for s in scores if s != -1 and not np.isnan(s)])\n",
        "print(f\"Number of Successfully Scored Heads: {num_scored} out of {len(scores)}\")\n",
        "\n",
        "avg_score = sum([s for s in scores if s != -1 and not np.isnan(s)]) / len([s for s in scores if s != -1 and not np.isnan(s)])\n",
        "print(f\"Average Score (excluding errors): {avg_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48c76a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# make scores into a matrix (12,12) with -1 for failed programs\n",
        "\n",
        "sq_score = np.full((12, 12), -1.0)\n",
        "for idx, score in enumerate(scores):\n",
        "    layer = idx // 12\n",
        "    head = idx % 12\n",
        "    sq_score[layer, head] = score\n",
        "\n",
        "sq_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a61ba7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# loop through bert and bert2 scores, take best score and build sq_Score matrix\n",
        "\n",
        "scores_1 = np.array([])\n",
        "scores_2 = np.array([])\n",
        "scores_min = np.array([])\n",
        "\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        score1_path = os.path.join(\"automation_results_bert\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        score2_path = os.path.join(\"automation_results_bert_2\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        if os.path.exists(score1_path):\n",
        "            with open(score1_path, \"r\") as f:\n",
        "                try:\n",
        "                    score1 = round(float(f.read().strip()), 2)\n",
        "                    scores_1 = np.append(scores_1, score1)\n",
        "                except:\n",
        "                    score1 = 1\n",
        "        # if os.path.exists(score2_path):\n",
        "        try:\n",
        "            with open(score2_path, \"r\") as f:\n",
        "                try:\n",
        "                    score2 = round(float(f.read().strip()), 2)\n",
        "                    scores_2 = np.append(scores_2, score2)\n",
        "                except:\n",
        "                    score2 = 1\n",
        "                    scores_2 = np.append(scores_2, score2)\n",
        "        except:\n",
        "            scores_2 = np.append(scores_2, 1)\n",
        "\n",
        "        best_score = min(score1, score2)\n",
        "        scores_min = np.append(scores_min, best_score)\n",
        "\n",
        "sq_scores = scores_min.reshape((12, 12))\n",
        "np.save(\"bert_scores_1.npy\", scores_1.reshape((12, 12)))\n",
        "np.save(\"bert_scores_2.npy\", scores_2.reshape((12, 12)))\n",
        "np.save(\"bert_headscores.npy\", scores_min.reshape((12, 12)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9020344",
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate through bert heads, check if corresponding score in automation_results_bert/scores and automation_results_bert_2/scores. save the higher score\n",
        "scores = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        score1_path = os.path.join(\"automation_results_bert\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        score2_path = os.path.join(\"automation_results_bert_3\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        score1, score2 = None, None\n",
        "        if os.path.exists(score1_path):\n",
        "            with open(score1_path, 'r') as f:\n",
        "                score1 = float(f.read().strip())\n",
        "            \n",
        "        if os.path.exists(score2_path):\n",
        "            with open(score2_path, 'r') as f:\n",
        "                try:\n",
        "                    score2 = float(f.read().strip())\n",
        "                except ValueError:\n",
        "                    print(f\"Invalid score in {score2_path}\")\n",
        "        \n",
        "        if score1 is not None and score2 is not None:\n",
        "            # print(\"score1:\", score1, \"score2:\", score2, \"min:\", min(score1, score2))\n",
        "            scores.append(min(score1, score2))\n",
        "        elif score1 is not None:\n",
        "            scores.append(score1)\n",
        "        elif score2 is not None:\n",
        "            scores.append(score2)\n",
        "        else:\n",
        "            scores.append(None)\n",
        "\n",
        "scores = np.array(scores).reshape(12, 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5478a1e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ordinary score plot\n",
        "\n",
        "colors = \"Grays\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "im2 = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im2.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im2, ax=ax)\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
        "ax.set_yticklabels([i for i in range(12)])\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "# put a space element in between automation and scores in text\n",
        "title = (\n",
        "    r'$\\mathbf{Automation \\ Scores}$'  # \\mathbf makes the text bold\n",
        "    '\\n\\nMethod: No Refinement'\n",
        "    f'\\nScores | {model.config.architectures[0]}'  # Example: replaced model.config...\n",
        ")\n",
        "plt.title(f\"{title}\\n\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa549158",
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = np.load(\"data/k62_bert_automation_scores.npy\")\n",
        "sq_score = np.reshape(scores, (12, 12))\n",
        "\n",
        "# Mask invalid entries\n",
        "masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
        "\n",
        "# Setup base heatmap\n",
        "colors = \"Greys\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "\n",
        "# --- Base image ---\n",
        "im = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "\n",
        "# --- Overlay red cells (< 0.4) ---\n",
        "threshold_value = 0.2\n",
        "highlight_mask = sq_score < threshold_value\n",
        "red_overlay = np.zeros((*sq_score.shape, 4))\n",
        "red_overlay[highlight_mask] = [0.9, 0, 0.2, 1]  # nice blue RGBA color: \n",
        "\n",
        "# Plot on top (no transparency, exact green)\n",
        "ax.imshow(red_overlay, aspect='auto')\n",
        "\n",
        "# Axes\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels(range(12), rotation=90)\n",
        "ax.set_yticklabels(range(12))\n",
        "\n",
        "# Title\n",
        "title = (\n",
        "    r'$\\mathbf{BERT \\ Automation \\ Scores}$'\n",
        "    f'\\nHighlights for Great Scores | Colorful if score < {threshold_value}'\n",
        "    '\\n\\nMethod: Auto (K=N)'\n",
        "    f'\\n{np.sum(highlight_mask)} highlighted scores ({100 * np.sum(highlight_mask) / highlight_mask.size:.1f}%) | Avg_Score: {np.mean(sq_score):.3f}'\n",
        ")\n",
        "plt.title(f\"{title}\\n\")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257999d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = \"Grays_r\"\n",
        "plt.rcParams['text.usetex'] = False\n",
        "\n",
        "score_threshold = 0.4\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "highlighted_sq = np.where(sq_score < score_threshold, sq_score, np.nan)\n",
        "# make all non-highlighted values white (1.0)\n",
        "highlighted_sq = np.where(np.isnan(highlighted_sq), 1.0, highlighted_sq)\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "im2 = ax.imshow(highlighted_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im2.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im2, ax=ax)\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
        "ax.set_yticklabels([i for i in range(12)])\n",
        "import matplotlib\n",
        "print(\"usetex:\", matplotlib.rcParams['text.usetex'])\n",
        "plt.rcParams['text.usetex'] = False\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "# make automation bold\n",
        "# get number of highlighted scores\n",
        "num_highlighted = np.sum(sq_score < score_threshold)\n",
        "title = (\n",
        "    r'$\\mathbf{Highlighted\\ Scores}$'\n",
        "    '\\n\\nMethod: No Refinement'\n",
        "    f'\\n {num_highlighted} scores < {score_threshold} ({num_highlighted/(len(sq_score)**2)*100:.0f}%) | {model.config.architectures[0]}\\n')\n",
        "# title = \"Automation Scores\\n\"\n",
        "plt.title(title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4273dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# go through every head, get score for each pattern in patterns, save the lowest ones in scores\n",
        "\n",
        "scores = []\n",
        "# bert\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "        head_scores = []\n",
        "        for pattern in patterns:\n",
        "            try:\n",
        "                sentence_collector = []\n",
        "                for sentence in sentences[:5]:\n",
        "                    score = score_prediction(model, tokenizer, (layer, head), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                    sentence_collector.append((sentence, score))\n",
        "                head_score = np.mean([s[1] for s in sentence_collector])\n",
        "                head_scores.append((pattern.__name__, head_score))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error scoring Layer {layer}, Head {head} with pattern {pattern.__name__}: {e}\")\n",
        "                head_scores.append((pattern.__name__, float('inf')))\n",
        "        \n",
        "        # get pattern with lowest score\n",
        "        best_pattern, best_score = min(head_scores, key=lambda x: x[1])\n",
        "        scores.append((layer, head, best_pattern, best_score))\n",
        "        print(f\"Layer {layer}, Head {head} - Best Pattern: {best_pattern} | Score: {best_score:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae3381ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sentences = sentences[135:150]\n",
        "# scores = np.zeros((model.config.num_hidden_layers, model.config.num_attention_heads))\n",
        "top_k = -1\n",
        "\n",
        "# model is bert\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# torch_model = model\n",
        "# torch_tokenizer = tokenizer \n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "\n",
        "# model is gpt-2\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "torch_model = model\n",
        "torch_tokenizer = tokenizer\n",
        "\n",
        "scores = np.zeros((model.config.num_hidden_layers, model.config.num_attention_heads))\n",
        "num_layers = torch_model.config.num_hidden_layers\n",
        "num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    # if layer < 8: continue\n",
        "    for head in range(num_heads):\n",
        "        # if layer < 8 and head < 4: continue\n",
        "        print(f\"Analyzing Layer {layer}, Head {head}...\")\n",
        "        sentence_scores = []\n",
        "        for sentence in test_sentences:\n",
        "            # inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "            # outputs = model(**inputs, output_attentions=True)\n",
        "            # attention = outputs.attentions[layer][0, head].detach().numpy()\n",
        "            # y = attention.flatten()\n",
        "            # num_tokens = len(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
        "\n",
        "            toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "            len_seq = len(toks.input_ids[0])\n",
        "            # assign full attention to one random index in each row\n",
        "            attention = np.zeros((len_seq, len_seq))\n",
        "            for i in range(len_seq):\n",
        "                attention[i, np.random.randint(0, len_seq)] = 1\n",
        "\n",
        "            # attention = np.random.rand(len_seq, len_seq)\n",
        "            attention = attention / attention.sum(axis=1, keepdims=True)\n",
        "            y = attention.flatten()\n",
        "\n",
        "            X = []\n",
        "            for pattern in patterns:\n",
        "                try:\n",
        "                    instance = pattern(sentence, tokenizer)[1].flatten()\n",
        "                    # asssert that instance is a 2d array\n",
        "                    if instance.ndim != 1:\n",
        "                        print(f\"\\t\\tRemoved pattern {pattern.__name__} due to incorrect output dimensions.\")\n",
        "                        del patterns[patterns.index(pattern)]\n",
        "                        continue\n",
        "\n",
        "\n",
        "                    # check if nan in instance:\n",
        "                    if np.isnan(instance).any():\n",
        "                        del patterns[patterns.index(pattern)]\n",
        "                        print(f\"\\t\\tRemoved pattern {pattern.__name__} due to NaN values.\")\n",
        "                        continue\n",
        "                    X.append(instance)\n",
        "                    # print(f\"Tested pattern: {pattern.__name__} successfully\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    # print(f\"Error testing pattern {pattern.__name__}: {e}\")\n",
        "                    print(f\"\\t\\tRemoved pattern {pattern.__name__} due to error during testing.\")\n",
        "                    del patterns[patterns.index(pattern)]\n",
        "                    continue\n",
        "\n",
        "            X_n = np.array(X).T\n",
        "            y = np.nan_to_num(y.flatten())\n",
        "\n",
        "            # methods = [\"lasso\", \"l1\", \"ridge\", \"linear\"]\n",
        "            # method = \"lasso\"\n",
        "\n",
        "            # if method == \"lasso\":\n",
        "            #     reg = Lasso(alpha=0.001).fit(X_n, y)\n",
        "            # elif method == \"l1\":\n",
        "            #     reg = ElasticNet(alpha=0.001, l1_ratio=1.0).fit(X_n, y)\n",
        "            # elif method == \"ridge\":\n",
        "            #     reg = Ridge(alpha=0.001).fit(X_n, y)\n",
        "            # else:\n",
        "            #     reg = LinearRegression().fit(X_n, y)\n",
        "\n",
        "            # reg = Lasso(alpha=0.01).fit(X_n, y)\n",
        "            # reg = Ridge(alpha=0.001).fit(X_n, y)\n",
        "            reg = LinearRegression().fit(X_n, y)\n",
        "            side_length = int(np.sqrt(len(y)))\n",
        "            y = y.reshape((side_length, side_length))\n",
        "            top_indices = np.argsort(np.abs(reg.coef_))[-top_k:]\n",
        "\n",
        "            if top_k == 1:\n",
        "                pred_atts = []\n",
        "                fn_to_trys = []\n",
        "\n",
        "                for i in range(5):\n",
        "                    fn_to_try = patterns[np.argsort(np.abs(reg.coef_))[-1 - i]]\n",
        "                    pred_att = fn_to_try(sentence, torch_tokenizer)[1]\n",
        "                    pred_atts.append(pred_att)\n",
        "                    fn_to_trys.append(fn_to_try)\n",
        "\n",
        "                scores_five = []\n",
        "                for pred_att in pred_atts:\n",
        "                    jensonshannon_distances = []\n",
        "                    for row_att, row_out in zip(y, pred_att):\n",
        "                        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "                    try_score = np.mean(jensonshannon_distances)\n",
        "                    scores_five.append(try_score)\n",
        "\n",
        "                fn_winner = fn_to_trys[np.argmin(scores_five)]\n",
        "                pred_att = pred_atts[np.argmin(scores_five)]\n",
        "\n",
        "                print(f\"\\tUsing pattern: {fn_winner.__name__}\")\n",
        "\n",
        "            elif top_k == -1:\n",
        "                pred_att = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "                pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "            else:\n",
        "                pred_att = reg.intercept_ + sum(reg.coef_[i] * X[i] for i in top_indices)\n",
        "                pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "            jensonshannon_distances = []\n",
        "            for row_att, row_out in zip(y, pred_att):\n",
        "                jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "            score = np.mean(jensonshannon_distances)\n",
        "            sentence_scores.append(score)\n",
        "        \n",
        "        # scores[layer, head] = np.mean(sentence_scores)\n",
        "        scores[layer, head] = np.mean(sentence_scores)\n",
        "        print(f\"\\tScore for Layer {layer}, Head {head}: {np.mean(sentence_scores):.2f}\\n\")\n",
        "\n",
        "# for each head: do linear interpolation on patterns, set pred_att to sum with top_k hypotheses based on parameter magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fac3928",
      "metadata": {},
      "outputs": [],
      "source": [
        "# scores nan to mean\n",
        "scores = np.nan_to_num(scores, nan=np.nanmean(scores))\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3b5d3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(patterns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aeb71ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"data/interpolation_experiment_gpt2_lasso_a=0.01_k=N.npy\", scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd2b457",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = []\n",
        "for pattern in patterns:\n",
        "    try:\n",
        "        instance = pattern(sentence, tokenizer)[1].flatten()\n",
        "        # asssert that instance is a 2d array\n",
        "        if instance.ndim != 1:\n",
        "            print(f\"\\t\\tRemoved pattern {pattern.__name__} due to incorrect output dimensions.\")\n",
        "            del patterns[patterns.index(pattern)]\n",
        "            continue\n",
        "\n",
        "\n",
        "        # check if nan in instance:\n",
        "        if np.isnan(instance).any():\n",
        "            del patterns[patterns.index(pattern)]\n",
        "            print(f\"\\t\\tRemoved pattern {pattern.__name__} due to NaN values.\")\n",
        "            continue\n",
        "        X.append(instance)\n",
        "        # print(instance.shape)\n",
        "        # print(f\"Tested pattern: {pattern.__name__} successfully\")\n",
        "\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error testing pattern {pattern.__name__}: {e}\")\n",
        "        print(f\"\\t\\tRemoved pattern {pattern.__name__} due to error during testing.\")\n",
        "        del patterns[patterns.index(pattern)]\n",
        "        continue\n",
        "X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15078a0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# dict: pattern name -> number of lines\n",
        "pattern_line_dict = {}\n",
        "for pattern, num_lines in zip(patterns, number_of_lines):\n",
        "    pattern_line_dict[pattern.__name__] = num_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009616a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "line_count = []\n",
        "# go through scores and assign the number of lines based on the patter name\n",
        "for layer, head, pattern_name, score in scores:\n",
        "    num_lines = pattern_line_dict.get(pattern_name, None)\n",
        "    line_count.append(num_lines)\n",
        "print(len(line_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b192807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# i only need score from scores, not head and layer\n",
        "final_scores = [s[3] for s in scores]\n",
        "\n",
        "line_count_np = np.array(line_count).flatten()\n",
        "final_scores_np = np.array(final_scores).flatten()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(line_count, final_scores, color='blue')\n",
        "plt.xlabel(\"Number of Non-Comment, Non-Blank Lines\\n\")\n",
        "plt.ylabel(\"Score\")\n",
        "# model is bert\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "#bold second line in title\n",
        "second_line = f\"{model.config.architectures[0]}\"\n",
        "# how to bold text in only part of title\n",
        "plt.title(f\"Function Length vs Score | Score is from \\nAuto (K=1) Refinement\\n\\n{second_line}\\n\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2788d64",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROGRAM COMPLEXITY ANALYSIS | FUNCTION LENGTH VS SCORE\n",
        "\n",
        "#make scatterplot, loop through functions and count number of non-comment, non-blank lines and then get scores and at the end plot all line/score pairs\n",
        "\n",
        "folder_of_interest = \"automation_results_gpt2\"\n",
        "\n",
        "complexity = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        score_path = os.path.join(folder_of_interest, \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        # program_path = os.path.join(folder_of_interest, \"llm_code\", f\"code_layer_{layer}\", f\"{head}_output.py\")\n",
        "        # program example: automation_results_bert\\llm_code\\layer0_head0_code.py\n",
        "        program_path = os.path.join(folder_of_interest, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "        if not os.path.exists(score_path):\n",
        "            print(\"here\")\n",
        "            continue\n",
        "\n",
        "        with open(score_path, \"r\") as f:\n",
        "            try:\n",
        "                score = float(f.read().strip())\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        with open(program_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "            code_lines = [line for line in lines if line.strip() and not line.strip().startswith(\"#\")]\n",
        "            num_code_lines = len(code_lines)\n",
        "        \n",
        "        complexity.append((num_code_lines, score))\n",
        "        print(f\"L{layer},H{head} | Lines: {num_code_lines} | Score: {score}\")\n",
        "\n",
        "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "line_counts = [item[0] for item in complexity]\n",
        "scores = [item[1] for item in complexity]\n",
        "plt.scatter(line_counts, scores, color='blue')\n",
        "plt.xlabel(\"Number of Non-Comment, Non-Blank Lines\\n\")\n",
        "plt.ylabel(\"Score\")\n",
        "# model is bert\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "#bold second line in title\n",
        "second_line = f\"{model.config.architectures[0]}\"\n",
        "# how to bold text in only part of title\n",
        "plt.title(f\"Function Length vs Score | Score is from \\nNo Refinement\\n\\n{second_line}\\n\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3a0037",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Hypothesis Structure Experiments\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e73244",
      "metadata": {},
      "outputs": [],
      "source": [
        "programs = [next_attention, previous_attention, same_attention, punctuation_attention, dependencies,\n",
        "            last_token_attention, uniform_attention, cls_attention, eos_attention, pos_alignment,\n",
        "            special_token_attention, repeated_attention, noun_modifier_attention, pronoun_attention,\n",
        "            single_token_attention, root_cluster_attention]\n",
        "programs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac957f3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c820d5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from programs import *\n",
        "\n",
        "programs = [next_attention, previous_attention, same_attention, punctuation_attention, dependencies,\n",
        "            last_token_attention, uniform_attention, cls_attention, eos_attention, pos_alignment,\n",
        "            special_token_attention, repeated_attention, noun_modifier_attention, pronoun_attention,\n",
        "            single_token_attention, root_cluster_attention]\n",
        "\n",
        "sentence_data = sentences[:25]\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def program_similarity(att_one, att_two):\n",
        "    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "        p = np.clip(p, 1e-12, 1.0)\n",
        "        q = np.clip(q, 1e-12, 1.0)\n",
        "        p /= p.sum()\n",
        "        q /= q.sum()\n",
        "        m = 0.5 * (p + q)\n",
        "        return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
        "\n",
        "    jensonshannon_distances = []\n",
        "    for row_att, row_out in zip(att_one, att_two):\n",
        "        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "    score = np.mean(jensonshannon_distances)\n",
        "    return score\n",
        "\n",
        "x = len(programs)\n",
        "S = np.zeros((x, x))\n",
        "for i in range(x):\n",
        "    print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
        "    for j in range(x):\n",
        "        if i < j:\n",
        "            similarities = []\n",
        "            program_one = programs[i]\n",
        "            program_two = programs[j]\n",
        "\n",
        "            for sentence in sentence_data:\n",
        "                h1, activations_one = program_one(sentence, tokenizer)\n",
        "                h2, activations_two = program_two(sentence, tokenizer)\n",
        "                similarities.append(program_similarity(activations_one, activations_two))\n",
        "            \n",
        "            S[i, j] = np.mean(similarities)\n",
        "            S[j, i] = S[i, j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1a3b62",
      "metadata": {},
      "outputs": [],
      "source": [
        "S = np.load('data/similarity_matrix_auto.npy')\n",
        "\n",
        "def group_similar_programs(programs, S, threshold=0.6):\n",
        "    groups, used = [], set()\n",
        "    for i in range(len(programs)):\n",
        "        if i in used: continue\n",
        "        group = [i]\n",
        "        used.add(i)\n",
        "        \n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for group_member in group:\n",
        "                for j in range(len(programs)):\n",
        "                    if j not in used and S[group_member, j] < threshold:\n",
        "                        group.append(j)\n",
        "                        used.add(j)\n",
        "                        changed = True\n",
        "        groups.append([programs[idx].__name__ for idx in group])\n",
        "    \n",
        "    return groups\n",
        "\n",
        "groups = group_similar_programs(programs, S, threshold=0.6)\n",
        "for i, group in enumerate(groups):\n",
        "    print(f\"Group {i+1}: {group}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4789011f",
      "metadata": {},
      "outputs": [],
      "source": [
        "folder = \"automation_results_bert\"\n",
        "programs = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        code_path = os.path.join(folder, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "        if os.path.exists(code_path):\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_head{head}\", code_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            try:\n",
        "                spec.loader.exec_module(module)\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        programs.append(attr)\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading program for Layer {layer}, Head {head}: {e}\")\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8f46ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8219481",
      "metadata": {},
      "outputs": [],
      "source": [
        "S = (S + S.T) / 2\n",
        "\n",
        "def group_similar_programs(programs, S, threshold=0.6):\n",
        "    groups, used = [], set()\n",
        "    for i in range(len(programs)):\n",
        "        if i in used: continue\n",
        "        group = [i]\n",
        "        used.add(i)\n",
        "        \n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for group_member in group:\n",
        "                for j in range(len(programs)):\n",
        "                    if j not in used and S[group_member, j] < threshold:\n",
        "                        group.append(j)\n",
        "                        used.add(j)\n",
        "                        changed = True\n",
        "        groups.append([programs[idx].__name__ for idx in group])\n",
        "    \n",
        "    return groups\n",
        "\n",
        "groups = group_similar_programs(programs, S, threshold=0.25)\n",
        "for i, group in enumerate(groups):\n",
        "    if len(group) >= 1:\n",
        "        print(f\"Group {i+1}: {group}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ed1f38",
      "metadata": {},
      "outputs": [],
      "source": [
        "for program in programs:\n",
        "    print(program.__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b464ed4",
      "metadata": {},
      "outputs": [],
      "source": [
        "name_to_idx = {fn.__name__: i for i, fn in enumerate(programs)}\n",
        "new_order = [name_to_idx[name] for group in groups for name in group]\n",
        "S_grouped = S[np.ix_(new_order, new_order)]\n",
        "# S_grouped = np.load(\"data/similarity_matrix_auto.npy\")\n",
        "colors = \"Blues_r\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
        "im2 = ax.imshow(S_grouped, cmap=colors, aspect='auto')\n",
        "# ax.set_axis_off()\n",
        "# ax.set_xticks(range(len(programs)))\n",
        "# ax.set_yticks(range(len(programs)))\n",
        "# ax.set_xticklabels([p.__name__ for p in programs], rotation=90)\n",
        "# ax.set_yticklabels([p.__name__ for p in programs])\n",
        "# axis off\n",
        "ax.set_axis_off()\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.title(\"Automated Programs | Similarity Matrix\\n\", weight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b0e864",
      "metadata": {},
      "outputs": [],
      "source": [
        "# automated_program_similarity_analysis\n",
        "\n",
        "import os\n",
        "import importlib.util\n",
        "import types\n",
        "# from AutoTokenizer import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "folder = \"automation_results_bert\"\n",
        "programs = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        code_path = os.path.join(folder, \"llm_code\", f\"programs-layer_{layer}\", f\"{head}_output.py\")\n",
        "        if os.path.exists(code_path):\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_head{head}\", code_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            try:\n",
        "                spec.loader.exec_module(module)\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        programs.append(attr)\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading program for Layer {layer}, Head {head}: {e}\")\n",
        "                continue\n",
        "\n",
        "# sentence_data = sentences[:10]\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# def program_similarity(att_one, att_two):\n",
        "#     def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "#         p = np.clip(p, 1e-12, 1.0)\n",
        "#         q = np.clip(q, 1e-12, 1.0)\n",
        "#         p /= p.sum()\n",
        "#         q /= q.sum()\n",
        "#         m = 0.5 * (p + q)\n",
        "#         return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
        "\n",
        "#     jensonshannon_distances = []\n",
        "#     for row_att, row_out in zip(att_one, att_two):\n",
        "#         jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "#     score = np.mean(jensonshannon_distances)\n",
        "#     return score\n",
        "\n",
        "# x = len(programs)\n",
        "# S = np.zeros((x, x))\n",
        "# for i in range(x):\n",
        "#     print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
        "#     for j in range(x):\n",
        "#         if j % 24 == 0: print(f\"  inner loop {j}/{x}\")\n",
        "#         if i != j:\n",
        "#             similarities = []\n",
        "#             program_one = programs[i]\n",
        "#             program_two = programs[j]\n",
        "\n",
        "#             for sentence in sentence_data:\n",
        "#                 h1, activations_one = program_one(sentence, tokenizer)\n",
        "#                 h2, activations_two = program_two(sentence, tokenizer)\n",
        "#                 similarities.append(program_similarity(activations_one, activations_two))\n",
        "            \n",
        "#             S[i, j] = np.mean(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66271c65",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Hypothesis Replacement Experiments\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce003186",
      "metadata": {},
      "outputs": [],
      "source": [
        "from programs import *\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "programs = [next_attention, previous_attention, same_attention, punctuation_attention, dependencies,\n",
        "            last_token_attention, uniform_attention, cls_attention, eos_attention, pos_alignment,\n",
        "            special_token_attention, repeated_attention, noun_modifier_attention, pronoun_attention,\n",
        "            single_token_attention, root_cluster_attention]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model_replace = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model.eval()\n",
        "model_replace.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b458bef0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper functions for replacing heads in llms\n",
        "\n",
        "def has_hooks(model):\n",
        "    # sanity check to see if any hooks are registered\n",
        "    for module in model.modules():\n",
        "        if module._forward_hooks or module._backward_hooks or module._forward_pre_hooks:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def remove_all_hooks(model):\n",
        "    for module in model.modules():\n",
        "        module._forward_hooks = {}\n",
        "        module._backward_hooks = {}\n",
        "        module._forward_pre_hooks = {}\n",
        "    worked = has_hooks(model)\n",
        "    print(f\"All hooks removed: {not worked}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3d20fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_hypothesis_dictionary(model, tokenizer, sentences):\n",
        "    best_fit_data = pd.read_csv('data/best_fit_refinement_bert.csv') # i, j, pattern, score\n",
        "    hypothesis_dict = {} # (layer, head) -> (hypothesis, score)\n",
        "    for row in best_fit_data.itertuples():\n",
        "        layer = row.i\n",
        "        head = row.j\n",
        "        hypothesis = row.Pattern\n",
        "        # hypothesis_dict[(layer, head)] = hypothesis\n",
        "        # attach hypothesis to dict if it's the lowest score for that head\n",
        "        if (layer, head) not in hypothesis_dict:\n",
        "            hypothesis_dict[(layer, head)] = (hypothesis, row.Score)\n",
        "        else:\n",
        "            if row.Score < hypothesis_dict[(layer, head)][1]:\n",
        "                hypothesis_dict[(layer, head)] = (hypothesis, row.Score)\n",
        "    \n",
        "    # collect hypothesis names set and define program_executables list\n",
        "    program_executables = []\n",
        "    hypothesis_names = set()\n",
        "    for key in hypothesis_dict: hypothesis_names.add(hypothesis_dict[key][0])\n",
        "\n",
        "    for name in hypothesis_names:\n",
        "        flag = False\n",
        "        for program in programs:\n",
        "            if program.__name__ == name:\n",
        "                program_executables.append(program)\n",
        "                flag = True\n",
        "                break\n",
        "        if not flag:\n",
        "            print(f\"Program {name} not found among available programs.\")\n",
        "\n",
        "    print(f\"\\nNumber of Heads with Hypotheses: {len(hypothesis_dict)}\")\n",
        "    print(f\"Percentage of Heads with Hypotheses: {len(hypothesis_dict) / 144 * 100:.2f}%\")\n",
        "    print(f\"Number of Program Executables: {len(program_executables)}\\n\")\n",
        "    return hypothesis_dict, program_executables\n",
        "\n",
        "best_fits, programs_ex = generate_hypothesis_dictionary(model, tokenizer, sentences)\n",
        "print([program.__name__ for program in programs_ex])\n",
        "best_fits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70eca0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_percentage = 0.2\n",
        "best_fits_sorted = sorted(best_fits.items(), key=lambda x: x[1][1])\n",
        "best_fits_top = best_fits_sorted[:int(len(best_fits_sorted) * top_percentage)]\n",
        "best_fits_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30646c5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def mask_sentence(sent, tokenizer):\n",
        "    tokens = tokenizer.tokenize(sent)\n",
        "    if not tokens:\n",
        "        return sent\n",
        "    idx = np.random.randint(len(tokens))\n",
        "    true_token = tokens[idx]\n",
        "    tokens[idx] = tokenizer.mask_token\n",
        "    return tokenizer.convert_tokens_to_string(tokens), true_token\n",
        "\n",
        "masked_sentences = []\n",
        "true_tokens = []\n",
        "for i, sent in enumerate(sentences):\n",
        "    masked_sent, true_token = mask_sentence(sent, tokenizer)\n",
        "    if i < 6: print(f\"Original: {sent}\\nMasked:   {masked_sent}\\n\")\n",
        "    masked_sentences.append(masked_sent)\n",
        "    true_tokens.append(true_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4634e65d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def single_perplexity(sentence, true_token):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    return torch.exp(-log_prob).item()\n",
        "\n",
        "def full_perplexity(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = torch.tensor([ids])\n",
        "    log_probs = []\n",
        "    for i in range(1, len(tokens)-1):  # consider all tokens except [CLS], [SEP]\n",
        "        masked = input_ids.clone()\n",
        "        masked[0, i] = tokenizer.mask_token_id\n",
        "        with torch.no_grad():\n",
        "            logits = model(masked).logits\n",
        "        prob = torch.log_softmax(logits[0, i], dim=-1)[input_ids[0, i]]\n",
        "        log_probs.append(prob)\n",
        "    return torch.exp(-torch.stack(log_probs).mean())\n",
        "\n",
        "average_ppl_single = []\n",
        "average_ppl_full = []\n",
        "for i, (sent, true_tok) in enumerate(zip(masked_sentences[:100], true_tokens[:100])):\n",
        "    ppl_s = single_perplexity(sent, true_tok)\n",
        "    ppl_f = full_perplexity(sent)\n",
        "    average_ppl_single.append(ppl_s)\n",
        "    average_ppl_full.append(ppl_f)\n",
        "    if i % 50 == 0: print(f\"Sentence {i}:\\tPerplexity(s): {ppl_s:.3f}\\tPerplexity(f): {ppl_f:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993ba5f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def register_hypothesis_hooks(model, best_fits, tokenizer):\n",
        "    hooks = []\n",
        "    def replace_activation_with_hypothesis(layer, head, sentence, tokenizer):\n",
        "        hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "        program_to_use = programs_ex[[i for i, p in enumerate(programs_ex) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "        # print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "        out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "        return out\n",
        "\n",
        "    for (layer_idx, head_idx) in best_fits:\n",
        "        target_layer = model.bert.encoder.layer[layer_idx].attention.self\n",
        "\n",
        "        def make_hook(layer_i, head_i):\n",
        "            def hook_fn(module, input, output):\n",
        "                context_layer, attn_probs = output\n",
        "                print(context_layer.shape)\n",
        "                # B, S, H, D = context_layer.shape  # (batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "                sentence = tokenizer.decode(input[0][0], skip_special_tokens=True)\n",
        "                out = replace_activation_with_hypothesis(layer_i, head_i, sentence, tokenizer)  # shape (S, S)\n",
        "\n",
        "                # project (S, S) â (B, S, D)\n",
        "                D = context_layer.size(-1)\n",
        "                rand_head = out.unsqueeze(0).repeat(B, 1, 1)[:, :, :D]\n",
        "                context_layer[:, :, head_i, :] = rand_head\n",
        "                return (context_layer, attn_probs)\n",
        "            return hook_fn\n",
        "\n",
        "        hook = target_layer.register_forward_hook(make_hook(layer_idx, head_idx))\n",
        "        hooks.append(hook)\n",
        "    return hooks\n",
        "\n",
        "hooks = register_hypothesis_hooks(\n",
        "    model_replace,\n",
        "    best_fits=best_fits,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e276add3",
      "metadata": {},
      "outputs": [],
      "source": [
        "ppl_subset_replaced = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:100], true_tokens[:100]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model_replace(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_subset_replaced.append(torch.exp(-log_prob).item())\n",
        "\n",
        "ppl_subset_normal = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:100], true_tokens[:100]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_subset_normal.append(torch.exp(-log_prob).item())\n",
        "\n",
        "print(f\"Average Perplexity disabled: pd-describe{(pd.Series(ppl_subset_replaced).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a58192c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(sentences[3])\n",
        "# tokenizer.decode(tokenizer(sentences[0])['input_ids'], skip_special_tokens=True)\n",
        "tokens = tokenizer.tokenize(sentences[0])\n",
        "input = tokenizer(sentences[0], return_tensors=\"pt\")\n",
        "print(len(input), input['input_ids'].shape[1])\n",
        "\n",
        "decoded = tokenizer.decode(input['input_ids'][0], skip_special_tokens=True)\n",
        "_, activation_hypothesis = get_hypothesis_head(layer_id, head, decoded, tokenizer)\n",
        "print(activation_hypothesis.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "232bd71d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hypothesis_head(layer, head, sentence, tokenizer):\n",
        "        hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "        program_to_use = programs_ex[[i for i, p in enumerate(programs_ex) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "        # print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "        out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "        return out\n",
        "\n",
        "def replace_attention_head(module, input, output):\n",
        "    \"\"\" module: BertSelfAttention | output: tuple (context_layer, attention_probs)\"\"\"\n",
        "    context_layer, attn_probs = output\n",
        "    batch, seq_len, hidden_dim = context_layer.shape\n",
        "    num_heads = module.num_attention_heads\n",
        "    head_dim = hidden_dim // num_heads\n",
        "    context_layer = context_layer.view(batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "    # print current sentence\n",
        "    # print(f\"Current Sentence: {inputs['input_ids']}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # decoded_sentences = tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # for decoded_sentence in decoded_sentences:\n",
        "        # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "\n",
        "    # layer_id = getattr(module, \"layer_id\", None)\n",
        "    # heads = []\n",
        "    # for (layer, head), (fn_name, score) in best_fits_top:\n",
        "        # if layer == layer_id:\n",
        "            # print(layer, head, fn_name)\n",
        "            # heads.append(head)\n",
        "\n",
        "    # zero_out = 10 # percent of all heads\n",
        "    # random_heads = np.random.choice(range(num_heads), size=int(num_heads * zero_out / 100), replace=False)\n",
        "    # random number from 1 to 100 (2 random numbers in a list)\n",
        "    random_heads = np.random.choice(range(num_heads), size=6, replace=False)\n",
        "    print(len(random_heads))\n",
        "    for i in random_heads:\n",
        "        context_layer[:, :, i, :] = 0\n",
        "\n",
        "    # for head in heads:\n",
        "        # activation_hypothesis = np.eye(seq_len)\n",
        "        # _, activation_hypothesis = get_hypothesis_head(layer_id, head, decoded_sentence, tokenizer)\n",
        "        # assert activation_hypothesis.shape == (seq_len, seq_len), f\"Hypothesis shape {activation_hypothesis.shape} does not match expected {(seq_len, seq_len)} for '{decoded_sentence}'\"\n",
        "\n",
        "        # broadcasted_hypothesis = torch.matmul(\n",
        "        #     torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "        #     context_layer[:, :, head, :]\n",
        "        # )\n",
        "        # context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 0:\n",
        "    #     head = 6\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 4\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 0\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len)) \n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 1:\n",
        "    #     head = 6\n",
        "    #     # cls attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, 0] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 9\n",
        "    #     # relative position attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     for i in range(seq_len):\n",
        "    #         for j in range(seq_len):\n",
        "    #             if j > i:\n",
        "    #                 activation_hypothesis[i, j] = 1.0\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 2:\n",
        "    #     # next attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[0, 1:] = 1.0\n",
        "    #     # activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     head = 0\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 9\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "        \n",
        "    # if layer_id == 7:\n",
        "    #     head = 6\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 8:\n",
        "    #     head = 2\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # [((2, 0), ('next_attention', 0.0134178736883759)),\n",
        "    # ((2, 9), ('next_attention', 0.0195240472425213)),\n",
        "    # ((0, 0), ('uniform_attention', 0.1473364017258112)),\n",
        "    # ((1, 6), ('cls_attention', 0.1820107218156672)),\n",
        "    # ((8, 2), ('eos_attention', 0.198991498540351)),\n",
        "    # ((0, 4), ('uniform_attention', 0.2081918466789174)),\n",
        "    # ((1, 9), ('relative_position_attention', 0.2308144694156767)),\n",
        "    # ((7, 6), ('eos_attention', 0.2383215559745129)),\n",
        "    # ((0, 6), ('uniform_attention', 0.246593192497212))]\n",
        "\n",
        "    context_layer = context_layer.reshape(batch, seq_len, hidden_dim)\n",
        "    return (context_layer, attn_probs)\n",
        "\n",
        "# target_layer = model_replace.bert.encoder.layer[2].attention.self\n",
        "# hook = target_layer.register_forward_hook(replace_attention_head)\n",
        "\n",
        "for layer_number, layer in enumerate(model_replace.bert.encoder.layer):\n",
        "    layer.attention.self.layer_id = layer_number\n",
        "    if layer_number in [2]:\n",
        "        layer.attention.self.register_forward_hook(replace_attention_head)\n",
        "\n",
        "ppl_zero_50 = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:400], true_tokens[:400]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model_replace(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "\n",
        "    # calculate full psuedo-perplexity\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_zero_50.append(torch.exp(-log_prob).item())\n",
        "\n",
        "# ppl_subset_normal = []\n",
        "# for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:400], true_tokens[:400]))):\n",
        "#     inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "#     mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "#     if len(mask_token_index[0]) == 0:\n",
        "#         raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "#     mask_pos = mask_token_index[1].item()\n",
        "#     masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "#     true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "#     log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "#     log_prob = log_probs[true_id] \n",
        "#     # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "#     ppl_subset_normal.append(torch.exp(-log_prob).item())\n",
        "\n",
        "# print('\\n')\n",
        "print('\\nWeird: ', np.median(ppl_subset_replaced), '\\t', np.mean(ppl_subset_replaced))\n",
        "print('Normal: ', np.median(ppl_subset_normal), '\\t', np.mean(ppl_subset_normal), '\\n')\n",
        "\n",
        "print(f\"Average Perplexity weird: pd-describe{(pd.Series(ppl_subset_replaced).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219244c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# boxplots:\n",
        "import matplotlib.pyplot as plt\n",
        "data = []\n",
        "for dataset in [ppl_subset_normal, ppl_zero_10, ppl_zero_20, ppl_zero_30]:\n",
        "    threshold = np.percentile(dataset, 92)\n",
        "    filtered_data = [x for x in dataset if x <= threshold]\n",
        "    data.append(filtered_data)\n",
        "normal_perplexity = f'Normal Perplexity\\nMedian={np.median(data[0]):.2f}\\nMean={np.mean(data[0]):.2f}\\nStd={np.std(data[0]):.2f}'\n",
        "strange_perplexity_10 = f'Zero Perplexity 10%\\nMedian={np.median(data[1]):.2f}\\nMean={np.mean(data[1]):.2f}\\nStd={np.std(data[1]):.2f}'\n",
        "strange_perplexity_20 = f'Zero Perplexity 20%\\nMedian={np.median(data[2]):.2f}\\nMean={np.mean(data[2]):.2f}\\nStd={np.std(data[2]):.2f}'\n",
        "strange_perplexity_30 = f'Zero Perplexity 30%\\nMedian={np.median(data[3]):.2f}\\nMean={np.mean(data[3]):.2f}\\nStd={np.std(data[3]):.2f}'\n",
        "\n",
        "labels = [normal_perplexity, strange_perplexity_10, strange_perplexity_20, strange_perplexity_30]\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.title(\"Perplexity Comparison | Zeroed Out Heads vs Normal Heads\\n\", weight='bold')\n",
        "plt.boxplot(data, labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b325320e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# boxplots:\n",
        "import matplotlib.pyplot as plt\n",
        "data = []\n",
        "for dataset in [ppl_subset_normal, ppl_weird]:\n",
        "    threshold = np.percentile(dataset, 95)\n",
        "    filtered_data = [x for x in dataset if x <= threshold]\n",
        "    data.append(filtered_data)\n",
        "normal_perplexity = f'Normal Perplexity\\nMedian={np.median(data[0]):.2f}\\nMean={np.mean(data[0]):.2f}\\nStd={np.std(data[0]):.2f}'\n",
        "strange_perplexity = f'Replacement Perplexity\\nMedian={np.median(data[1]):.2f}\\nMean={np.mean(data[1]):.2f}\\nStd={np.std(data[1]):.2f}'\n",
        "labels = [normal_perplexity, strange_perplexity]\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.title(\"Perplexity Comparison | Hypothesis-Replaced Heads vs Normal Heads\\n\", weight='bold')\n",
        "plt.boxplot(data, labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1435cd32",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.median(ppl_weird)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b643d9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "arr = [1,2,3,4,5,6,7,8,9]\n",
        "# use pd describe to show statistics\n",
        "import pandas as pd\n",
        "print(pd.Series(arr).describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e77eca",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_replace = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model_replace.load_state_dict(model.state_dict(), strict=True)\n",
        "model_replace.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f6e5a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_replace = remove_all_hooks(model_replace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd18a26b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Model has hooks: model_replace\", has_hooks(model_replace))\n",
        "print(\"Model has hooks: model\", has_hooks(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e4f9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_outliers(data):\n",
        "    iqr = False\n",
        "    if iqr:\n",
        "        q1 = np.percentile(data, 25)\n",
        "        q3 = np.percentile(data, 75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        return [x for x in data if lower_bound <= x <= upper_bound]\n",
        "    else:\n",
        "        return data[:-20]\n",
        "\n",
        "average_ppl_single_out = handle_outliers(average_ppl_single)\n",
        "average_ppl_full_out = handle_outliers(average_ppl_full)\n",
        "\n",
        "single = f'Single Token Perplexity\\nMedian={np.median(average_ppl_single):.2f}\\nMean={np.mean(average_ppl_single):.2f}\\nStd={np.std(average_ppl_single):.2f}'\n",
        "all_tokens = f'All Token Perplexity\\nMedian={np.median(average_ppl_full):.2f}\\nMean={np.mean(average_ppl_full):.2f}\\nStd={np.std(average_ppl_full):.2f}'\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot([average_ppl_single_out, average_ppl_full_out, average_ppl_full_out, average_ppl_full_out], labels=[single, all_tokens, all_tokens, all_tokens])\n",
        "plt.title(f'Perplexity Comparison: BERT-base\\nDataset: Tiny Sentences | len={len(average_ppl_full)}', weight='bold')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5deae86",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c670a7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_subset = sentences[:1_000]\n",
        "\n",
        "def replace_activation_with_hypothesis(layer, head, sentence, model, tokenizer):\n",
        "    hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "    program_to_use = programs_ex[[i for i, p in enumerate(programs_ex) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "\n",
        "    print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "    out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "    return out\n",
        "\n",
        "def make_replacement_hook(layer_idx, head_idx):\n",
        "    \"\"\"\n",
        "    Returns a hook function that replaces a specific head's activations.\n",
        "    \"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # output shape: (batch_size, num_heads, seq_length, head_dim)\n",
        "        batch_size, num_heads, seq_length, head_dim = output.shape\n",
        "        for b in range(batch_size):\n",
        "            sentence = tokenizer.decode(input[0][b])\n",
        "            replacement = replace_activation_with_hypothesis(layer_idx, head_idx, sentence, model, tokenizer)\n",
        "            if replacement is not None:\n",
        "                output[b, head_idx] = torch.tensor(replacement, dtype=output.dtype)\n",
        "        return output\n",
        "    return hook\n",
        "# \n",
        "def mask_sentence(sent, tokenizer):\n",
        "    tokens = tokenizer.tokenize(sent)\n",
        "    if not tokens:\n",
        "        return sent\n",
        "    idx = np.random.randint(len(tokens))\n",
        "    tokens[idx] = tokenizer.mask_token\n",
        "    return tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "hooks = []\n",
        "for (layer, head), (hypothesis, score) in best_fits_top:\n",
        "    hook = make_replacement_hook(layer, head)\n",
        "    hooks.append(hook)\n",
        "\n",
        "# evaluate bertbase model on masking with no hooks and calculate perplexity\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.nn import CrossEntropyLoss\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model.eval()\n",
        "loss_fct = CrossEntropyLoss()\n",
        "perplexities = []\n",
        "for i, sent in enumerate(sentence_subset):\n",
        "    masked = mask_sentence(sent, tokenizer)\n",
        "    inputs = tokenizer(masked, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    logits = logits.view(-1, model.config.vocab_size)\n",
        "    perplexity = torch.exp(loss_fct(logits, inputs[\"input_ids\"].view(-1)))\n",
        "    perplexities.append(perplexity.item())\n",
        "    if i % 20 == 0: print(f\"{i}/{len(sentence_subset)}: Perplexity without Hooks: {perplexity.item():.2f}\")\n",
        "print(f\"Average Perplexity without Hooks: {np.mean(perplexities):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c09770ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.nn import CrossEntropyLoss\n",
        "loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "def compute_bert_cross_entropy(sentence, model, tokenizer, replacement=False):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    n_tokens = len(tokens)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(n_tokens):\n",
        "        masked_input_ids = input_ids.copy()\n",
        "        masked_input_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "        inputs = torch.tensor([masked_input_ids])\n",
        "        labels = torch.tensor([input_ids])\n",
        "        labels[0, :i] = -100\n",
        "        labels[0, i+1:] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if replacement:\n",
        "                \n",
        "            else:\n",
        "                outputs = model(inputs, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                losses.append(loss.item())\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "sentence_subset = sentences[:50]\n",
        "cross_entropies = []\n",
        "\n",
        "for i, sent in enumerate(sentence_subset):\n",
        "    ce_loss = compute_bert_cross_entropy(sent, model, tokenizer)\n",
        "    cross_entropies.append(ce_loss)\n",
        "    if i % 2 == 0:\n",
        "        print(f\"{i}/{len(sentence_subset)}: Cross-Entropy = {ce_loss:.4f}\")\n",
        "\n",
        "print(f\"Average Cross-Entropy: {np.mean(cross_entropies):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9418594",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a550aad3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "# from nnsight import InstrumentedModel\n",
        "from nnsight import LanguageModel\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. Setup\n",
        "# ------------------------------------------------------------\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# We'll use the MLM head so logits are token-prediction probabilities\n",
        "hf_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "model = LanguageModel(hf_model)   # wrap for nnsight tracing\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "hf_model.to(device)\n",
        "\n",
        "# Heads you plan to replace\n",
        "replace_heads = [(0, 0), (4, 4), (11, 11)]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Prepare evaluation sentences\n",
        "# ------------------------------------------------------------\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is transforming many industries.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"BERT models are powerful for natural language understanding.\",\n",
        "    \"Machine learning can detect patterns in data.\"\n",
        "]\n",
        "\n",
        "# Simple masking utility: mask one random non-special token per sentence\n",
        "def mask_sentence(sent, tokenizer):\n",
        "    tokens = tokenizer.tokenize(sent)\n",
        "    if not tokens:\n",
        "        return sent\n",
        "    idx = random.randrange(len(tokens))\n",
        "    tokens[idx] = tokenizer.mask_token\n",
        "    return tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "masked_sentences = [mask_sentence(s, tokenizer) for s in sentences]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Utility: compute MLM loss/perplexity for a batch\n",
        "# ------------------------------------------------------------\n",
        "def compute_mlm_loss(model, tokenizer, sentences, device):\n",
        "    \"\"\"Return total cross-entropy loss and perplexity for a list of masked sentences.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        inputs = tokenizer(sent, return_tensors=\"pt\").to(device)\n",
        "        # find masked index\n",
        "        mask_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "        if len(mask_index[0]) == 0:\n",
        "            continue\n",
        "        labels = inputs[\"input_ids\"].clone()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += 1\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Evaluate baseline (no head replacement)\n",
        "# ------------------------------------------------------------\n",
        "print(\"Evaluating baseline BERT ...\")\n",
        "baseline_loss, baseline_ppl = compute_mlm_loss(hf_model, tokenizer, masked_sentences, device)\n",
        "print(f\"Baseline cross-entropy: {baseline_loss:.4f}, Perplexity: {baseline_ppl:.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. Evaluate with head replacements\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\nEvaluating with replaced heads ...\")\n",
        "\n",
        "total_loss_replaced = 0.0\n",
        "total_tokens = 0\n",
        "\n",
        "for sent in tqdm(masked_sentences):\n",
        "    inputs = tokenizer(sent, return_tensors=\"pt\").to(device)\n",
        "    labels = inputs[\"input_ids\"].clone()\n",
        "\n",
        "    # Run a traced forward pass where we inject our hypotheses\n",
        "    with model.trace(inputs) as trace:\n",
        "        # Get seq_len and batch size\n",
        "        seq_len = trace[\"input_ids\"].shape[1]\n",
        "        batch_size = trace[\"input_ids\"].shape[0]\n",
        "\n",
        "        for (layer_i, head_j) in replace_heads:\n",
        "            attn_path = f\"bert.encoder.layer.{layer_i}.attention.self.softmax\"\n",
        "            attn_probs = trace[attn_path]  # shape [batch, num_heads, seq_len, seq_len]\n",
        "\n",
        "            # Obtain your hypothesis nÃn matrix\n",
        "            hypothesis = hypothesize_attention(layer_i, head_j).to(device)\n",
        "\n",
        "            # Ensure shapes match (pad/trim if necessary)\n",
        "            if hypothesis.shape != (seq_len, seq_len):\n",
        "                raise ValueError(f\"Hypothesis shape {hypothesis.shape} != ({seq_len}, {seq_len})\")\n",
        "            attn_probs[:, head_j, :, :] = hypothesis\n",
        "\n",
        "        outputs = trace.output  # forward pass continues with replaced attention\n",
        "\n",
        "    # Compute MLM loss for this sentence\n",
        "    logits = outputs.logits\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fn(logits.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
        "    total_loss_replaced += loss.item()\n",
        "    total_tokens += 1\n",
        "\n",
        "avg_loss_replaced = total_loss_replaced / total_tokens\n",
        "ppl_replaced = math.exp(avg_loss_replaced)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. Compare results\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n=== RESULTS ===\")\n",
        "print(f\"Baseline Cross-Entropy: {baseline_loss:.4f}\")\n",
        "print(f\"Replaced  Cross-Entropy: {avg_loss_replaced:.4f}\")\n",
        "print(f\"Î Cross-Entropy: {avg_loss_replaced - baseline_loss:+.4f}\")\n",
        "print(f\"Baseline Perplexity: {baseline_ppl:.4f}\")\n",
        "print(f\"Replaced  Perplexity: {ppl_replaced:.4f}\")\n",
        "print(f\"Î Perplexity: {ppl_replaced - baseline_ppl:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e172eb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for the bar chart\n",
        "scores = [5.6, 0.0]\n",
        "labels = ['baseline-BERT', 'replaced-BERT']\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'salmon'])\n",
        "plt.ylabel('Score')\n",
        "plt.title('BERT Score Comparison (Bar Chart)')\n",
        "plt.ylim(0, 6) # Ensure the y-axis starts at 0\n",
        "plt.savefig('bert_score_comparison_bar_chart.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b639c4a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318aa7c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hypothesis_head(layer, head, sentence, tokenizer):\n",
        "        hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "        program_to_use = programs_ex[[i for i, p in enumerate(programs_ex) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "        # print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "        out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "        return out\n",
        "\n",
        "def replace_attention_head(module, input, output):\n",
        "    \"\"\" module: BertSelfAttention | output: tuple (context_layer, attention_probs)\"\"\"\n",
        "    context_layer, attn_probs = output\n",
        "    batch, seq_len, hidden_dim = context_layer.shape\n",
        "    num_heads = module.num_attention_heads\n",
        "    head_dim = hidden_dim // num_heads\n",
        "    context_layer = context_layer.view(batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "    # print current sentence\n",
        "    # print(f\"Current Sentence: {inputs['input_ids']}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # decoded_sentences = tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # for decoded_sentence in decoded_sentences:\n",
        "        # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "\n",
        "    layer_id = getattr(module, \"layer_id\", None)\n",
        "    # heads = []\n",
        "    # for (layer, head), (fn_name, score) in best_fits_top:\n",
        "        # if layer == layer_id:\n",
        "            # print(layer, head, fn_name)\n",
        "            # heads.append(head)\n",
        "\n",
        "    # ((5, 1), ('eos_attention', 0.2554249071807607)),\n",
        "    # ((1, 8), ('relative_position_attention', 0.2565078611780894)),\n",
        "    # ((4, 3), ('pronoun_reference', 0.2626135723148083)),\n",
        "    # ((7, 7), ('eos_attention', 0.2630709228109905)),\n",
        "    # ((8, 6), ('eos_attention', 0.2714455279356093)),\n",
        "    # ((3, 1), ('relative_position_attention', 0.2823563840604032)),\n",
        "    # ((3, 7), ('relative_position_attention', 0.2825369197971396)),\n",
        "    # ((7, 3), ('eos_attention', 0.2884066405333515)),\n",
        "    # ((2, 3), ('relative_position_attention', 0.2897239912937058)),\n",
        "    # ((2, 5), ('relative_position_attention', 0.2932481224141057))]\n",
        "\n",
        "    # for layer in range(12):\n",
        "    #     if layer in best_fits_top[0] and layer_id == layer:\n",
        "    \n",
        "    # if layer_id == 0:\n",
        "    #     head = 6\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 4\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 0\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len)) \n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 1:\n",
        "    #     head = 6\n",
        "    #     # cls attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, 0] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     # head = 9\n",
        "    #     # # relative position attention np\n",
        "    #     # activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     # for i in range(seq_len):\n",
        "    #     #     for j in range(seq_len):\n",
        "    #     #         if j > i:\n",
        "    #     #             activation_hypothesis[i, j] = 1.0\n",
        "    #     # activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     # broadcasted_hypothesis = torch.matmul(\n",
        "    #     #     torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #     #     context_layer[:, :, head, :]\n",
        "    #     # )\n",
        "    #     # context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 2:\n",
        "    #     # next attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[0, 1:] = 1.0\n",
        "    #     # activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     head = 0\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 9\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "        \n",
        "    # if layer_id == 7:\n",
        "    #     head = 6\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 8:\n",
        "    #     head = 2\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # zero_out = 10 # percent of all heads\n",
        "    # random_heads = np.random.choice(range(num_heads), size=int(num_heads * zero_out / 100), replace=False)\n",
        "    # random number from 1 to 100 (2 random numbers in a list)\n",
        "    random_heads = np.random.choice(range(num_heads), size=4, replace=False)\n",
        "    # print(len(random_heads))\n",
        "    for i in random_heads:\n",
        "        context_layer[:, :, i, :] = 0\n",
        "\n",
        "    context_layer = context_layer.reshape(batch, seq_len, hidden_dim)\n",
        "    return (context_layer, attn_probs)\n",
        "\n",
        "# target_layer = model_replace.bert.encoder.layer[2].attention.self\n",
        "# hook = target_layer.register_forward_hook(replace_attention_head)\n",
        "\n",
        "for layer_number, layer in enumerate(model_replace.bert.encoder.layer):\n",
        "    layer.attention.self.layer_id = layer_number\n",
        "    # if layer_number in [2]:\n",
        "    layer.attention.self.register_forward_hook(replace_attention_head)\n",
        "\n",
        "ppl_weird = []\n",
        "save_tokens = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(sentences[:100], true_tokens[:100]))):\n",
        "    # calculate full psuedo-perplexity\n",
        "    # inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    # mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    # if len(mask_token_index[0]) == 0:\n",
        "    #     raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    # with torch.no_grad():\n",
        "    #     logits = model_replace(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    # tokens = tokenizer.tokenize(sentence)\n",
        "    # ids = tokenizer.convert_tokens_to_ids(tokens)   \n",
        "    # input_ids = torch.tensor([ids])\n",
        "    # log_probs = []\n",
        "    # for i in range(1, len(tokens)-1):  # consider all tokens except [CLS], [SEP]\n",
        "    #     masked = input_ids.clone()\n",
        "    #     masked[0, i] = tokenizer.mask_token_id\n",
        "    #     with torch.no_grad():\n",
        "    #         logits = model_replace(masked).logits\n",
        "    #     prob = torch.log_softmax(logits[0, i], dim=-1)[input_ids[0, i]]\n",
        "    #     log_probs.append(prob)\n",
        "    # ppl_weird.append(torch.exp(-torch.stack(log_probs).mean()).item())\n",
        "\n",
        "    # calculate single token perplexity\n",
        "    enc = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    input_ids = enc[\"input_ids\"][0]\n",
        "    attention_mask = enc[\"attention_mask\"]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    # for token in tokens:\n",
        "    #     save_tokens.append(token)\n",
        "    per_token_ppl = []\n",
        "\n",
        "    # Loop over tokens EXCLUDING CLS (0) and SEP (last)\n",
        "    for i in range(1, len(input_ids) - 1):\n",
        "        original_id = input_ids[i].item()\n",
        "        original_token = tokens[i]\n",
        "        masked_ids = input_ids.clone()\n",
        "        masked_ids[i] = tokenizer.mask_token_id\n",
        "        with torch.no_grad():\n",
        "            logits = model_replace(\n",
        "                masked_ids.unsqueeze(0),\n",
        "                attention_mask=attention_mask).logits\n",
        "        log_probs = torch.log_softmax(logits[0, i], dim=-1)\n",
        "        log_prob_original = log_probs[original_id]\n",
        "        ppl = torch.exp(-log_prob_original).item()\n",
        "        \n",
        "        # if original_token != \"[MASK]\": \n",
        "        ppl_weird.append(ppl)\n",
        "        save_tokens.append(original_token)\n",
        "\n",
        "# ppl_subset_normal = []\n",
        "# for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:100], true_tokens[:100]))):\n",
        "#     inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "#     mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "#     if len(mask_token_index[0]) == 0:\n",
        "#         raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    \n",
        "#     # calculate full psuedo-perplexity\n",
        "#     tokens = tokenizer.tokenize(sentence)\n",
        "#     ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "#     input_ids = torch.tensor([ids])\n",
        "#     log_probs = []\n",
        "#     for i in range(1, len(tokens)-1):  # consider all tokens except [CLS], [SEP]\n",
        "#         masked = input_ids.clone()\n",
        "#         masked[0, i] = tokenizer.mask_token_id\n",
        "#         with torch.no_grad():\n",
        "#             logits = model(masked).logits\n",
        "#         prob = torch.log_softmax(logits[0, i], dim=-1)[input_ids[0, i]]\n",
        "#         log_probs.append(prob)\n",
        "#     ppl_subset_normal.append(torch.exp(-torch.stack(log_probs).mean()).item())\n",
        "\n",
        "# print('\\n')\n",
        "# get rid of top 5% outliers\n",
        "# ppl_weird = np.percentile(sorted(ppl_weird), 95)\n",
        "\n",
        "print('\\nWeird: ', np.median(ppl_weird), '\\t', np.mean(ppl_weird), '\\t', np.std(ppl_weird))\n",
        "print('Normal: ', np.median(ppl_subset_normal), '\\t', np.mean(ppl_subset_normal), '\\n')\n",
        "\n",
        "print(f\"Average Perplexity weird: pd-describe{(pd.Series(ppl_weird).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f85b2ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_replace = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model_replace.load_state_dict(model.state_dict(), strict=True)\n",
        "model_replace.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1ea626",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normal:  1.806810438632965 \t 7845.270243987441 \t 74461.00454762368\n",
        "# Weird"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c390dd97",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids = enc[\"input_ids\"][0]\n",
        "attention_mask = enc[\"attention_mask\"]\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "per_token_ppl = []\n",
        "\n",
        "# Loop over tokens EXCLUDING CLS (0) and SEP (last)\n",
        "for i in range(1, len(input_ids) - 1):\n",
        "    original_id = input_ids[i].item()\n",
        "    original_token = tokens[i]\n",
        "    masked_ids = input_ids.clone()\n",
        "    masked_ids[i] = tokenizer.mask_token_id\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            masked_ids.unsqueeze(0),\n",
        "            attention_mask=attention_mask).logits\n",
        "    log_probs = torch.log_softmax(logits[0, i], dim=-1)\n",
        "    log_prob_original = log_probs[original_id]\n",
        "    ppl = torch.exp(-log_prob_original).item()\n",
        "    \n",
        "    ppl_weird.append(ppl)\n",
        "    save_tokens.append(original_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ac7d78",
      "metadata": {},
      "outputs": [],
      "source": [
        "zipped_results = list(zip(save_tokens, ppl_weird))\n",
        "np.savetxt('data/twelve_heads_replacement_perplexity_results_500.txt', zipped_results, fmt='%s', delimiter='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ca43e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "for token, ppl in zip(save_tokens, ppl_weird):\n",
        "    print(f\"Token: {token}\\t\\tPerplexity: {ppl}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0a8951",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Setup the sample data (recreating the DataFrame)\n",
        "# The data provided by the user:\n",
        "\n",
        "sentence = full_story\n",
        "\n",
        "# Encode with special tokens\n",
        "enc = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
        "input_ids = enc[\"input_ids\"][0]\n",
        "attention_mask = enc[\"attention_mask\"]\n",
        "\n",
        "# Convert ids â tokens (these include [CLS] and [SEP])\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "per_token_ppl = []\n",
        "\n",
        "# Loop over tokens EXCLUDING CLS (0) and SEP (last)\n",
        "for i in range(1, len(input_ids) - 1):\n",
        "\n",
        "    original_id = input_ids[i].item()\n",
        "    original_token = tokens[i]\n",
        "\n",
        "    masked_ids = input_ids.clone()\n",
        "    masked_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            masked_ids.unsqueeze(0),\n",
        "            attention_mask=attention_mask\n",
        "        ).logits\n",
        "\n",
        "    log_probs = torch.log_softmax(logits[0, i], dim=-1)\n",
        "    log_prob_original = log_probs[original_id]\n",
        "\n",
        "    ppl = torch.exp(-log_prob_original).item()\n",
        "\n",
        "    per_token_ppl.append((original_token, ppl))\n",
        "\n",
        "tokens, perplexities = zip(*per_token_ppl)\n",
        "    \n",
        "raw_data = np.array([\n",
        "    [token, f\"{prob:.2f}\"] for token, prob in zip(tokens, perplexities)\n",
        "])\n",
        "\n",
        "# Create a DataFrame for context (assuming columns were 'Token' and 'Perplexity')\n",
        "df = pd.DataFrame(raw_data, columns=['Token', 'Perplexity'])\n",
        "\n",
        "# Convert Perplexity column to numeric for calculations\n",
        "perplexity_values = df['Perplexity'].astype(float)\n",
        "\n",
        "# 2. Setup the plot and transpose the table data\n",
        "fig, ax = plt.subplots(figsize=(16, 1.25)) # Slightly increased height for statistics text\n",
        "ax.axis('off') # Hide the axis lines\n",
        "\n",
        "# --- Core modification: Transposing the data ---\n",
        "cell_text = df.values.T\n",
        "\n",
        "# Define the new row labels based on the original columns\n",
        "row_labels = ['Token', 'Perplexity']\n",
        "\n",
        "# 3. Create the table with transposed data and updated labels\n",
        "# --- Modification 1: Setting colLabels=None to remove the empty header row ---\n",
        "table = ax.table(\n",
        "    cellText=cell_text,\n",
        "    rowLabels=row_labels,  # 'Token' and 'Perplexity' are now row headers\n",
        "    colLabels=None,        # Removed column labels to eliminate the top empty row\n",
        "    cellLoc='center',\n",
        "    loc='center')\n",
        "\n",
        "# Optional: Adjust table properties\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1, 2) # Scale height slightly for better spacing\n",
        "\n",
        "# 4. Calculate and display statistics below the table\n",
        "median_val = np.median(perplexity_values)\n",
        "mean_val = np.mean(perplexity_values)\n",
        "std_val = np.std(perplexity_values)\n",
        "\n",
        "stats_text = (\n",
        "    f\"Perplexity Statistics | MODEL UNTOUCHED:\\n\"\n",
        "    f\"Median: {median_val:.2f} | Mean: {mean_val:.2f} | Std Dev: {std_val:.0f}\"\n",
        ")\n",
        "\n",
        "# --- Modification 2: Removed plt.title() ---\n",
        "plt.title(stats_text)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe8a9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "765b2886",
      "metadata": {},
      "outputs": [],
      "source": [
        "perplexity_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb26d1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46454c1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nWeird: ', np.median(ppl_zero_30), '\\t', np.mean(ppl_zero_30))\n",
        "print('Normal: ', np.median(ppl_subset_normal), '\\t', np.mean(ppl_subset_normal), '\\n')\n",
        "\n",
        "print(f\"Average Perplexity weird: pd-describe{(pd.Series(ppl_subset_replaced).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
