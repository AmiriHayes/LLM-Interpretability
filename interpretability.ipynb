{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6443588e",
      "metadata": {
        "id": "6443588e"
      },
      "source": [
        "Author: Amiri Hayes \\\n",
        "Date Updated: 7/15/25 \\\n",
        "Title: ViewLLM\n",
        "\n",
        "Starter code !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99",
      "metadata": {
        "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99"
      },
      "outputs": [],
      "source": [
        "# package installs if necessary\n",
        "!pip install matplotlib torch spacy nltk tqdm transformers\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3e36de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3e36de",
        "outputId": "5c233452-f8b9-4137-f8b9-cd2881649f94"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import Callable, Optional\n",
        "from matplotlib.colors import PowerNorm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "print(\"Imports completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012daf28",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Data & Pattern Initialization Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbe22fd",
      "metadata": {
        "id": "7cbe22fd"
      },
      "outputs": [],
      "source": [
        "# POSITIONAL FILTERING PATTERNS:\n",
        "\n",
        "def next_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i+1] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Next Head Attention Pattern\", out\n",
        "\n",
        "def previous_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i-1] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Previous Head Attention Pattern\", out\n",
        "\n",
        "def same_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Same Head Attention Pattern\", out\n",
        "\n",
        "def punctuation_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = tokenizer.convert_ids_to_tokens(toks.input_ids[0])\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    punctuation_indices = [i for i, tok in enumerate(words) if any(p in tok for p in punctuation_set)]\n",
        "    for i in range(len_seq):\n",
        "        future_punct = [j for j in punctuation_indices if j > i]\n",
        "        if future_punct:\n",
        "            for j in future_punct:\n",
        "                out[i, j] = 1.0\n",
        "            out[i] /= out[i].sum()\n",
        "        else:\n",
        "            out[i, i] = 1.0\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Punctuation Pattern\", out\n",
        "\n",
        "def repeated_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    input_ids = toks.input_ids[0].tolist()\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        token_id = input_ids[i]\n",
        "        for j in range(1, len_seq-1):\n",
        "            if input_ids[j] == token_id:\n",
        "                out[i, j] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Repitition Pattern\", out\n",
        "\n",
        "# LINGUISTIC ROLE ALIGNMENT PATTERNS:\n",
        "\n",
        "def pos_alignment(sentence, tokenizer):\n",
        "    toks = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True, add_special_tokens=True)\n",
        "    input_ids = toks.input_ids[0].tolist()\n",
        "    word_ids = toks.word_ids(0)\n",
        "    len_seq = len(input_ids)\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    doc = nlp(sentence)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    pos_to_token_indices = {}\n",
        "    for token_idx, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None or word_idx >= len(pos_tags):\n",
        "            continue\n",
        "        pos = pos_tags[word_idx]\n",
        "        pos_to_token_indices.setdefault(pos, []).append(token_idx)\n",
        "    for token_indices in pos_to_token_indices.values():\n",
        "        if len(token_indices) > 1:\n",
        "            for i in token_indices:\n",
        "                for j in token_indices:\n",
        "                    out[i, j] = 1\n",
        "        else:\n",
        "            i = token_indices[0]\n",
        "            out[i, i] = 1\n",
        "    out[0, 0] = 1\n",
        "    out[-1, -1] = 1\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Part of Speech Pattern\", out\n",
        "\n",
        "def dependencies(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = sentence.split()\n",
        "    doc = nlp(\" \".join(words))\n",
        "    check_errors = False\n",
        "    if check_errors:\n",
        "        if len(doc) == 0: print(\"problem, doc empty\")\n",
        "        if len(doc) != (len_seq-2): print(\"problem, doc length mismatch\", len(doc), len(toks)-2)\n",
        "    for stok in doc:\n",
        "        parent_index = stok.i\n",
        "        for child_stok in stok.children:\n",
        "            child_index = child_stok.i\n",
        "            out[parent_index+1, child_index+1] = 1\n",
        "            out[child_index+1, parent_index+1] = 1\n",
        "    out[0, 0] = 1\n",
        "    out[-1, 0] = 1\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Dependency Parsing Pattern\", out\n",
        "\n",
        "# SEMI-STRUCTURED EVALUATION PATTERN:\n",
        "\n",
        "def chainofthought_pattern(sentence, tokenizer):\n",
        "    out = []\n",
        "    return \"\", out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d51a2fa",
      "metadata": {
        "id": "9d51a2fa"
      },
      "outputs": [],
      "source": [
        "# GENERATE & VISUALIZE ATTENTION PATTERN SCORES\n",
        "\n",
        "def js_divergence(p, q):\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    q = np.clip(q, 1e-12, 1.0)\n",
        "    p /= p.sum()\n",
        "    q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
        "\n",
        "def score_prediction(torch_model, torch_tokenizer, head_loc, pattern, sentence_1, sentence_2: Optional[str] = None, distance=\"jsd\", output=False):\n",
        "    layer, head = head_loc\n",
        "    tokens = torch_tokenizer(sentence_1, return_tensors=\"pt\")\n",
        "\n",
        "    if torch_model.config.is_encoder_decoder: # decoder model case ->\n",
        "        decoder_input_ids = tokens[\"input_ids\"]\n",
        "        outputs = torch_model(input_ids=tokens[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "        att = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "    else: # encoder-decoder model case ->\n",
        "        if sentence_2:\n",
        "            att = 0\n",
        "        else:\n",
        "            att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "    name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    if distance == \"raw\":\n",
        "        score = np.abs(att - pred_att).sum()\n",
        "    elif distance == \"jsd\":\n",
        "      jensonshannon_distances = []\n",
        "      for row_att, row_out in zip(att, pred_att):\n",
        "          jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "      score = np.mean(jensonshannon_distances)\n",
        "\n",
        "    if output == True:\n",
        "        colors=\"inferno\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        im1 = axes[0].imshow(att, cmap=colors, aspect='auto')\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        fig.colorbar(im1, ax=axes[0])\n",
        "        im2 = axes[1].imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        fig.colorbar(im2, ax=axes[1])\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nSentence: \\\"{sentence}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"optimal\":\n",
        "        colors = \"Oranges\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(pred_att, cmap=colors, aspect='auto') #\n",
        "        ax.set_axis_off()\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"actual\":\n",
        "        colors = \"Reds\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5)) # Changed to 1 column, 1 row\n",
        "        im2 = ax.imshow(att, cmap=colors, aspect='auto') # Plot directly on 'ax'\n",
        "        ax.set_title(\"Example Head Attention for Pattern\")\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n899pxRzSWRe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n899pxRzSWRe",
        "outputId": "470ad98d-f61e-4b36-bde9-e6c2bb80882d"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (1):\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file = '/content/drive/MyDrive/small_text.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "sentences = []\n",
        "for paragraph in df['text']:\n",
        "    sentences.extend(sent_tokenize(paragraph))\n",
        "sentences = sentences[:10_000]\n",
        "\n",
        "print(\"Sentences from Tiny Stories Dataset:\")\n",
        "for sentence in sentences[20:30]:\n",
        "  print(f\"\\t{sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mXlJrNbqLLtH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXlJrNbqLLtH",
        "outputId": "1f9567e8-2c9f-4c61-fa9c-e2bedd4fc15a"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (2):\n",
        "\n",
        "splits = {'test': 'abstract_algebra/test-00000-of-00001.parquet', 'validation': 'abstract_algebra/validation-00000-of-00001.parquet', 'dev': 'abstract_algebra/dev-00000-of-00001.parquet'}\n",
        "df = pd.read_parquet(\"hf://datasets/cais/mmlu/\" + splits[\"test\"])\n",
        "df.head()\n",
        "\n",
        "print(\"Sentences from Abstract Algebra Dataset:\")\n",
        "for i in range(0,8):\n",
        "  print(\"\\t\", df.iloc[i]['question'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1015c0ec",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "  Analysis Helper Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff49f93",
      "metadata": {
        "id": "1ff49f93",
        "outputId": "d8f00b5c-d395-4564-885c-d1f65e5c815d"
      },
      "outputs": [],
      "source": [
        "# ANALYZE PATTERN AT LAYER AND HEAD\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "sentence = \"I like apples and I like bananas. I like apples more though.\"\n",
        "\n",
        "models = [\"bert-base-uncased\", \"openai-community/gpt2\", \"google-t5/t5-small\", \"Meta-Llama-3.1-8B-Instruct\"]\n",
        "\n",
        "i = 0\n",
        "name = models[i]\n",
        "\n",
        "if i in [0,1,2]: # small models ( <400M Params )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    model = AutoModel.from_pretrained(name, output_attentions=True)\n",
        "    model.eval()\n",
        "\n",
        "elif i == 3: # big model ( 8B Params, Load from Compute Node )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model.eval()\n",
        "\n",
        "layer, head = 2, 0\n",
        "score_prediction(sentence, model, tokenizer, (layer, head), pos_alignment, distance=\"jsd\", output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd29726",
      "metadata": {
        "id": "9fd29726",
        "outputId": "feeb18a4-11de-4928-9582-fdf4b4c816df"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (1 SENTENCE)\n",
        "\n",
        "def visualize_full_model(sentence, torch_model, torch_tokenizer, pattern, title, bias_towards_best=0.9):\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    num_heads = model.config.num_attention_heads\n",
        "    model_viz = np.zeros((num_layers, num_heads))\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_heads):\n",
        "            score = score_prediction(sentence, torch_model, torch_tokenizer, (i, j), pattern, distance=\"jsd\", output=False)\n",
        "            model_viz[i, j] = score\n",
        "    print(f\"Best Score: {min(map(min, model_viz)):.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=model_viz.min(), vmax=model_viz.max())\n",
        "    plt.imshow(model_viz, cmap='bone', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_three = list(zip(*np.unravel_index(np.argsort(model_viz, axis=None)[:3], model_viz.shape)))\n",
        "    for tuple_val in top_three:\n",
        "        layer, head = tuple_val\n",
        "        score = model_viz[layer, head]\n",
        "        print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return model_viz, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "pattern_name = \"Punctuation Attention Pattern\"\n",
        "underlined_name_unicode = \"\".join([char + '\\u0332' for char in pattern_name])\n",
        "visualize_full_model(sentence, model, tokenizer, punctuation_attention, title=f\"Top Heads: {underlined_name_unicode} | {model_name}\\nSentence: \\\"{sentence}\\\"\\n\", bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G90Rgk6fVKIZ",
      "metadata": {
        "id": "G90Rgk6fVKIZ",
        "outputId": "56ce27bd-6ea5-4f3d-cbf7-f6dc1b69f015"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (MULTIPLE SENTENCES)\n",
        "\n",
        "def visualize_full_model(sentences, torch_model, torch_tokenizer, pattern, title, bias_towards_best=0.9):\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    average_score = np.zeros((num_layers, num_heads))\n",
        "    for sentence in tqdm(sentences):\n",
        "        model_score = np.zeros((num_layers, num_heads))\n",
        "        for i in range(num_layers):\n",
        "            for j in range(num_heads):\n",
        "                score = score_prediction(sentence, torch_model, torch_tokenizer, (i, j), pattern, output=False)\n",
        "                model_score[i, j] = score\n",
        "        average_score += model_score\n",
        "    average_score /= len(sentences)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=average_score.min(), vmax=average_score.max())\n",
        "    plt.imshow(average_score, cmap='bone', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_three = list(zip(*np.unravel_index(np.argsort(average_score, axis=None)[3:], average_score.shape)))\n",
        "    for tuple_val in top_three:\n",
        "            layer, head = tuple_val\n",
        "            score = average_score[layer, head]\n",
        "            print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return average_score, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "visualize_full_model(sentences[0:5], model, tokenizer, punctuation_attention, title=\"Top Heads: Punctuation Attention Pattern [AVERAGE]\\n\"+model_name, bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eTFYV1Gpzv",
      "metadata": {
        "id": "b8eTFYV1Gpzv"
      },
      "outputs": [],
      "source": [
        "# FILTER SENTENCES TO TARGET SPECIFIC PATTERNS\n",
        "\n",
        "def filterer(sentences, length_matters=False, punctuation_matters=False, duplicates=False):\n",
        "    filtered_sentences = []\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    for sentence in sentences:\n",
        "\n",
        "        passes_all_active_filters = True\n",
        "        if length_matters:\n",
        "            if len(sentence.split()) < 5:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if punctuation_matters:\n",
        "            punctuation_count = sum(1 for char in sentence if char in punctuation_set)\n",
        "            if punctuation_count < 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if duplicates:\n",
        "            set_l = len(set(sentence.split()))\n",
        "            sent_1 = len(sentence.split())\n",
        "            if set_l >= sent_1 - 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if passes_all_active_filters: filtered_sentences.append(sentence)\n",
        "\n",
        "    return filtered_sentences\n",
        "\n",
        "filtered_sentences = filterer(sentences, length_matters = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mR56LNrRd37k",
      "metadata": {
        "id": "mR56LNrRd37k",
        "outputId": "4b42cff5-d843-439d-8409-d5c47e9b87a7"
      },
      "outputs": [],
      "source": [
        "# ANALYZE HEAD PATTERN ON SELECTED SENTENCES\n",
        "\n",
        "def visualize_highest_head(sentences, top_n, torch_model, torch_tokenizer, head_loc, pattern):\n",
        "    layer, head = head_loc\n",
        "    scores = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        score = score_prediction(sentence, torch_model, torch_tokenizer, (layer, head), pattern, distance=\"jsd\")\n",
        "        scores.append(score)\n",
        "\n",
        "    indexed_scores = list(enumerate(scores))\n",
        "    sorted_scores = sorted(indexed_scores, key= lambda x: x[1])\n",
        "    top_scores = sorted_scores[:top_n]\n",
        "\n",
        "    for idx, score in top_scores:\n",
        "        print(f\"Sentence #{idx} Score: {score}\")\n",
        "        sentence = sentences[idx]\n",
        "        score_prediction(sentence, torch_model, torch_tokenizer, (layer, head), pattern, output=True)\n",
        "    return 0\n",
        "\n",
        "layer, head = 7, 8\n",
        "visualize_highest_head(filtered_sentences, 8, model, tokenizer, (layer, head), punctuation_attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fdf02a",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Filter Automation Efforts\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8066bd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5510c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# AUTOMATICALLY GENERATE PATTERNS FOR A HEAD\n",
        "\n",
        "def generate_prompt(sentences, layer=7, head=1, top_k_ratio=0.1):\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model_name,\n",
        "        \"examples\": []\n",
        "    }\n",
        "    def scrape_head(att, tokens, ignore_special=True, top_k_ratio=0.1):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append({\n",
        "                f\"from_token_{i}\": tokens[i],\n",
        "                f\"to_token_{j}\": tokens[j],\n",
        "                \"weight\": float(score)\n",
        "            })\n",
        "        return top_activations\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=top_k_ratio)\n",
        "        item = {\n",
        "            \"sentence\": sentence,\n",
        "            \"attention\": top_activations\n",
        "        }\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "    data = json.dumps(data, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    Using the following pieces of data based on {len(sentences[0])} sentences, generate three\n",
        "    hypothesises about the linguistic role the following head is responsible for based on patterns\n",
        "    in the activations. These patterns can be simple or complex. Here are some examples: {data} Then, choose the most fitting hypothesis for the head responsibility using a few examples from the data.\n",
        "    Finally, using the linguistic hypothesis you determine, write a python function which takes in a sentence and tokenizer as parameters and\n",
        "    outputs the name of the pattern you hypothesize along with a 'predicted_matrix' (size: token_len * token_len),\n",
        "    which is the rule-encoded matrix that mirroring attention patterns you'd predict for any given sentence for\n",
        "    Layer {layer}, Head {head}. Feel free to use the capacbilities of proved libraries like spacey and nltk for describing linguistic concepts. Feel free to encode complex functions. Make sure you generalize your hypothesis pattern to any sentence. As examples:,\n",
        "    Layer 3, Head 9 has been found to be responsible for dependency parsing. It's predicted pseudocode would look like:\n",
        "    def dependencies(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = sentence.split()\n",
        "    # use spacey nlp to split word into doc dependency tree\n",
        "    # loop through each node in tree and assign directional attention\n",
        "    # to the matrix 'out' by adding one when there is an outgoing edge.\n",
        "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention\n",
        "    # Normalize out matrix by row (results in uniform attention) and return out\n",
        "    return 'Dependency Parsing Pattern', out\n",
        "    Here is another pseudocode example for one method to implement part-of-speech:\n",
        "    def pos_alignment(sentence, tokenizer):\n",
        "    # assign toks, input_ids, word_ids, len_seq, out, doc\n",
        "    # use spacey to get pos_tags for tokens in docs [token.pos_ for token in doc]\n",
        "    # for token in pos_tags:\n",
        "    # loop through pos_tags and increment out[i,j] when pos_tags match\n",
        "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention\n",
        "    # Normalize out matrix by row (results in uniform attention) and return out\n",
        "    # return 'Part of Speech Implementation 1', out\n",
        "    \"\"\"\n",
        "    return ' '.join(prompt.strip().split())\n",
        "\n",
        "generate_prompt(same_length_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be54500e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(question, choices):\n",
        "    options = [f\"{chr(65 + i)}. {choice}\" for i, choice in enumerate(choices)]\n",
        "    return f\"{question}\\n\" + \"\\n\".join(options) + \"\\nAnswer:\"\n",
        "\n",
        "def print_ans(actual, predicted):\n",
        "        print(f\"Answer | Actual = {actual}, Predicted = {predicted}\")\n",
        "\n",
        "zero_token_id = tokenizer.encode(\"0\", add_special_tokens=False)[0]\n",
        "one_token_id = tokenizer.encode(\"1\", add_special_tokens=False)[0]\n",
        "two_token_id = tokenizer.encode(\"2\", add_special_tokens=False)[0]\n",
        "three_token_id = tokenizer.encode(\"3\", add_special_tokens=False)[0]\n",
        "\n",
        "def evaluate_model_answer(model, tokenizer, question_text, correct_answer_idx):\n",
        "    input_text = question_text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    valid_token_ids = torch.tensor([zero_token_id, one_token_id, two_token_id, three_token_id], device=model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1,  # Generate only one new token\n",
        "            num_return_sequences=1,\n",
        "            do_sample=False,   # For deterministic output\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            output_scores=True, # Get logits to verify\n",
        "            return_dict_in_generate=True\n",
        "        )\n",
        "\n",
        "    predicted_token_id = outputs.sequences[0, -1].item()\n",
        "\n",
        "    if predicted_token_id == zero_token_id: predicted_answer_idx = 0; print_ans(0, 0)\n",
        "    elif predicted_token_id == one_token_id: predicted_answer_idx = 1; print_ans(1, 1)\n",
        "    elif predicted_token_id == two_token_id: predicted_answer_idx = 2; print_ans(2, 2)\n",
        "    elif predicted_token_id == three_token_id: predicted_answer_idx = 3; print_ans(3, 3)\n",
        "    else:\n",
        "        print_ans(correct_answer_idx, predicted_token_id)\n",
        "        return False\n",
        "\n",
        "    return predicted_answer_idx == correct_answer_idx\n",
        "\n",
        "print(\"\\nEvaluating model without hints...\")\n",
        "df['correct_no_hint'] = False # Initialize column\n",
        "\n",
        "for index in tqdm(range(2)):\n",
        "    row = df.iloc[index]\n",
        "    question_with_choices = format_prompt(row['question'], row['choices'])\n",
        "    df.at[index, 'correct_no_hint'] = evaluate_model_answer(\n",
        "        model, tokenizer, question_with_choices, row['answer']\n",
        "    )\n",
        "\n",
        "print(\"\\nEvaluating model with hints...\")\n",
        "df['correct_with_hint'] = False # Initialize column\n",
        "\n",
        "for index in tqdm(range(2)):\n",
        "    row = df.iloc[index]\n",
        "    correct_choice_letter = chr(65 + row['answer'])\n",
        "    question_with_choices = format_prompt(row['question'], row['choices'])\n",
        "    hinted_question = f\"{question_with_choices} (Answer: {correct_choice_letter})\"\n",
        "\n",
        "    df.at[index, 'correct_with_hint'] = evaluate_model_answer(\n",
        "        model, tokenizer, hinted_question, row['answer']\n",
        "    )\n",
        "\n",
        "print(\"\\n\\n--- Evaluation Results ---\")\n",
        "print(f\"Total questions: {len(df)}\")\n",
        "# print(f\"Number correct without hint: {len(df_correct_no_hint)}\")\n",
        "# print(f\"Number incorrect without hint: {len(df_incorrect_no_hint)}\")\n",
        "# print(f\"Number correct with hint: {len(df_correct_with_hint)}\")\n",
        "# print(f\"Number incorrect with hint: {len(df_incorrect_with_hint)}\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20d035f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# INITIAL AUTOMATED / LLM-GENERATED FILTERS\n",
        "\n",
        "def direct_object_prepositional_object_alignment(sentence, tokenizer):\n",
        "    \"\"\"\n",
        "    Hypothesizes that Layer 7, Head 1 is responsible for aligning verbs and prepositions\n",
        "    with their direct or prepositional objects.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence.\n",
        "        tokenizer: The tokenizer object (e.g., AutoTokenizer.from_pretrained(\"bert-base-uncased\")).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A string describing the pattern and a 2D numpy array\n",
        "               representing the predicted attention matrix.\n",
        "    \"\"\"\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    input_ids = toks.input_ids[0]\n",
        "    token_len = len(input_ids)\n",
        "    predicted_matrix = np.zeros((token_len, token_len))\n",
        "\n",
        "    # Get word IDs to align with spaCy tokens\n",
        "    word_ids = toks.word_ids()\n",
        "\n",
        "    # Process sentence with spaCy\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Map spaCy token indices to BERT token indices\n",
        "    spacy_to_bert_map = {}\n",
        "    bert_to_spacy_map = {}\n",
        "    current_spacy_token_idx = -1\n",
        "    for bert_idx, word_id in enumerate(word_ids):\n",
        "        if word_id is not None and (current_spacy_token_idx == -1 or word_id != word_ids[bert_idx - 1]):\n",
        "            current_spacy_token_idx = word_id\n",
        "            spacy_to_bert_map[current_spacy_token_idx] = bert_idx\n",
        "            bert_to_spacy_map[bert_idx] = current_spacy_token_idx\n",
        "        elif word_id is not None:\n",
        "            bert_to_spacy_map[bert_idx] = current_spacy_token_idx\n",
        "\n",
        "    # Iterate through spaCy tokens to find verbs and prepositions and their objects\n",
        "    for i, token in enumerate(doc):\n",
        "        # Find BERT index for the current spaCy token\n",
        "        from_bert_idx_start = -1\n",
        "        for bert_idx, spacy_id in bert_to_spacy_map.items():\n",
        "            if spacy_id == i:\n",
        "                from_bert_idx_start = bert_idx\n",
        "                break\n",
        "\n",
        "        if from_bert_idx_start == -1: # Skip if spaCy token doesn't map to BERT token\n",
        "            continue\n",
        "\n",
        "        # Look for direct objects (dobj) or prepositional objects (pobj)\n",
        "        if token.pos_ == \"VERB\":\n",
        "            for child in token.children:\n",
        "                if child.dep_ == \"dobj\":\n",
        "                    # Distribute attention from the verb to its direct object tokens\n",
        "                    to_bert_idx_start = -1\n",
        "                    for bert_idx, spacy_id in bert_to_spacy_map.items():\n",
        "                        if spacy_id == child.i:\n",
        "                            to_bert_idx_start = bert_idx\n",
        "                            break\n",
        "                    if to_bert_idx_start != -1:\n",
        "                        # Find all BERT tokens that correspond to the spaCy child token\n",
        "                        bert_indices_for_child = [b_idx for b_idx, s_id in bert_to_spacy_map.items() if s_id == child.i]\n",
        "                        if bert_indices_for_child:\n",
        "                            # Assign high attention from the 'from' BERT token (verb)\n",
        "                            # to all BERT tokens that form the 'to' (object)\n",
        "                            for to_b_idx in bert_indices_for_child:\n",
        "                                predicted_matrix[from_bert_idx_start, to_b_idx] = 0.8 # High weight\n",
        "\n",
        "        elif token.pos_ == \"ADP\":  # Adposition (preposition or postposition)\n",
        "            for child in token.children:\n",
        "                if child.dep_ == \"pobj\":\n",
        "                    # Distribute attention from the preposition to its object tokens\n",
        "                    to_bert_idx_start = -1\n",
        "                    for bert_idx, spacy_id in bert_to_spacy_map.items():\n",
        "                        if spacy_id == child.i:\n",
        "                            to_bert_idx_start = bert_idx\n",
        "                            break\n",
        "                    if to_bert_idx_start != -1:\n",
        "                        bert_indices_for_child = [b_idx for b_idx, s_id in bert_to_spacy_map.items() if s_id == child.i]\n",
        "                        if bert_indices_for_child:\n",
        "                            for to_b_idx in bert_indices_for_child:\n",
        "                                predicted_matrix[from_bert_idx_start, to_b_idx] = 0.8 # High weight\n",
        "\n",
        "    # Add self-attention for [CLS] and [SEP] tokens\n",
        "    predicted_matrix[0, 0] = 1.0\n",
        "    predicted_matrix[token_len - 1, token_len - 1] = 1.0\n",
        "\n",
        "    # For any row where no attention has been assigned, distribute attention uniformly\n",
        "    # or assign to [CLS] for general context\n",
        "    for i in range(token_len):\n",
        "        if np.sum(predicted_matrix[i, :]) == 0:\n",
        "            # Fallback: if no specific object found, distribute attention somewhat broadly\n",
        "            # or assign to CLS for general context (this is a heuristic)\n",
        "            predicted_matrix[i, 0] = 0.5 # Attend to CLS for general context\n",
        "            predicted_matrix[i, i] = 0.5 # Self-attention\n",
        "\n",
        "    # Normalize each row to sum to 1\n",
        "    for i in range(token_len):\n",
        "        row_sum = np.sum(predicted_matrix[i, :])\n",
        "        if row_sum > 0:\n",
        "            predicted_matrix[i, :] = predicted_matrix[i, :] / row_sum\n",
        "\n",
        "    return 'Direct Object / Prepositional Object Alignment', predicted_matrix\n",
        "\n",
        "def determiner_noun_phrase_linking(sentence: str, tokenizer) -> tuple[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Hypothesizes attention patterns where determiners link to the nouns\n",
        "    and adjectives within their associated noun phrases.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence.\n",
        "        tokenizer: The tokenizer object (e.g., from Hugging Face Transformers).\n",
        "\n",
        "    Returns:\n",
        "        tuple[str, np.ndarray]: A tuple containing the name of the pattern\n",
        "                                and the predicted attention matrix.\n",
        "    \"\"\"\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    input_ids = toks.input_ids[0]\n",
        "    token_len = len(input_ids)\n",
        "    predicted_matrix = np.zeros((token_len, token_len))\n",
        "\n",
        "    # Get spaCy doc for linguistic analysis\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Create a mapping from tokenizer's token indices to spaCy's token indices\n",
        "    # This is crucial for aligning the attention matrix with linguistic features.\n",
        "    # The tokenizer's `word_ids` method is ideal for this.\n",
        "    word_ids = toks.word_ids(batch_index=0) # Get word_ids for the first (and only) sentence in the batch\n",
        "\n",
        "    for i in range(token_len):\n",
        "        current_word_idx = word_ids[i]\n",
        "        if current_word_idx is not None and current_word_idx < len(doc):\n",
        "            spacy_token = doc[current_word_idx]\n",
        "\n",
        "            # If the current token (from the tokenizer) corresponds to a determiner in spaCy\n",
        "            if spacy_token.pos_ == \"DET\":\n",
        "                # Find the head of the determiner (typically the noun it modifies)\n",
        "                head_spacy_token = spacy_token.head\n",
        "\n",
        "                # Attend from the determiner's subword token(s) to its head's subword token(s)\n",
        "                for j in range(token_len):\n",
        "                    target_word_idx = word_ids[j]\n",
        "                    if target_word_idx is not None and target_word_idx == head_spacy_token.i:\n",
        "                        predicted_matrix[i, j] = 1.0\n",
        "\n",
        "                # Also attend from the determiner's subword token(s) to any adjectives\n",
        "                # that are children of the head and appear before the head\n",
        "                for child in head_spacy_token.children:\n",
        "                    if child.pos_ == \"ADJ\" and child.i < head_spacy_token.i:\n",
        "                        for j in range(token_len):\n",
        "                            target_word_idx = word_ids[j]\n",
        "                            if target_word_idx is not None and target_word_idx == child.i:\n",
        "                                predicted_matrix[i, j] = 1.0\n",
        "\n",
        "\n",
        "    # Apply self-attention for [CLS] and [SEP] tokens\n",
        "    predicted_matrix[0, 0] = 1.0\n",
        "    predicted_matrix[token_len - 1, token_len - 1] = 1.0\n",
        "\n",
        "    # Normalize rows to sum to 1 to represent attention probabilities\n",
        "    # Avoid division by zero for rows that might still be all zeros (e.g., padding tokens)\n",
        "    row_sums = predicted_matrix.sum(axis=1, keepdims=True)\n",
        "    predicted_matrix = np.where(row_sums == 0, 0, predicted_matrix / row_sums)\n",
        "\n",
        "    return \"Determiner-Noun/Adjective-Noun Phrase Linking\", predicted_matrix\n",
        "\n",
        "def verb_phrase_modifier_attention(sentence: str, tokenizer) -> tuple[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Hypothesizes the attention pattern for a head responsible for connecting\n",
        "    verbs to their related phrases and modifiers (subjects, objects, adverbs, PPs).\n",
        "\n",
        "    Args:\n",
        "        sentence: The input sentence.\n",
        "        tokenizer: The tokenizer object (e.g., from Hugging Face Transformers).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            - The name of the hypothesized pattern.\n",
        "            - A NumPy array (predicted_matrix) representing the rule-encoded\n",
        "              attention pattern.\n",
        "    \"\"\"\n",
        "    # Load the English NLP model for spaCy\n",
        "    try:\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        print(\"Downloading en_core_web_sm model for spaCy. Please run 'python -m spacy download en_core_web_sm' once.\")\n",
        "        spacy.cli.download(\"en_core_web_sm\")\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Tokenize the sentence using the provided tokenizer\n",
        "    tokens = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    input_ids = tokens.input_ids[0].tolist()\n",
        "    token_ids = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    len_seq = len(token_ids)\n",
        "    predicted_matrix = np.zeros((len_seq, len_seq))\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Create a mapping from spaCy token index to BERT token indices\n",
        "    # This handles WordPiece tokenization where one spaCy token might be multiple BERT tokens\n",
        "    spacy_to_bert_map = []\n",
        "    current_bert_idx = 1  # Start after [CLS]\n",
        "\n",
        "    for spacy_token in doc:\n",
        "        # Tokenize the spaCy token to get its BERT sub-tokens\n",
        "        bert_sub_tokens = tokenizer.tokenize(spacy_token.text)\n",
        "        bert_indices_for_spacy_token = list(range(current_bert_idx, current_bert_idx + len(bert_sub_tokens)))\n",
        "        spacy_to_bert_map.append(bert_indices_for_spacy_token)\n",
        "        current_bert_idx += len(bert_sub_tokens)\n",
        "\n",
        "    # Iterate through spaCy tokens to identify verbs and their relations\n",
        "    for i, spacy_token in enumerate(doc):\n",
        "        # Get the BERT indices corresponding to the current spaCy token\n",
        "        from_bert_indices = spacy_to_bert_map[i]\n",
        "\n",
        "        # Prioritize attention to verb and its direct dependents\n",
        "        if spacy_token.pos_ == \"VERB\":\n",
        "            # Direct attention from the verb to its subject (nsubj) and direct object (dobj)\n",
        "            for child in spacy_token.children:\n",
        "                if child.dep_ in [\"nsubj\", \"dobj\", \"iobj\", \"attr\", \"acomp\", \"xcomp\", \"prep\", \"advcl\", \"advmod\"]:\n",
        "                    if child.i < len(spacy_to_bert_map): # Ensure child index is within bounds\n",
        "                        to_bert_indices = spacy_to_bert_map[child.i]\n",
        "                        for from_idx in from_bert_indices:\n",
        "                            for to_idx in to_bert_indices:\n",
        "                                if from_idx < len_seq and to_idx < len_seq:\n",
        "                                    predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "            # Also attend from the verb to itself for self-attention\n",
        "            for idx in from_bert_indices:\n",
        "                if idx < len_seq:\n",
        "                    predicted_matrix[idx, idx] = 1.0\n",
        "\n",
        "        # Prioritize attention from subjects/adverbs/prepositions to their governing verb\n",
        "        elif spacy_token.dep_ in [\"nsubj\", \"advmod\", \"prep\", \"aux\", \"auxpass\"]:\n",
        "            if spacy_token.head and spacy_token.head.pos_ == \"VERB\":\n",
        "                head_bert_indices = spacy_to_bert_map[spacy_token.head.i]\n",
        "                for from_idx in from_bert_indices:\n",
        "                    for to_idx in head_bert_indices:\n",
        "                        if from_idx < len_seq and to_idx < len_seq:\n",
        "                            predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "        # Prioritize attention from direct objects/complement to their governing verb\n",
        "        elif spacy_token.dep_ in [\"dobj\", \"iobj\", \"attr\", \"acomp\", \"xcomp\", \"ccomp\", \"acl\"]:\n",
        "            if spacy_token.head and spacy_token.head.pos_ == \"VERB\":\n",
        "                head_bert_indices = spacy_to_bert_map[spacy_token.head.i]\n",
        "                for from_idx in from_bert_indices:\n",
        "                    for to_idx in head_bert_indices:\n",
        "                        if from_idx < len_seq and to_idx < len_seq:\n",
        "                            predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "        # Attention from prepositions to the noun phrase they introduce\n",
        "        elif spacy_token.pos_ == \"ADP\": # Adposition (preposition or postposition)\n",
        "            for child in spacy_token.children:\n",
        "                if child.dep_ == \"pobj\": # Object of preposition\n",
        "                    if child.i < len(spacy_to_bert_map):\n",
        "                        to_bert_indices = spacy_to_bert_map[child.i]\n",
        "                        for from_idx in from_bert_indices:\n",
        "                            for to_idx in to_bert_indices:\n",
        "                                if from_idx < len_seq and to_idx < len_seq:\n",
        "                                    predicted_matrix[from_idx, to_idx] = 1.0\n",
        "                # If the preposition is attached to a verb, also attend back to the verb\n",
        "                if spacy_token.head and spacy_token.head.pos_ == \"VERB\":\n",
        "                    head_bert_indices = spacy_to_bert_map[spacy_token.head.i]\n",
        "                    for from_idx in from_bert_indices:\n",
        "                        for to_idx in head_bert_indices:\n",
        "                            if from_idx < len_seq and to_idx < len_seq:\n",
        "                                predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "        # Adjectives attending to their noun or verb (if copular)\n",
        "        elif spacy_token.pos_ == \"ADJ\":\n",
        "            if spacy_token.head:\n",
        "                if spacy_token.head.pos_ == \"NOUN\" or (spacy_token.head.pos_ == \"VERB\" and spacy_token.dep_ == \"acomp\"):\n",
        "                    head_bert_indices = spacy_to_bert_map[spacy_token.head.i]\n",
        "                    for from_idx in from_bert_indices:\n",
        "                        for to_idx in head_bert_indices:\n",
        "                            if from_idx < len_seq and to_idx < len_seq:\n",
        "                                predicted_matrix[from_idx, to_idx] = 1.0\n",
        "\n",
        "        # Handle attention from [CLS] and [SEP] tokens\n",
        "        # [CLS] token (index 0) often has broad attention or self-attention\n",
        "        predicted_matrix[0, 0] = 1.0\n",
        "        # [SEP] token (last token) often attends to [CLS] or has self-attention\n",
        "        if len_seq > 1:\n",
        "            predicted_matrix[len_seq - 1, 0] = 1.0\n",
        "            predicted_matrix[len_seq - 1, len_seq - 1] = 1.0\n",
        "\n",
        "        # Ensure all rows sum to 1 by distributing any remaining attention to [CLS] or [SEP]\n",
        "    for i in range(len_seq):\n",
        "        current_row_sum = predicted_matrix[i].sum()\n",
        "        if current_row_sum == 0:\n",
        "            # If a row is all zeros, distribute attention to [CLS] and [SEP]\n",
        "            # or to itself if it's [CLS] or [SEP]\n",
        "            if i == 0:  # [CLS] token\n",
        "                predicted_matrix[i, 0] = 1.0\n",
        "            elif i == len_seq - 1:  # [SEP] token\n",
        "                predicted_matrix[i, len_seq - 1] = 1.0\n",
        "            else:\n",
        "                # For other tokens, distribute attention to [CLS] and [SEP]\n",
        "                # You could also consider distributing to the token itself or other meaningful global tokens\n",
        "                predicted_matrix[i, 0] = 0.5\n",
        "                if len_seq > 1:\n",
        "                    predicted_matrix[i, len_seq - 1] = 0.5\n",
        "        else:\n",
        "            predicted_matrix[i] = predicted_matrix[i] / current_row_sum\n",
        "\n",
        "    return \"Verb-Related Phrase and Modifier Focus\", predicted_matrix"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
