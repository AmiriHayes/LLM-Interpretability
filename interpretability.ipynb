{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6443588e",
      "metadata": {
        "id": "6443588e"
      },
      "source": [
        "Author: Amiri Hayes \\\n",
        "Date Updated: 1/20/26 \\\n",
        "Title: ViewLLM\n",
        "\n",
        "Starter code !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99",
      "metadata": {
        "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99"
      },
      "outputs": [],
      "source": [
        "# package installs if necessary\n",
        "!pip install torch spacy nltk tqdm transformers datasets scikit-learn openai\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fb3e36de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3e36de",
        "outputId": "5c233452-f8b9-4137-f8b9-cd2881649f94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\amkah\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import Optional, Tuple, Callable\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import PowerNorm, ListedColormap\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
        "from openai import OpenAI\n",
        "load_dotenv(find_dotenv())\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)\n",
        "print(\"Imports completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012daf28",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Data & Pattern Initialization Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbe22fd",
      "metadata": {
        "id": "7cbe22fd"
      },
      "outputs": [],
      "source": [
        "# IMPORT THE PROGRAM DATABASE:\n",
        "\n",
        "from programs import *\n",
        "from golden_programs import *\n",
        "executables = [obj for name, obj in inspect.getmembers(golden_programs) if inspect.isfunction(obj)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9d51a2fa",
      "metadata": {
        "id": "9d51a2fa"
      },
      "outputs": [],
      "source": [
        "# GENERATE & VISUALIZE ATTENTION PATTERN SCORES FROM PROGRAMS\n",
        "\n",
        "def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    q = np.clip(q, 1e-12, 1.0)\n",
        "    p /= p.sum()\n",
        "    q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
        "\n",
        "def score_prediction(torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: Tuple[int, int], pattern: Callable, sentence_1: str, sentence_2: Optional[str] = None, distance=\"jsd\", output=False):\n",
        "    layer, head = head_loc\n",
        "    tokens = torch_tokenizer(sentence_1, return_tensors=\"pt\")\n",
        "\n",
        "    if torch_model.config.is_encoder_decoder:\n",
        "        decoder_input_ids = tokens[\"input_ids\"]\n",
        "        outputs = torch_model(input_ids=tokens[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "        att = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "        name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    else:\n",
        "        if sentence_2 and pattern.__name__ == \"chainofthought_pattern\":\n",
        "            name = \"Chain of Thought Pattern\"\n",
        "            tokens_2 = torch_tokenizer(sentence_2, return_tensors=\"pt\")\n",
        "\n",
        "            att = torch_model(**tokens_2, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            pred_att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "            if output: print(\"RUNNING FIRST WITH NO HINT\")\n",
        "            question, answer, vector_att = chainofthought_pattern(sentence_1, torch_tokenizer, pred_att, hint=False)\n",
        "            if output: print(\"RUNNING AFTER WITH A HINT\")\n",
        "            question, answer, vector_pred_att = chainofthought_pattern(sentence_2, torch_tokenizer, att, hint=True)\n",
        "\n",
        "            att, pred_att = vector_att.copy(), vector_pred_att.copy()\n",
        "        else:\n",
        "            att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            if pattern.__name__ == \"linear_fit\":\n",
        "                name, pred_att = pattern(sentence_1, torch_tokenizer, idx=0)\n",
        "            else: name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    if distance == \"raw\":\n",
        "        score = np.abs(att - pred_att).sum()\n",
        "    elif distance == \"jsd\":\n",
        "        jensonshannon_distances = []\n",
        "        for row_att, row_out in zip(att, pred_att):\n",
        "            jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "        score = np.mean(jensonshannon_distances)\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\":\n",
        "        score = np.sqrt(js_divergence(att, pred_att))\n",
        "\n",
        "    if output == \"cot\":\n",
        "        colors = \"inferno\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 9))\n",
        "        axes[0].plot(att, color=plt.get_cmap(colors)(0.6))\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        axes[1].plot(pred_att, color=plt.get_cmap(colors)(0.9))\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        bound_axes = False\n",
        "        for i in range(2):\n",
        "            axes[i].set_xlabel(\"Token Index\")\n",
        "            axes[i].set_ylabel(\"Attention Weight\")\n",
        "            axes[i].grid(True)\n",
        "            if bound_axes:\n",
        "                axes[i].set_ylim(0, 1)\n",
        "                axes[i].set_xlim(0, len(att) - 1)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        question_chart = question.replace(\".\", \".\\n\")\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nQuestion: \\\"{question_chart}\\n\\nAnswer: \\\"{answer}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "    \n",
        "    toks = torch_tokenizer([sentence_1], return_tensors=\"pt\")\n",
        "    token_ids = toks[\"input_ids\"][0]\n",
        "    tokens = torch_tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    if output == True:\n",
        "        colors=\"Greens\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        im1 = axes[0].imshow(att, cmap=colors, aspect='auto')\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        fig.colorbar(im1, ax=axes[0])\n",
        "        im2 = axes[1].imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        fig.colorbar(im2, ax=axes[1])\n",
        "        for i in range(2):\n",
        "            axes[i].set_xticks(range(len(tokens)))\n",
        "            axes[i].set_yticks(range(len(tokens)))\n",
        "            # get rid of the weird special characters in each token in tokens\n",
        "            for token in tokens:\n",
        "                if token.startswith(\"Ġ\"):\n",
        "                    tokens[tokens.index(token)] = token[1:]\n",
        "            axes[i].set_xticklabels(tokens, rotation=90)\n",
        "            axes[i].set_yticklabels(tokens)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nSentence: \\\"{sentence_1}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"optimal\":\n",
        "        colors = \"Blues\"\n",
        "        title = name\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        # ax.set_title(title)\n",
        "        im2 = ax.imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        for i in range(1):\n",
        "            ax.set_xticks(range(len(tokens)))\n",
        "            ax.set_yticks(range(len(tokens)))\n",
        "            # get rid of the weird special characters in each token in tokens\n",
        "            for token in tokens:\n",
        "                if token.startswith(\"Ġ\"):\n",
        "                    tokens[tokens.index(token)] = token[1:]\n",
        "            ax.set_xticklabels(tokens, rotation=90)\n",
        "            ax.set_yticklabels(tokens)\n",
        "        # ax.set_axis_off()\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"actual\":\n",
        "        colors = \"Blues\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(att, cmap=colors, aspect='auto')\n",
        "        # ax.set_title(\"Activation Example: Bert Layer 3, Head 9\")\n",
        "        for i in range(1):\n",
        "            ax.set_xticks(range(len(tokens)))\n",
        "            ax.set_yticks(range(len(tokens)))\n",
        "            # get rid of the weird special characters in each token in tokens\n",
        "            for token in tokens:\n",
        "                if token.startswith(\"Ġ\"):\n",
        "                    tokens[tokens.index(token)] = token[1:]\n",
        "            ax.set_xticklabels(tokens, rotation=90)\n",
        "            ax.set_yticklabels(tokens)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb95d92",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the math500 data, make a list of sentences from the dataset\n",
        "# curl -X GET \\ \"curl -X GET \\ \"https://datasets-server.huggingface.co/rows?dataset=HuggingFaceH4%2FMATH-500&config=default&split=test&offset=0&length=100\"\"\n",
        "\n",
        "import requests\n",
        "response = requests.get(\"https://datasets-server.huggingface.co/rows?dataset=HuggingFaceH4%2FMATH-500&config=default&split=test&offset=0&le\")\n",
        "data = response.text.splitlines()\n",
        "\n",
        "data_json = json.loads(\"\".join(data))\n",
        "math_sentences = []\n",
        "for i, item in enumerate(data_json['rows']):\n",
        "    if i > 6 and i > 10: print(f\"{i}:\\t{item['row']['problem']}\")\n",
        "    math_sentences.append(item['row']['problem'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "n899pxRzSWRe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n899pxRzSWRe",
        "outputId": "470ad98d-f61e-4b36-bde9-e6c2bb80882d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences from Tiny Stories Dataset:\n",
            "\tWhen it was time to go home, Beep knew he needed more fuel.\n",
            "\tHe went to the fuel place and got more healthy fuel.\n",
            "\tNow, Beep was ready to go fast and play again the next day.\n",
            "\tAnd Beep lived happily ever after.\n",
            "\tOne day, a little fish named Fin was swimming near the shore.\n",
            "\tHe saw a big crab and wanted to be friends.\n",
            "\t\"Hi, I am Fin.\n",
            "\tDo you want to play?\"\n",
            "\tasked the little fish.\n",
            "\tThe crab looked at Fin and said, \"No, I don't want to play.\n",
            "\n",
            "Generic Sentences:\n",
            "\tThe sun dipped below the horizon, painting the sky with vibrant hues of orange, pink, and purple.\n",
            "\tShe wondered, 'Will he ever understand the complexities of this intricate problem?'\n",
            "\tDespite the heavy rain, the children played joyfully outside, splashing in puddles, laughing, and shouting.\n",
            "\tThe old, creaky house, standing on the hill, seemed to whisper secrets of times long past, didn't it?\n",
            "\tReading a good book, especially on a quiet afternoon, can transport you to another world, full of adventure and mystery.\n",
            "\tHe packed his bags carefully: clothes, toiletries, a map, and a sense of hopeful anticipation for the journey ahead.\n",
            "\tThe aroma of freshly baked bread, warm and inviting, filled the entire kitchen, making everyone hungry.\n",
            "\tWhy did the chicken cross the road? To get to the other side, obviously, but what was on the other side?\n",
            "\tThe intricate details of the ancient tapestry, woven with skill and precision, told a fascinating story of kings, queens, and battles.\n",
            "\tIf you truly want to succeed, you must work diligently, persevere through challenges, and never give up on your dreams.\n"
          ]
        }
      ],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (1):\n",
        "\n",
        "file = 'data/small_text.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "sentences = []\n",
        "for paragraph in df['text']:\n",
        "    sentences.extend(sent_tokenize(paragraph))\n",
        "\n",
        "sentences = sentences[:10_000]\n",
        "print(\"Sentences from Tiny Stories Dataset:\")\n",
        "for sentence in sentences[20:30]:\n",
        "  print(f\"\\t{sentence}\")\n",
        "\n",
        "df_json = pd.read_json('data/generic_sentences.json')\n",
        "generic_sentences = df_json[0].tolist()\n",
        "print(\"\\nGeneric Sentences:\")\n",
        "for sentence in generic_sentences[:10]:\n",
        "  print(f\"\\t{sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0adb7458",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (2):\n",
        "\n",
        "math_data = pd.read_json('data/math_problems_results.jsonl', lines=True)\n",
        "\n",
        "filtered_results = math_data[\n",
        "    (math_data['consistency'] == \"False\") &\n",
        "    (math_data['evaluated_answer_nohint'] != \"DNF: llm did not finish\") &\n",
        "    (math_data['evaluated_answer_hint'] != \"DNF: llm did not finish\")\n",
        "]\n",
        "\n",
        "answers_nohint = filtered_results['answer_nohint'].tolist()\n",
        "answers_hint = filtered_results['answer_hint'].tolist()\n",
        "prefix = \"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nuser\\n\\n\"\n",
        "\n",
        "prompts = []\n",
        "for s1, s2 in zip(answers_nohint, answers_hint):\n",
        "    if s1.startswith(prefix): s1 = s1[len(prefix):]\n",
        "    if s2.startswith(prefix): s2 = s2[len(prefix):]\n",
        "\n",
        "    i_suffix_s1 = s1.find(\"assistant\")\n",
        "    if i_suffix_s1 != -1: s1 = s1[:i_suffix_s1].strip()\n",
        "\n",
        "    i_suffix_s2 = s2.find(\"assistant\")\n",
        "    if i_suffix_s2 != -1: s2 = s2[:i_suffix_s2].strip()\n",
        "\n",
        "    if s1 and s2: prompts.append((s1, s2))\n",
        "\n",
        "print(len(prompts), \"relevant prompts loaded from math problems dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1015c0ec",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "  Analysis Helper Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5703d5da",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    output_hidden_states=True,\n",
        "    output_attentions=True,\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "id": "1ff49f93",
      "metadata": {
        "id": "1ff49f93",
        "outputId": "d8f00b5c-d395-4564-885c-d1f65e5c815d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x500 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "np.float64(0.5508890247286303)"
            ]
          },
          "execution_count": 342,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ANALYZE PATTERN AT LAYER AND HEAD\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "# sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "# sentence = \"I like apples and I like bananas. I like apples more though.\"\n",
        "\n",
        "models = [\"bert-base-uncased\", \"openai-community/gpt2\", \"google-t5/t5-small\", \"Meta-Llama-3.1-8B-Instruct\"]\n",
        "\n",
        "i = 0\n",
        "name = models[i]\n",
        "\n",
        "if i in [0,1,2]: # small models ( <400M Params )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    model = AutoModel.from_pretrained(name, output_attentions=True)\n",
        "    model.eval()\n",
        "\n",
        "elif i == 3: # big model & cot ( 8B Params, Load from Compute Node )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt_num = 0\n",
        "    sentence = prompts[prompt_num][0]  # Use the prompt's first sentence (no hint)\n",
        "    sentence_with_hint = prompts[prompt_num][1]  # Use prompt's second sentence (hint)\n",
        "\n",
        "import numpy as np\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "\n",
        "import numpy as np\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "\n",
        "def dependencies(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = sentence.split()\n",
        "    doc = nlp(\" \".join(words))\n",
        "    check_errors = False\n",
        "    if check_errors:\n",
        "        if len(doc) == 0: print(\"problem, doc empty\")\n",
        "        if len(doc) != (len_seq-2): print(\"problem, doc length mismatch\", len(doc), len(toks)-2)\n",
        "    for stok in doc:\n",
        "        parent_index = stok.i\n",
        "        for child_stok in stok.children:\n",
        "            child_index = child_stok.i\n",
        "            out[parent_index+1, child_index+1] = 1\n",
        "            out[child_index+1, parent_index+1] = 1\n",
        "\n",
        "    # get index of the root token\n",
        "    root_index = None\n",
        "    for stok in doc:\n",
        "        if stok.head == stok:\n",
        "            root_index = stok.i\n",
        "            break\n",
        "    print(\"Root index:\", root_index)\n",
        "    print(\"Root token:\", doc[root_index].text)\n",
        "\n",
        "    # # remove attention below lower diagonal\n",
        "    for i in range(len_seq):\n",
        "        for j in range(i):\n",
        "            out[i, j] = 0\n",
        "            out[i, 0] = 0.8\n",
        "            out[i, -1] = 0.2\n",
        "            \n",
        "    out[0, 0] = 1\n",
        "    out[-1, 0] = 1\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Dependency Parsing Pattern\", out\n",
        "\n",
        "layer, head = 3,9\n",
        "# sentence = \"he said no, he did not, he will not, he feels weirdly good about it\"_\n",
        "# math_sentence = \"If (3x + 5) * 11 = 22, what is the value of x?\"\n",
        "score_prediction(model, tokenizer, (layer, head), programs[34], sentence, distance=\"jsd\", output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd29726",
      "metadata": {
        "id": "9fd29726",
        "outputId": "feeb18a4-11de-4928-9582-fdf4b4c816df"
      },
      "outputs": [],
      "source": [
        "decoder_patterns = [previous_attention, same_attention, cls_attention, relative_position_attention, uniform_attention, special_token_attention]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "G90Rgk6fVKIZ",
      "metadata": {
        "id": "G90Rgk6fVKIZ",
        "outputId": "56ce27bd-6ea5-4f3d-cbf7-f6dc1b69f015"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'answers_nohint' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m average_score, top_three\n\u001b[32m     53\u001b[39m model_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.config.architectures[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m sentences_zipped = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43manswers_nohint\u001b[49m[:\u001b[32m5\u001b[39m], answers_hint[:\u001b[32m5\u001b[39m]))\n\u001b[32m     55\u001b[39m visualize_full_model(sentences_zipped, model, tokenizer, relative_position_attention, title=\u001b[33m\"\u001b[39m\u001b[33mTop Heads: Chain-of_Thought Evaluation [AVERAGE]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m+model_name, bias_towards_best=\u001b[32m0.7\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'answers_nohint' is not defined"
          ]
        }
      ],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (MULTIPLE SENTENCES)\n",
        "\n",
        "def visualize_full_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    header = [\"Layer\", \"Head\", \"Score\"]\n",
        "    csv_file_name = \"scores.csv\"\n",
        "    file_exists = os.path.exists(csv_file_name)\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "    \n",
        "        average_score = np.zeros((num_layers, num_heads))\n",
        "        for sentence in sentences:\n",
        "            sentence_1 = sentence[0]  # first sentence (no hint)\n",
        "            sentence_2 = sentence[1]  # second sentence (hint)\n",
        "            model_score = np.zeros((num_layers, num_heads))\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    score = score_prediction(model, tokenizer, (i, j), chainofthought_pattern, sentence_1, sentence_2, distance=\"jsd\", output=False)\n",
        "                    writer.writerow([i, j, f\"{score:.2f}\"])\n",
        "                    print(f\"Layer {i}, Head {j} - Score: {score:.2f}\")\n",
        "                    model_score[i, j] = score\n",
        "            average_score += model_score\n",
        "        average_score /= len(sentences)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=average_score.min(), vmax=average_score.max())\n",
        "    plt.imshow(average_score, cmap='Reds', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\": \n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score.ravel())[::-1][:3], average_score.shape))) # highest scores\n",
        "    else:\n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score, axis=None)[3:], average_score.shape))) # lowest scores\n",
        "        top_three = np.sort(average_score)\n",
        "\n",
        "    for tuple_val in top_three:\n",
        "            layer, head = tuple_val\n",
        "            score = average_score[layer, head]\n",
        "            print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return average_score, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentences_zipped = list(zip(answers_nohint[:5], answers_hint[:5]))\n",
        "visualize_full_model(sentences_zipped, model, tokenizer, relative_position_attention, title=\"Top Heads: Chain-of_Thought Evaluation [AVERAGE]\\n\"+model_name, bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eTFYV1Gpzv",
      "metadata": {
        "id": "b8eTFYV1Gpzv"
      },
      "outputs": [],
      "source": [
        "# FILTER SENTENCES TO TARGET SPECIFIC PATTERNS\n",
        "\n",
        "def filterer(sentences: list[str], length_matters: bool=False, punctuation_matters: bool=False, duplicates: bool=False) -> list[str]:\n",
        "    filtered_sentences = []\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    for sentence in sentences:\n",
        "        passes_all_active_filters = True\n",
        "        if length_matters:\n",
        "            if len(sentence.split()) < 5:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if punctuation_matters:\n",
        "            punctuation_count = sum(1 for char in sentence if char in punctuation_set)\n",
        "            if punctuation_count < 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if duplicates:\n",
        "            set_l = len(set(sentence.split()))\n",
        "            sent_1 = len(sentence.split())\n",
        "            if set_l >= sent_1 - 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if passes_all_active_filters: filtered_sentences.append(sentence)\n",
        "    return filtered_sentences\n",
        "\n",
        "filtered_sentences = filterer(generic_sentences, length_matters = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mR56LNrRd37k",
      "metadata": {
        "id": "mR56LNrRd37k",
        "outputId": "4b42cff5-d843-439d-8409-d5c47e9b87a7"
      },
      "outputs": [],
      "source": [
        "# ANALYZE HEAD PATTERN ON SELECTED SENTENCES\n",
        "\n",
        "def visualize_highest_head(sentences: list[str], top_n:  int, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: tuple[int, int], pattern: Callable):\n",
        "    layer, head = head_loc\n",
        "    scores = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        score = score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, distance=\"jsd\", output=False)\n",
        "        scores.append(score)\n",
        "\n",
        "    indexed_scores = list(enumerate(scores))\n",
        "    sorted_scores = sorted(indexed_scores, key= lambda x: x[1])\n",
        "    top_scores = sorted_scores[:top_n]\n",
        "\n",
        "    for idx, score in top_scores:\n",
        "        print(f\"Sentence #{idx} Score: {score}\")\n",
        "        sentence = sentences[idx]\n",
        "        score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, output=True)\n",
        "    return 0\n",
        "\n",
        "layer, head = 7, 8\n",
        "visualize_highest_head(filtered_sentences, 3, model, tokenizer, (layer, head), punctuation_attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be980bdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS\n",
        "\n",
        "def classify_whole_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, patterns: list[Callable]) -> dict[Tuple[int, int], Tuple[str, float]]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "    activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "    header = [\"i\", \"j\", \"Pattern\", \"Score\"]\n",
        "    \n",
        "    csv_file_name = \"data/best_fit_refinement_gpt2_new.csv\"\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        file_exists = os.path.exists(csv_file_name)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "\n",
        "        for pattern in patterns:\n",
        "            try:\n",
        "                print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "                all_scores = []\n",
        "                for idx, sentence in enumerate(sentences):\n",
        "                    if idx % 20 == 0: print(f\"\\tProcessing sentence {idx}/{len(sentences)}\")\n",
        "                    for i in range(num_layers):\n",
        "                        for j in range(num_heads):\n",
        "                            score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                            if score < 0.55: print(f\"sentence #{idx}|\", i, j, score)\n",
        "                            all_scores.append(score)\n",
        "\n",
        "                average_scores = np.array(all_scores).reshape(len(sentences), num_layers * num_heads).mean(axis=0)\n",
        "                head_performance = average_scores.reshape(num_layers, num_heads)\n",
        "                print(head_performance)\n",
        "\n",
        "                ix, jx = np.where(head_performance < 0.45)\n",
        "                pairs = list(zip(ix, jx))\n",
        "\n",
        "                for (ix, jx) in pairs:\n",
        "                    print(ix, jx, head_performance[ix, jx])\n",
        "                    # if key not in activations or pattern_score < activations[key][1]:\n",
        "                    #     activations[key] = (pattern.__name__, pattern_score)\n",
        "                    writer.writerow([ix, jx, pattern.__name__, head_performance[ix, jx]])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing pattern {pattern.__name__}: {e}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            # print(len(avg_scores))\n",
        "            # pattern_score = np.mean(avg_scores)\n",
        "            # print(i, j, pattern_score)\n",
        "\n",
        "            # if pattern_score > 0.5: continue\n",
        "            # key = (i, j)\n",
        "\n",
        "            # if key not in activations or pattern_score < activations[key][1]:\n",
        "            #     activations[key] = (pattern.__name__, pattern_score)\n",
        "            # writer.writerow([i, j, pattern.__name__, pattern_score])\n",
        "\n",
        "    return activations\n",
        "\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "# patterns = [next_attention, previous_attention, same_attention, punctuation_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "activations = classify_whole_model(generic_sentences[:5], model, tokenizer, patterns)\n",
        "print(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6a8ddd",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# NEVERMIND, loop through all heads find the corresponsing programs from llm_code in automation_results_bert and bert2 and save to best_fit_refinement_bert.csv if score < 0.4\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    for head in range(num_heads):\n",
        "        # print(f\"Layer {layer}, Head {head}\")\n",
        "        # look for layer, head in automation_results_bert/scores (e.g. C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\automation_results_bert\\scores\\layer0_head0_score.txt)\n",
        "        with open(f\"automation_results_gpt2/scores/layer{layer}_head{head}_score.txt\", 'r') as f:\n",
        "            score_1 = float(f.read().strip())\n",
        "\n",
        "        if score_1 < 0.25:\n",
        "            # copy the name of the corresponding program from automation_results_bert/llm_code (e.g. C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\automation_results_bert\\llm_code\\layer0_head0_code.py)\n",
        "            with open(f\"automation_results_bert/llm_code/layer{layer}_head{head}_code.py\", 'r') as f:\n",
        "                # program name is after \"def \" and before \"(\", isn't necessarily the first line, read all lines and join\n",
        "                all_lines = f.readlines()\n",
        "\n",
        "                # strip all_lines of stuff before def and after ( \n",
        "                program_name = \" \".join([line.strip().split(\" \")[1] for line in all_lines if line.startswith(\"def \")])\n",
        "                program_name = program_name.split(\"(\")[0]\n",
        "\n",
        "            # save to best_fit_refinement_bert.csv\n",
        "            # with open('best_fit_refinement_bert.csv', 'a') as f:\n",
        "            #     f.write(f\"{layer},{head},{program_name},{score_1}\\n\")\n",
        "            print(f\"{layer},{head},{program_name},{score_1}\")\n",
        "\n",
        "            # for som reason the append isn't working\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70ecc7a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from programs import *\n",
        "import importlib.util\n",
        "import types\n",
        "patterns = [next_attention, previous_attention, same_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "master_list_dir = \"automation_refinement_bert/master_list\"\n",
        "for filename in os.listdir(master_list_dir):\n",
        "    if filename.endswith(\".py\"):\n",
        "        module_name = filename[:-3]\n",
        "        if module_name not in [p.__name__ for p in patterns]:\n",
        "            try:\n",
        "                program_path = os.path.join(master_list_dir, filename)\n",
        "                spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                module.__dict__['np'] = np\n",
        "                # module.__dict__['torch'] = torch\n",
        "                module.__dict__['spacy'] = spacy\n",
        "                module.__dict__['nlp'] = nlp\n",
        "                module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "                module.__dict__['Optional'] = Optional\n",
        "                module.__dict__['Tuple'] = Tuple\n",
        "                module.__dict__['Callable'] = Callable\n",
        "                spec.loader.exec_module(module)\n",
        "            except Exception as e:\n",
        "                print(f\"Program loading failed: {str(e)}\")\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    program_to_test = attr\n",
        "                    break\n",
        "\n",
        "            patterns.append(program_to_test)\n",
        "\n",
        "master_list_dir = \"automation_refinement/master_list\"\n",
        "for filename in os.listdir(master_list_dir):\n",
        "    if filename.endswith(\".py\"):\n",
        "        module_name = filename[:-3]\n",
        "        if module_name not in [p.__name__ for p in patterns]:\n",
        "            try:\n",
        "                program_path = os.path.join(master_list_dir, filename)\n",
        "                spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                module.__dict__['np'] = np\n",
        "                # module.__dict__['torch'] = torch\n",
        "                module.__dict__['spacy'] = spacy\n",
        "                module.__dict__['nlp'] = nlp\n",
        "                module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "                module.__dict__['Optional'] = Optional\n",
        "                module.__dict__['Tuple'] = Tuple\n",
        "                module.__dict__['Callable'] = Callable\n",
        "                spec.loader.exec_module(module)\n",
        "            except Exception as e:\n",
        "                print(f\"Program loading failed: {str(e)}\")\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    program_to_test = attr\n",
        "                    break\n",
        "\n",
        "            patterns.append(program_to_test)\n",
        "\n",
        "print(f\"Total patterns to analyze: {len(patterns)}\")\n",
        "for pattern in patterns: print(f\"\\tPattern: {pattern.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08432fb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS (CONTINUED)\n",
        "\n",
        "model = AutoModel.from_pretrained(\"google-t5/t5-small\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "\n",
        "torch_model = model\n",
        "torch_tokenizer = tokenizer\n",
        "\n",
        "num_layers = torch_model.config.num_hidden_layers\n",
        "num_heads = torch_model.config.num_attention_heads\n",
        "activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "\n",
        "from programs import *\n",
        "patterns = [next_attention, previous_attention, same_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "master_list_dir = \"automation_refinement_gpt2/master_list\"\n",
        "for filename in os.listdir(master_list_dir):\n",
        "    if filename.endswith(\".py\"):\n",
        "        module_name = filename[:-3]\n",
        "        if module_name not in [p.__name__ for p in patterns]:\n",
        "            try:\n",
        "                program_path = os.path.join(master_list_dir, filename)\n",
        "                spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                module.__dict__['np'] = np\n",
        "                # module.__dict__['torch'] = torch\n",
        "                module.__dict__['spacy'] = spacy\n",
        "                module.__dict__['nlp'] = nlp\n",
        "                module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "                module.__dict__['Optional'] = Optional\n",
        "                module.__dict__['Tuple'] = Tuple\n",
        "                module.__dict__['Callable'] = Callable\n",
        "                spec.loader.exec_module(module)\n",
        "            except Exception as e:\n",
        "                print(f\"Program loading failed: {str(e)}\")\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    program_to_test = attr\n",
        "                    break\n",
        "\n",
        "            patterns.append(program_to_test)\n",
        "\n",
        "print(f\"Total patterns to analyze: {len(patterns)}\")\n",
        "for pattern in patterns: print(f\"\\tPattern: {pattern.__name__}\")\n",
        "\n",
        "short = sentences[:8]\n",
        "csv_file_name = \"data/best_fit_refinement.csv\"\n",
        "with open(csv_file_name, 'a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for pattern in patterns:\n",
        "        print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "        avg_score = []\n",
        "        for idx, sentence in enumerate(short):\n",
        "            print(f\"\\tProcessing sentence {idx}/{len(short)}\")\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    if i != 3 or j != 9: continue\n",
        "                    score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                    if score < 0.55:\n",
        "                        avg_score.append((idx, pattern.__name__, i, j, score))\n",
        "        \n",
        "        score_dict = {}\n",
        "        for idx, pattern_name, i, j, score in avg_score:\n",
        "            score_dict.setdefault((i, j), []).append((pattern_name, score))\n",
        "        for (i, j), values in score_dict.items():\n",
        "            scores = [score for _, score in values]\n",
        "            avg_score_val = sum(scores) / len(scores)\n",
        "            pattern_name = values[0][0]\n",
        "            activations[(i, j)] = (pattern_name, avg_score_val)\n",
        "            print(f\"Layer {i}, Head {j} - Score: {avg_score_val:.2f}\")\n",
        "            writer.writerow([i, j, pattern.__name__, avg_score_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8374c077",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANALYZE EFFECT OF LINEAR WEIGHTS ON ATTENTION ACTIVATION ACCURACY\n",
        "\n",
        "def generate_dataset(patterns: list[Callable], model: PreTrainedModel, tokenizer: PreTrainedTokenizerBase, sentences: list[str], layer_head: tuple[int, int]):\n",
        "    layer, head = layer_head\n",
        "    X_data, y_data = [], []\n",
        "    print(\"Generating dataset for Layer\", layer, \", Head\", head)\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            attn = outputs.attentions[layer][0, head]\n",
        "        X_i_list = []\n",
        "        for pattern in patterns:\n",
        "            _, X_i = pattern(sentence, tokenizer)\n",
        "            X_i = torch.tensor(X_i, dtype=torch.float32)\n",
        "            X_i_list.append(X_i)\n",
        "        X_data.append(X_i_list)\n",
        "        y_data.append(attn)\n",
        "\n",
        "    torch.save({'X': X_data, 'y': y_data}, \"data/attention_dataset.pt\")\n",
        "    print(\"Dataset generated and saved to 'data/attention_dataset.pt'.\")\n",
        "\n",
        "def train_linearregression() -> pd.DataFrame:\n",
        "    data = torch.load(\"data/attention_dataset.pt\")\n",
        "    X, y = data['X'], data['y']\n",
        "    X, y = data['X'], data['y']\n",
        "    output = []\n",
        "\n",
        "    for i, (xb, yb) in enumerate(zip(X, y)):\n",
        "        xb = torch.stack(xb)\n",
        "        X_flat = (xb.reshape(len(xb), -1).T).numpy()\n",
        "        y_flat = yb.flatten().numpy()\n",
        "        reg = LinearRegression().fit(X_flat, y_flat)\n",
        "        if i % 100 == 0: print(f\"Sentence #{i} - Coeffs: {[float(f\"{coef:.2f}\") for coef in reg.coef_]}, Intercept: {reg.intercept_:.2f}\")\n",
        "        output.append([reg.coef_.tolist(), float(reg.intercept_)])\n",
        "\n",
        "    output = pd.DataFrame(output, columns=[\"Coefficients\", \"Intercept\"]).to_csv(\"data/linear_regression_results.csv\", index=False)\n",
        "    return output\n",
        "\n",
        "head_loc = (3, 9)\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "generate_dataset(patterns, model, tokenizer, sentences, head_loc)\n",
        "output = train_linearregression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1612da99",
      "metadata": {},
      "outputs": [],
      "source": [
        "# VISUALIZE BEST FIT PATTERNS ACROSS LAYERS AND HEADS\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"roberta-base\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "threshold = 0.45\n",
        "\n",
        "df = pd.read_csv('data/best_fit_refinement_gpt2_new.csv')\n",
        "# ignore columns where scores >= threshold\n",
        "df = df[df['Score'] < threshold]\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "mat = np.zeros((num_layers, num_heads), dtype=object)\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        mat[r, c] = []\n",
        "\n",
        "for (i, j), group in df.groupby(['i', 'j']):\n",
        "    sorted_group = group.sort_values(by='Score', ascending=False)\n",
        "    mat[i, j] = [(row['Pattern'], row['Score']) for idx, row in sorted_group.iterrows()]\n",
        "\n",
        "unique_patterns = df['Pattern'].unique()\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import ListedColormap, to_rgb\n",
        "\n",
        "colors_40 = plt.cm.get_cmap('tab20').colors + plt.cm.get_cmap('tab20b').colors\n",
        "custom_cmap_40 = ListedColormap(colors_40, name='tab40')\n",
        "cmap_patterns = plt.get_cmap(custom_cmap_40, len(unique_patterns))\n",
        "\n",
        "pattern_colors = {pattern: cmap_patterns(i) for i, pattern in enumerate(unique_patterns)}\n",
        "white_color = (1, 1, 1, 1)\n",
        "plotting_matrix_rgb = np.zeros((num_layers, num_heads, 3))\n",
        "\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        cell_data = mat[r, c]\n",
        "        \n",
        "        if not cell_data:\n",
        "            plotting_matrix_rgb[r, c] = white_color[:3]\n",
        "        elif len(cell_data) == 1: \n",
        "            pattern_name = cell_data[0][0]\n",
        "            plotting_matrix_rgb[r, c] = pattern_colors[pattern_name][:3]\n",
        "        else:\n",
        "            top_pattern_1 = cell_data[0][0]\n",
        "            top_pattern_2 = cell_data[1][0]\n",
        "            color_1 = pattern_colors[top_pattern_1][:3]\n",
        "            color_2 = pattern_colors[top_pattern_2][:3]\n",
        "            plotting_matrix_rgb[r, c] = color_1 \n",
        "\n",
        "custom_draw_mask = np.zeros((num_layers, num_heads), dtype=bool)\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        if len(mat[r, c]) > 1:\n",
        "            custom_draw_mask[r, c] = True\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 18))\n",
        "img = ax.imshow(plotting_matrix_rgb, origin='lower', extent=[-0.5, num_heads - 0.5, -0.5, num_layers - 0.5])\n",
        "\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        if custom_draw_mask[r, c]:\n",
        "            cell_data = mat[r, c]\n",
        "            top_pattern_1 = cell_data[0][0]\n",
        "            top_pattern_2 = cell_data[1][0]\n",
        "            \n",
        "            color_1 = pattern_colors[top_pattern_1]\n",
        "            color_2 = pattern_colors[top_pattern_2]\n",
        "            triangle1 = mpatches.Polygon(\n",
        "                [[c - 0.5, r - 0.5], [c + 0.5, r - 0.5], [c + 0.5, r + 0.5]],\n",
        "                facecolor=color_1, edgecolor='none'\n",
        "            )\n",
        "            ax.add_patch(triangle1)\n",
        "            triangle2 = mpatches.Polygon(\n",
        "                [[c - 0.5, r - 0.5], [c - 0.5, r + 0.5], [c + 0.5, r + 0.5]],\n",
        "                facecolor=color_2, edgecolor='none'\n",
        "            )\n",
        "            ax.add_patch(triangle2)\n",
        "\n",
        "ax.set_xticks(np.arange(num_heads))\n",
        "ax.set_yticks(np.arange(num_layers))\n",
        "ax.set_xticks(np.arange(-0.5, num_heads, 1), minor=True)\n",
        "ax.set_yticks(np.arange(-0.5, num_layers, 1), minor=True)\n",
        "ax.set_xlabel(f'{model.config.architectures[0]} - Heads', fontsize=14)\n",
        "ax.set_ylabel(f'{model.config.architectures[0]} - Layers', fontsize=14)\n",
        "# center title across whole figure\n",
        "# ax.set_title('What is GPT doing? - Best Fit Patterns Across All Heads and Layers', fontsize=18, pad=20, loc='center')\n",
        "ax.set_aspect('equal')\n",
        "# ax.grid(color='black', linestyle='-', linewidth=0.5)\n",
        "ax.grid(which='minor', color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "legend_handles = []\n",
        "for pattern, color in pattern_colors.items():\n",
        "    patch = mpatches.Patch(color=color, label=pattern)\n",
        "    legend_handles.append(patch)\n",
        "\n",
        "# center title\n",
        "title_value_text = 'What is BERT doing? - Best Fit Patterns Across Attention Heads'\n",
        "underlined_title_unicode = \"\".join([char + '\\u0332' for char in title_value_text])\n",
        "# plt.suptitle(underlined_title_unicode, fontsize=20, y=0.725, x=0.4)\n",
        "# sort handles alphabetically by label\n",
        "# legend_handles = sorted(legend_handles, key=lambda x: x.get_label())\n",
        "\n",
        "white_patch = mpatches.Patch(facecolor=white_color, label='No Hypotheses Validated', edgecolor='black', linewidth=0.2)\n",
        "legend_handles.append(white_patch)\n",
        "\n",
        "name = 'Confidence'\n",
        "underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "\n",
        "ax.legend(\n",
        "    handles=legend_handles, \n",
        "    loc='center left', \n",
        "    bbox_to_anchor=(1.05, 0.5),\n",
        "    ncol=3, \n",
        "    fancybox=True, \n",
        "    shadow=True, \n",
        "    title=f\"HIGH SCORING HYPOTHESES - {model.config.architectures[0]} | {underlined_name_unicode}: avg_score < {threshold}\\n\",\n",
        "    title_fontsize=16,\n",
        "    fontsize='large', # Make legend text bigger. Can use 'medium', 'x-large', 'xx-large' or a numerical value (e.g., 12)\n",
        "    labelspacing=1.5, # Adjust vertical spacing between legend entries (default is 0.5)\n",
        "    handlelength=2.5, # Adjust length of the color patch/line in the legend\n",
        "    handletextpad=0.8, # Adjust space between the handle (color patch) and the text label\n",
        "    borderpad=0.5 # Adjust padding between the legend content and its border \n",
        ") \n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1]) # Adjust layout to make space for the legend\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42abc485",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    instructions=\"You are a coding assistant that talks like a pirate.\",\n",
        "    input=\"How do I check if a Python object is an instance of a class?\",\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45650d28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt pieces (sequential)\n",
        "# 1 look at attention matrix and try to predict pattern\n",
        "# 2. come up with five candidate hypotheses and pick the top one\n",
        "# 3. explain why you picked that hypothesis\n",
        "# 4. write code that generates that pattern according to the hypothesis\n",
        "\n",
        "from automation_helper import example_one, example_two, example_three\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "layer = 3\n",
        "head = 9\n",
        "sentences = sentences[:10]\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model.config.architectures[0],\n",
        "        \"examples\": []}\n",
        "\n",
        "def handle_score(score):\n",
        "        # convert to percentage with 0 decimal places\n",
        "        return \"{:.0f}\".format(score * 100)\n",
        "\n",
        "def scrape_head(att, tokens, top_k_ratio, ignore_special=True):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append(f\"[{str(tokens[i])}|{str(tokens[j])}:{handle_score(score)}]\")\n",
        "        #make top activations str and delete brackets\n",
        "        top_activations_str = \" \".join(top_activations).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "        # print(top_activations_str)\n",
        "        return top_activations_str\n",
        "\n",
        "for idx, sentence in enumerate(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=0.05)\n",
        "        item = {f\"sentence {idx}\": \" \".join(tokens), \"sentence attention\": top_activations}\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "prompt_1 = f\"\"\"\n",
        "        You are given the following attention matrices sourced from {model.config.architectures[0]} based on {len(sentences)} sentences\n",
        "        from Layer {layer}, Head {head}. Look at the attenion matrix subset below and try to predict the most fitting three hypotheses\n",
        "        for the head function. Then choose what you think is the best hypothesis and explain why you picked that one. DATA: {data}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_2 = f\"\"\"\n",
        "        Now use your explanation to write a Python function that generates an attention matrix for any given sentence according to your hypothesis  in less than 50 lines. The input to your function is def fn(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: and \n",
        "        the output is a tuple of a string (the name of the pattern) and a numpy array (the attention matrix). Your response should be a single code block with no extra text, and must be wrapped in ```python``` and include imports.\n",
        "        Think carefully before generating any code. These patterns can be simple or complex.  For uniformity, the first three lines of your function must be 'toks = tokenizer([sentence], return_tensors=\"pt\") len_seq = len(toks.input_ids[0]) out = np.zeros((len_seq, len_seq))'.\n",
        "        Make sure the token sequences from your tokenizer and spaCy (if you must use spaCy) are aligned via a dictionary if necessary, because they split text differently. Make sure you generalize your hypothesis pattern to any sentence. Functions can almost \n",
        "        always be expressed in fewer than 50 lines of code. As examples, it has been discovered one head is responsible for the complex task of dependency parsing. It's simplistic predicted pseudocode looks like: \n",
        "        {example_one}. Example 2: '''{example_two}''' Example 3: '''{example_three}'''. Always finalize your code with normalization to ensure rows sum to 1 and the output is a valid attention matrix.\n",
        "        \"\"\"\n",
        "\n",
        "# ----- AGENT GENERATES HYPOTHESIS ----- #\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a coding assistant well-versed in linguistics.\"}\n",
        "]\n",
        "conversation_history.append({\"role\": \"user\", \"content\": prompt_1})\n",
        "response_1 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", messages=conversation_history, temperature = 0.5\n",
        ")\n",
        "assistant_response_1 = response_1.choices[0].message.content\n",
        "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "\n",
        "# ----- AGENT WRITES PYTHON PROGRAM ----- #\n",
        "prompt_2 = \"Now, give me a Python function that uses a linguistic concept from your previous explanation.\"\n",
        "conversation_history.append({\"role\": \"user\", \"content\": prompt_2})\n",
        "response_2 = client.chat.completions.create(\n",
        "    model=\"gpt-5\",\n",
        "    messages=conversation_history\n",
        ")\n",
        "assistant_response_2 = response_2.choices[0].message.content\n",
        "print(f\"\\n--- Response 2 ---\\n{assistant_response_2}\")\n",
        "\n",
        "# ----- SAVE AGENT RESULTS ----- #\n",
        "folder = \"automation_results_gpt4o\"\n",
        "if not os.path.exists(folder): os.makedirs(folder)\n",
        "subfolders = [\"prompts\", \"llm_code\"]\n",
        "for subfolder in subfolders:\n",
        "    if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "        os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "with open(os.path.join(folder, \"prompts\", f\"layer{layer}_head{head}_prompts.txt\"), \"w\") as f:\n",
        "    f.write(f\"--- Response 1 ---\\n{assistant_response_1}\\n\\n--- Response 2 ---\\n{assistant_response_2}\")\n",
        "\n",
        "python_snippet = assistant_response_2.split(\"```python\")[1].split(\"```\")[0].strip()\n",
        "with open(os.path.join(folder, \"llm_code\", f\"layer{layer}_head{head}_code.py\"), \"w\") as f:\n",
        "    f.write(python_snippet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c1a30c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#helpers\n",
        "\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "def parse_model_output(output_text: str):\n",
        "    \"\"\"\n",
        "    Robustly parse model output with 'hypothesis' and 'program' keys.\n",
        "    Handles triple quotes, code fences, and inconsistent indentation.\n",
        "    Ignores extra text after the JSON object.\n",
        "    \"\"\"\n",
        "    # 1️⃣ Remove outer markdown fences\n",
        "    cleaned = re.sub(r\"^```(?:json)?|```$\", \"\", output_text.strip(), flags=re.MULTILINE).strip()\n",
        "\n",
        "    # 2️⃣ Handle malformed JSON with triple quotes\n",
        "    if '\"\"\"' in cleaned:\n",
        "        hyp_match = re.search(r'\"hypothesis\"\\s*:\\s*\"([^\"]+)\"', cleaned)\n",
        "        prog_match = re.search(r'\"program\"\\s*:\\s*(\"\"\"|```python|```)(.*?)(\\1|```)', cleaned, re.DOTALL)\n",
        "        if not hyp_match or not prog_match:\n",
        "            raise ValueError(\"Could not locate hypothesis or program block.\")\n",
        "        hypothesis = hyp_match.group(1).strip()\n",
        "        program_raw = prog_match.group(2)\n",
        "        program = textwrap.dedent(program_raw).strip()\n",
        "        program = program.replace('\\r\\n', '\\n')\n",
        "        return {\"hypothesis\": hypothesis, \"program\": program}\n",
        "\n",
        "    # 3️⃣ Parse valid JSON normally, but extract only the first JSON object\n",
        "    # This ignores any extra text after the first closing brace\n",
        "    json_match = re.search(r\"\\{.*?\\}\\s*(?=\\n|$)\", cleaned, flags=re.DOTALL)\n",
        "    if not json_match:\n",
        "        raise ValueError(\"No JSON object found in model output.\")\n",
        "\n",
        "    json_str = json_match.group(0)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Invalid JSON: {e}\\nExtracted text:\\n{json_str}\")\n",
        "\n",
        "    if not all(k in data for k in (\"hypothesis\", \"program\")):\n",
        "        raise KeyError(f\"Missing keys. Found: {list(data.keys())}\")\n",
        "\n",
        "    # Clean code block & indentation\n",
        "    program_raw = re.sub(r\"^```(?:python)?|```$\", \"\", data[\"program\"].strip(), flags=re.MULTILINE)\n",
        "    program = textwrap.dedent(program_raw).strip().replace('\\r\\n', '\\n')\n",
        "\n",
        "    return {\n",
        "        \"hypothesis\": data[\"hypothesis\"].strip(),\n",
        "        \"program\": program,\n",
        "    }\n",
        "\n",
        "import traceback\n",
        "import importlib.util\n",
        "import types\n",
        "def validate_program(program_path, model, tokenizer, layer, head, sentences):       \n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        spec.loader.exec_module(module)\n",
        "    except Exception as e:\n",
        "        print(f\"Program loading failed: {str(e)}\")\n",
        "        return str(e)\n",
        "\n",
        "    for attr_name in dir(module):\n",
        "        attr = getattr(module, attr_name)\n",
        "        if isinstance(attr, types.FunctionType):\n",
        "            program_to_test = attr\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        # print(\"Scoring program...\")\n",
        "        score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        error = traceback.format_exc()\n",
        "        full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "        return full_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aea170",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "#print name, num heads, num layers, \n",
        "print(model.config.architectures[0])\n",
        "print(model.config.num_attention_heads)\n",
        "print(model.config.num_hidden_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6ba30d",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "torch_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "torch_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "        if (layer, head) in [(0, 1), (0, 2), (0, 3), (0, 4), (0,5)]: continue\n",
        "\n",
        "        if sq_score[layer, head] < 0.7: continue\n",
        "        \n",
        "        # if (layer, head) not in fails: continue\n",
        "\n",
        "        success = False\n",
        "\n",
        "        print(f\"Layer {layer}, Head {head}\")\n",
        "        # max_attempts = 3  # Initial try + two more attempt \n",
        "        head_attempts = 0\n",
        "\n",
        "        # while head_attempts < max_attempts:\n",
        "        if success: break\n",
        "\n",
        "        try:\n",
        "            # from automation_helper import generate_prompt, validate_program\n",
        "            # print(\"got here\")\n",
        "            fullprompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "\n",
        "            conversation_history = [\n",
        "                {\"role\": \"system\",\n",
        "                \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "            ]\n",
        "            conversation_history.append({\"role\": \"user\", \"content\": fullprompt})\n",
        "            response_1 = client.chat.completions.create(\n",
        "                model=\"gpt-4o\", messages=conversation_history\n",
        "            )\n",
        "            assistant_response_1 = response_1.choices[0].message.content\n",
        "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "            print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "            try:\n",
        "                parsed = parse_model_output(assistant_response_1)\n",
        "            except:\n",
        "                continue\n",
        "            feedback = \"invalid_output\"\n",
        "\n",
        "            folder = \"automation_results_bert_2\"\n",
        "            if not os.path.exists(folder): os.makedirs(folder)\n",
        "            subfolders = [\"prompts\", \"llm_code\", \"scores\"]\n",
        "            for subfolder in subfolders:\n",
        "                if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "                    os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "            hypothesis_path = f\"{folder}/prompts/layer{layer}_head{head}_prompts.txt\"\n",
        "            with open(hypothesis_path, \"w\") as f: f.write(parsed[\"hypothesis\"])\n",
        "\n",
        "            python_path = f\"{folder}/llm_code/layer{layer}_head{head}_code.py\"\n",
        "            with open(python_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "\n",
        "            import traceback\n",
        "            import importlib.util\n",
        "            import types\n",
        "            def validate_program(program_path, model, tokenizer, layer, head, sentences):       \n",
        "                try:\n",
        "                    spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                    module = importlib.util.module_from_spec(spec)\n",
        "                    module.__dict__['np'] = np\n",
        "                    spec.loader.exec_module(module)\n",
        "                except Exception as e:\n",
        "                    print(f\"Program loading failed: {str(e)}\")\n",
        "                    return str(e)\n",
        "\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        program_to_test = attr\n",
        "                        break\n",
        "\n",
        "                try:\n",
        "                    # print(\"Scoring program...\")\n",
        "                    head_scores = []\n",
        "                    for sentence in sentences[:10]:\n",
        "                            score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "                            head_scores.append(score)\n",
        "                    score = np.mean(head_scores)\n",
        "                    return score\n",
        "                except Exception as e:\n",
        "                    error = traceback.format_exc()\n",
        "                    full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "                    return full_error\n",
        "\n",
        "            max_refinements = 2 # no refinements\n",
        "            while type(feedback) is str or feedback > 0.7:\n",
        "                if max_refinements >= 3: break\n",
        "                \n",
        "                feedback = validate_program(python_path, model, tokenizer, layer, head, sentences)\n",
        "                print(feedback)\n",
        "                if isinstance(feedback, np.float64) and feedback <= 0.7:\n",
        "                    with open(python_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "                    success = True\n",
        "                    break\n",
        "\n",
        "                if type(feedback) is str:\n",
        "                    conversation_history.append({\"role\": \"user\", \"content\": f\"The following error was encountered when running your code: {feedback}. Please fix your code and return json with two keys: 'hypothesis' and 'program', where hypothesis is the explanation of what you fixed and program is the updated program. Here is your previous code: {parsed['program']}\"})\n",
        "                else:\n",
        "                    conversation_history.append({\"role\": \"user\", \"content\": f\"The score of your program was {feedback:.2f}, which is not good enough. Please refine your code and return json with two keys: 'hypothesis' and 'program', where hypothesis is the explanation of what you fixed and program is the updated program. Here is your previous code: {parsed['program']}\"})\n",
        "                response_2 = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    messages=conversation_history\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    parsed = parse_model_output(response_2.choices[0].message.content)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "                    continue\n",
        "\n",
        "                with open(os.path.join(folder, \"prompts\", f\"layer{layer}_head{head}_prompts_try{max_refinements}.txt\"), \"w\") as f:\n",
        "                    f.write(parsed[\"hypothesis\"])\n",
        "                \n",
        "                max_refinements += 1\n",
        "\n",
        "            score = feedback\n",
        "            score_path = f\"{folder}/scores/layer{layer}_head{head}_score.txt\"\n",
        "            with open(score_path, \"w\") as f: f.write(str(score))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing Layer {layer}, Head {head}: {e}\")\n",
        "            head_attempts += 1\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e3efaf5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# check that gpt_call works as expected\n",
        "\n",
        "prompt = generate_prompt(sentences[:25], model, tokenizer, (3, 9), top_k_ratio=0.025)\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "]\n",
        "conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", messages=conversation_history\n",
        ")\n",
        "assistant_response = response.choices[0].message.content\n",
        "print(assistant_response)\n",
        "try:\n",
        "    output = parse_model_output(assistant_response)\n",
        "except:\n",
        "    output = \"parsing_error\"\n",
        "    \n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "id": "5ac696c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gpt_call(prompt):\n",
        "    conversation_history = [\n",
        "        {\"role\": \"system\",\n",
        "        \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "    ]\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\", messages=conversation_history\n",
        "    )\n",
        "    assistant_response = response.choices[0].message.content\n",
        "    # print(assistant_response)\n",
        "    return assistant_response\n",
        "    \n",
        "def make_program_executable(program_str: str) -> Callable:\n",
        "    import importlib.util\n",
        "    import types\n",
        "    import tempfile\n",
        "    import os\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\") as temp_file:\n",
        "        temp_file.write(program_str.encode())\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"loaded_program\", temp_file_path)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "        module.__dict__['Optional'] = Optional\n",
        "        module.__dict__['Tuple'] = Tuple\n",
        "        module.__dict__['Callable'] = Callable\n",
        "        module.__dict__['spacy'] = spacy\n",
        "        spec.loader.exec_module(module)\n",
        "    finally:\n",
        "        os.remove(temp_file_path)\n",
        "\n",
        "    for attr_name in dir(module):\n",
        "        attr = getattr(module, attr_name)\n",
        "        if isinstance(attr, types.FunctionType):\n",
        "            return attr\n",
        "\n",
        "    raise ValueError(\"No function found in the provided program.\")\n",
        "\n",
        "def diff(program, model, tokenizer, sentences):\n",
        "    # take a short list (5 examples for each failed program) of the token pairs with the biggest difference between the actual activations and the hypothesis matrix.\n",
        "    # return a list of tuples (token1, token2, actual_score, predicted_score)\n",
        "    differences = []\n",
        "    # activation is a matrix i, j of actual attention scores\n",
        "    # program is a function that takes in a sentence and tokenizer and returns a matrix of predicted\n",
        "\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        activations = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().cpu().numpy()\n",
        "\n",
        "        predicted = program(sentence, tokenizer)[1]\n",
        "\n",
        "        seq_len = activations.shape[0]\n",
        "        for i in range(seq_len):\n",
        "            for j in range(seq_len):\n",
        "                actual_score = activations[i, j]\n",
        "                predicted_score = predicted[i, j]\n",
        "                token_i = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][i].item())\n",
        "                token_j = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][j].item())\n",
        "                differences.append([token_i, token_j, actual_score, predicted_score])\n",
        "\n",
        "    differences = sorted(differences, key=lambda x: abs(x[2] - x[3]), reverse=True)\n",
        "    differences = differences[:10]\n",
        "    actual_scores = [f\"{diff[2]:.4f}\" for diff in differences]\n",
        "    predicted_scores = [f\"{diff[3]:.4f}\" for diff in differences]\n",
        "    differences = [[diff[0], diff[1], actual, predicted] for diff, actual, predicted in zip(differences, actual_scores, predicted_scores)]\n",
        "    return differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05336f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------- GREEDY REFINEMENT PIPELINE ---------------\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "from automation_helper import generate_prompt\n",
        "\n",
        "save_path = \"automation_results_bert_greedy\"\n",
        "subdirectories = [\"llm_code\", \"scores\"]\n",
        "for subdir in subdirectories:\n",
        "    dir_path = os.path.join(save_path, subdir)\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "        if (layer, head) in [(0,0), (0,1), (0,2), (0,3)]: continue\n",
        "        try:\n",
        "\n",
        "            score_1_path = os.path.join(\"automation_results_bert\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "            score_2_path = os.path.join(\"automation_results_bert_2\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "            score_1 = float(open(score_1_path).read()) if os.path.exists(score_1_path) else 1.0\n",
        "            score_2 = float(open(score_2_path).read()) if os.path.exists(score_2_path) else 1.0\n",
        "            if score_1 < 0.4 or score_2 < 0.4:\n",
        "                print(f\"Skipping Layer {layer}, Head {head} due to existing low score ({min(score_1, score_2):.2f})\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nAnalyzing Layer {layer}, Head {head} of [{model.config.architectures[0]} - {model_name}]\")\n",
        "            prompt = generate_prompt(sentences[:25], model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "            raw_response = gpt_call(prompt)\n",
        "            try:\n",
        "                assistant_response = parse_model_output(raw_response)\n",
        "            except:\n",
        "                max_retries = 0\n",
        "                while max_retries < 3:\n",
        "                    print(\"\\tParsing error encountered. Retrying...\")\n",
        "                    assistant_response = gpt_call(prompt)\n",
        "                    if assistant_response != \"parsing_error\": break\n",
        "                    max_retries += 1\n",
        "\n",
        "                if max_retries == 3: \n",
        "                    print(f\"\\tFailed to parse after {max_retries} attempts. Skipping Layer {layer}, Head {head}.\")\n",
        "                    continue\n",
        "            program = assistant_response['program']\n",
        "            program_path = os.path.join(save_path, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "            with open(program_path, \"w\") as f: f.write(program.rstrip(\"}\"))\n",
        "            feedback = validate_program(program_path, model, tokenizer, layer, head, sentences)\n",
        "            print(f\"\\tProgram parsed successfully. Feedback: {feedback}\")\n",
        "\n",
        "            retries = 0\n",
        "            failed_programs = []\n",
        "            while (isinstance(feedback, float) and feedback > 0.4) or (isinstance(feedback, str)) and retries < 10:\n",
        "                if isinstance(feedback, float) and feedback <= 0.4: break\n",
        "\n",
        "                retries += 1\n",
        "                refinement = \"\"\n",
        "                if isinstance(feedback, float):\n",
        "                    try:\n",
        "                        executable_program = make_program_executable(program)\n",
        "                        failed_programs.append(executable_program.__name__)\n",
        "                        tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "                        activations = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().cpu().numpy()\n",
        "                        top_differences = diff(executable_program, model, tokenizer, sentences[:25])\n",
        "                        refinement += f\"\\nPrevious failed programs: {failed_programs}. The top 10 differences in form (token1, token2, actual_score, predicted_score): {top_differences}\"\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "                \n",
        "                raw_response = gpt_call(prompt + refinement)\n",
        "                assistant_response = parse_model_output(raw_response)\n",
        "                try:\n",
        "                    program = assistant_response['program']\n",
        "                except Exception as e:\n",
        "                    print(f\"\\tParsing error on refinement {retries}: {e}\")\n",
        "                    feedback = \"parsing_error\"\n",
        "                    continue\n",
        "                program_path = os.path.join(save_path, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "                with open(program_path, \"w\") as f: f.write(program.rstrip(\"}\"))\n",
        "                feedback = validate_program(program_path, model, tokenizer, layer, head, sentences)\n",
        "\n",
        "            score = feedback\n",
        "            llm_code_path = os.path.join(save_path, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "            score_path = os.path.join(save_path, \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "            with open(llm_code_path, \"w\") as f: f.write(program.rstrip(\"}\"))\n",
        "            with open(score_path, \"w\") as f: f.write(str(score))\n",
        "            print(f\"\\tFinal score for Layer {layer}, Head {head}: {score}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\tError processing Layer {layer}, Head {head}: {e}\")\n",
        "            continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88a9b0ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "differences = []\n",
        "    # activation is a matrix i, j of actual attention scores\n",
        "    # program is a function that takes in a sentence and tokenizer and returns a matrix of predicted\n",
        "\n",
        "for sentence in sentences[:25]:\n",
        "    tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    activations = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().cpu().numpy()\n",
        "    predicted = executable_program(sentence, tokenizer)[1]\n",
        "\n",
        "    seq_len = activations.shape[0]\n",
        "    for i in range(seq_len):\n",
        "        for j in range(seq_len):\n",
        "            actual_score = activations[i, j]\n",
        "            predicted_score = predicted[i, j]\n",
        "            token_i = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][i].item())\n",
        "            token_j = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][j].item())\n",
        "            differences.append([token_i, token_j, actual_score, predicted_score])\n",
        "\n",
        "differences = sorted(differences, key=lambda x: abs(x[2] - x[3]), reverse=True)\n",
        "differences = differences[:10]\n",
        "actual_scores = [f\"{diff[2]:.4f}\" for diff in differences]\n",
        "predicted_scores = [f\"{diff[3]:.4f}\" for diff in differences]\n",
        "differences = [[diff[0], diff[1], actual, predicted] for diff, actual, predicted in zip(differences, actual_scores, predicted_scores)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b357d9b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(seq_len)\n",
        "for i in range(seq_len):\n",
        "    print(\"-----\")\n",
        "    for j in range(seq_len):\n",
        "        print(activations[i,j])\n",
        "        # how to get the token corresponding to index i and j\n",
        "        print(tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][i].item()), tokenizer.convert_ids_to_tokens(tokens['input_ids'][0][j].item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886ded6c",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(actual_score, predicted_score, diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ec0ce1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#print the code that makes up executable_program\n",
        "import inspect\n",
        "print(inspect.getsource(executable_program))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3e6325",
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate through scores folder and count how many are floats and how many are strings\n",
        "# edit the following code to count nan as a failure\n",
        "\n",
        "folder = r\"automation_results_gpt2\\scores\"\n",
        "float_count = 0\n",
        "string_count = 0\n",
        "nan_count = 0\n",
        "no_code_count = 0\n",
        "\n",
        "fails = []\n",
        "\n",
        "for i in range(12):\n",
        "    for j in range(12):\n",
        "        layer, head = i, j\n",
        "        if not os.path.exists(os.path.join(folder, f\"layer{layer}_head{head}_score.txt\")):\n",
        "            fails.append((layer, head))\n",
        "            no_code_count += 1\n",
        "\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\"_score.txt\"):\n",
        "        with open(os.path.join(folder, filename), \"r\") as f:\n",
        "            content = f.read().strip()\n",
        "            try:\n",
        "                value = float(content)\n",
        "                if value != value:  # Check for NaN\n",
        "                    nan_count += 1\n",
        "                    layer = int(filename.split(\"_\")[0][5:])\n",
        "                    head = int(filename.split(\"_\")[1][4:])\n",
        "                    fails.append((layer, head))\n",
        "                else:\n",
        "                    float_count += 1\n",
        "            except ValueError:\n",
        "                layer = int(filename.split(\"_\")[0][5:])\n",
        "                head = int(filename.split(\"_\")[1][4:])\n",
        "                fails.append((layer, head))\n",
        "                string_count += 1\n",
        "\n",
        "print(f\"Float scores: {float_count}\")\n",
        "print(f\"String scores: {string_count}\")\n",
        "print(f\"NaN scores: {nan_count}\")\n",
        "print(f\"No code scores: {no_code_count}\")\n",
        "print(f\"Total scores: {float_count + string_count + nan_count + no_code_count}\\n\\n\")\n",
        "print(fails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac782014",
      "metadata": {},
      "outputs": [],
      "source": [
        "# retest the bert scores using existing functions with 100 sentences for each head\n",
        "\n",
        "retest = [(11, 10), (4,11)]\n",
        "\n",
        "scores = []\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "\n",
        "        if (layer, head) not in retest: continue\n",
        "\n",
        "        print(f\"Layer {layer}, Head {head}\")\n",
        "        # find file in llm_code e.g. layer0_head0_code\n",
        "        program_path = f\"automation_results_bert/llm_code/layer{layer}_head{head}_code.py\"\n",
        "\n",
        "        try:\n",
        "            spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "        except Exception as e:\n",
        "            print(f\"Program loading failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        for attr_name in dir(module):\n",
        "            attr = getattr(module, attr_name)\n",
        "            if isinstance(attr, types.FunctionType):\n",
        "                program_to_test = attr\n",
        "                break\n",
        "\n",
        "        head_scores = []\n",
        "        for sentence in sentences[:100]:\n",
        "            try:\n",
        "                score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "                head_scores.append(score)\n",
        "            except Exception as e:\n",
        "                error = traceback.format_exc()\n",
        "                full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "                continue\n",
        "        \n",
        "        scores.append(np.mean(head_scores))\n",
        "        print(f\"Average Score for Layer {layer}, Head {head}: {np.mean(head_scores)}\")\n",
        "        # overwrite score file\n",
        "        score_path = f\"automation_results_bert/scores/layer{layer}_head{head}_score.txt\"\n",
        "        with open(score_path, \"w\") as f: \n",
        "            f.write(str(np.mean(head_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84555c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# folder = r\"automation_results_bert\\scores\"\n",
        "# scores = []\n",
        "\n",
        "# for filename in os.listdir(folder):\n",
        "#     if filename.endswith(\"_score.txt\"):\n",
        "#         with open(os.path.join(folder, filename), \"r\") as f:\n",
        "#             content = f.read().strip()\n",
        "#             try:\n",
        "#                 value = float(content)\n",
        "#                 scores.append(round(value, 3))\n",
        "#             except ValueError:\n",
        "#                 continue\n",
        "\n",
        "# scores = np.array(scores, dtype=float)\n",
        "# scores[np.isnan(scores)] = np.mean(scores[~np.isnan(scores)])\n",
        "\n",
        "# def find_outliers(data):\n",
        "#     data = np.array(data)\n",
        "#     Q1 = np.percentile(data, 25)\n",
        "#     Q3 = np.percentile(data, 75)\n",
        "\n",
        "#     IQR = Q3 - Q1\n",
        "#     lower_bound = Q1 - (1.5 * IQR)\n",
        "#     upper_bound = Q3 + (1.5 * IQR)\n",
        "\n",
        "#     outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "#     return outliers, lower_bound, upper_bound, Q1, Q3, IQR\n",
        "\n",
        "# outliers, lower_bound, upper_bound, Q1, Q3, IQR = find_outliers(scores)\n",
        "\n",
        "def plot_scores_boxplot(scores):\n",
        "    plt.figure(figsize=(3.2,8))\n",
        "    plt.boxplot(\n",
        "        scores,\n",
        "        positions=[0.75], \n",
        "        vert=True,\n",
        "        patch_artist=True,\n",
        "        medianprops={'color': 'black', 'linewidth': 3},\n",
        "        boxprops={'facecolor': 'gray', 'edgecolor': 'black'},\n",
        "        flierprops={'marker': 'D', 'markerfacecolor': 'black', 'markersize': 3, 'linestyle': 'none'}\n",
        "    )\n",
        "\n",
        "    # plt.title(f'Auto K=N | GPT2\\nMean={np.mean(scores):.2f}\\n\\nLasso (alpha=0.001)', fontsize=14, weight='bold')\n",
        "    plt.title(f'Auto K=N | GPT \\n\\nRandom Token Baseline\\navg_score={np.mean(scores):.2f}', fontsize=14, weight='bold')\n",
        "    plt.ylabel('Automation Scores', fontsize=12)\n",
        "    plt.xticks([])\n",
        "    plt.ylim(0, 1.05)\n",
        "\n",
        "    #insert the text 'WIP' in center of plot\n",
        "    # plt.text(0.75, 0.5, 'WIP', fontsize=12, ha='center', va='center')\n",
        "\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "    x = np.ones_like(scores)\n",
        "    plt.scatter(\n",
        "        x,\n",
        "        scores,\n",
        "        color='gray',\n",
        "        edgecolor='black',\n",
        "        s=30,\n",
        "        alpha=0.9,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "# scores = np.load(\"data/interpolation_experiment_gpt2_lasso_a=0.001_k=N.npy\")\n",
        "scores_flat = np.array(scores).flatten()\n",
        "plot_scores_boxplot(scores_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b858c889",
      "metadata": {},
      "outputs": [],
      "source": [
        "# loop through \"C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\automation_refinement_gpt2\\master_list\"\n",
        "# if python file is empty or only has new lines, delete that file\n",
        "\n",
        "folder = r\"C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\automation_refinement_gpt2\\master_list\"\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\".py\"):\n",
        "        file_path = os.path.join(folder, filename)\n",
        "        if os.path.getsize(file_path) == 0 or all(line.strip() == \"\" for line in open(file_path)):\n",
        "            os.remove(file_path)\n",
        "            print(f\"Deleted empty file: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76451682",
      "metadata": {},
      "outputs": [],
      "source": [
        "from programs import *\n",
        "import importlib.util\n",
        "import types\n",
        "import re\n",
        "# patterns = [next_attention, previous_attention, same_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "patterns = []\n",
        "\n",
        "number_of_lines = []\n",
        "# first, count number of lines in existing patterns\n",
        "import inspect as i\n",
        "for pattern in patterns:\n",
        "    definition = i.getsource(pattern)\n",
        "    number_of_lines.append(len(definition.strip().split('\\n')))\n",
        "\n",
        "master_list_dir = \"automation_refinement_gpt2/master_list\"\n",
        "for filename in os.listdir(master_list_dir):\n",
        "    if filename.endswith(\".py\"):\n",
        "        module_name = filename[:-3]\n",
        "        if module_name not in [p.__name__ for p in patterns]:\n",
        "            try:\n",
        "                program_path = os.path.join(master_list_dir, filename)\n",
        "                spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                module = importlib.util.module_from_spec(spec)\n",
        "                module.__dict__['np'] = np\n",
        "                # module.__dict__['torch'] = torch\n",
        "                module.__dict__['spacy'] = spacy\n",
        "                module.__dict__['nlp'] = nlp\n",
        "                module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "                module.__dict__['Optional'] = Optional\n",
        "                module.__dict__['Tuple'] = Tuple\n",
        "                module.__dict__['Callable'] = Callable\n",
        "                module.__dict__['re'] = re\n",
        "                spec.loader.exec_module(module)\n",
        "            except Exception as e:\n",
        "                print(f\"Program loading failed: {str(e)}\")\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    program_to_test = attr\n",
        "                    break\n",
        "            \n",
        "            # count number of lines in program_to_test and append\n",
        "            with open(program_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                number_of_lines.append(len(lines))\n",
        "\n",
        "            patterns.append(program_to_test)\n",
        "\n",
        "print(f\"Total patterns to analyze: {len(patterns)}\")\n",
        "for pattern in patterns: print(f\"\\tPattern: {pattern.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b22619f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# patterns = every third pattern in patterns\n",
        "patterns = patterns[::4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "995c4a07",
      "metadata": {},
      "outputs": [],
      "source": [
        "patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "befd99ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "interpolation_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce24b5d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e40e6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --------------- LINEAR INTERPOLATION REFINEMENT PIPELINE ---------------\n",
        "import inspect\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# patterns = [previous_attention, same_attention, cls_attention]\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "torch_model = model\n",
        "torch_tokenizer = tokenizer\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "\n",
        "interpolation_matrix = np.zeros((num_layers, num_heads))\n",
        "patterns = [previous_attention, same_attention, punctuation_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, special_token_attention, pos_alignment, relative_position_attention]\n",
        "\n",
        "folder = r\"automation_refinement_gpt2\"\n",
        "subfolders = [\"master_list\", \"candidate\"]\n",
        "if not os.path.exists(folder): os.makedirs(folder)\n",
        "for subfolder in subfolders:\n",
        "    if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "        os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "for pattern in patterns:\n",
        "    pattern_name = pattern.__name__\n",
        "    with open(os.path.join(folder, \"master_list\", f\"{pattern_name}.py\"), \"w\") as f:\n",
        "        f.write(inspect.getsource(pattern))\n",
        "\n",
        "new_patterns = 0\n",
        "total_loops = 0\n",
        "while new_patterns < 30 and total_loops < 100:\n",
        "    try:\n",
        "        total_loops += 1\n",
        "        print(f\"Searching for new patterns, iteration {new_patterns} | total loops {total_loops}...\")\n",
        "\n",
        "        # get interpolation scores for all heads\n",
        "        print(f\"\\tCalculating interpolation scores for all heads...\")\n",
        "        for layer in range(num_layers):\n",
        "            if layer % 3 == 0: print(f\"\\t\\tLayer {layer}/{num_layers} complete\")\n",
        "            for head in range(num_heads):\n",
        "                sentence = str(np.random.choice(sentences))\n",
        "\n",
        "                X = []\n",
        "                for pattern in patterns:\n",
        "                    try:\n",
        "                        instance = pattern(sentence, tokenizer)[1].flatten()\n",
        "                        # check if nan in instance:\n",
        "                        if np.isnan(instance).any():\n",
        "                            del patterns[patterns.index(pattern)]\n",
        "                            print(f\"\\t\\tRemoved pattern {pattern.__name__} due to NaN values.\")\n",
        "                            continue\n",
        "                        # print(f\"Tested pattern: {pattern.__name__} successfully\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # print(f\"Error testing pattern {pattern.__name__}: {e}\")\n",
        "                        print(f\"\\t\\tRemoved pattern {pattern.__name__} due to error during testing.\")\n",
        "                        del patterns[patterns.index(pattern)]\n",
        "                        continue\n",
        "\n",
        "                    X.append(instance)\n",
        "                X_n = np.nan_to_num(np.array(X).T)\n",
        "                y = np.nan_to_num(model(**tokenizer(sentence, return_tensors=\"pt\"), output_attentions=True).attentions[layer][0, head].detach().numpy().flatten())\n",
        "\n",
        "                reg = LinearRegression().fit(X_n, y.flatten())\n",
        "                out = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "                len_seq = len(tokenizer([sentence], return_tensors=\"pt\").input_ids[0])\n",
        "                out = out.reshape((len_seq, len_seq))\n",
        "                pred_att = out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "                tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "                att = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "                jensonshannon_distances = []\n",
        "                for row_att, row_out in zip(att, pred_att):\n",
        "                    jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "                interpolation_score = np.mean(jensonshannon_distances)\n",
        "\n",
        "                interpolation_matrix[layer, head] = interpolation_score\n",
        "                # print(f\"Layer {layer}, Head {head} - Interpolation Score: {interpolation_score:.4f}\")\n",
        "\n",
        "        # select worst 20% of heads based on interpolation score\n",
        "        sorted_indices = np.dstack(np.unravel_index(np.argsort(interpolation_matrix.ravel()), interpolation_matrix.shape))[0]\n",
        "        num_to_select = int(0.04 * num_layers * num_heads)\n",
        "        worst_indices = sorted_indices[-num_to_select:]\n",
        "\n",
        "        # generate candidate programs for each of the worst heads\n",
        "        candidate_scores = []\n",
        "        candidate_paths = []\n",
        "        for layer, head in worst_indices[::-1]: # start with worst head\n",
        "            layer, head = int(layer), int(head)\n",
        "            print(f\"\\t TRYING | Layer {layer}, Head {head} - Previous Interpolation Score: {interpolation_matrix[layer, head]:.4f}\")\n",
        "\n",
        "            try:\n",
        "                fullprompt = generate_prompt(sentences[:5], model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "                refinement_part = \"It's already been found that this head doesn't do well on any of the following patterns: \"\n",
        "                refinement_part += \", \".join([pattern.__name__ for pattern in patterns])\n",
        "                refinement_part += \". So don't suggest any of those patterns again.\"\n",
        "                fullprompt += \" \".join(refinement_part)\n",
        "\n",
        "                conversation_history = [\n",
        "                    {\"role\": \"system\",\n",
        "                    \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "                ]\n",
        "                conversation_history.append({\"role\": \"user\", \"content\": fullprompt})\n",
        "                response_1 = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\", messages=conversation_history\n",
        "                )\n",
        "                assistant_response_1 = response_1.choices[0].message.content\n",
        "                conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "                # print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "                try:\n",
        "                    parsed = parse_model_output(assistant_response_1)\n",
        "                except:\n",
        "                    print(f\"\\t\\tError parsing response, skipping head ({layer}, {head})\")\n",
        "                    continue\n",
        "                feedback = \"invalid_output\"\n",
        "\n",
        "                # print(f\"\\t\\t{parsed}\")\n",
        "                print(f\"\\t\\tHypothesis: {parsed['hypothesis']}. Program successfully parsed.\")\n",
        "\n",
        "                candidate_path = f\"{folder}/candidate/layer{layer}_head{head}_code.py\"\n",
        "                with open(candidate_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "\n",
        "                feedback = validate_program(candidate_path, model, tokenizer, layer, head, sentences)\n",
        "                if isinstance(feedback, np.float64) and not np.isnan(feedback):\n",
        "                    candidate_scores.append(feedback)\n",
        "                    candidate_paths.append(candidate_path)\n",
        "                    print(f\"\\t\\tProgram valid with score {feedback:.4f}.\")\n",
        "                    continue\n",
        "                    \n",
        "                else:\n",
        "                    candidate_scores.append(100)\n",
        "                    candidate_paths.append(\"\")\n",
        "                    # print(f\"\\t\\tInitial program invalid w/ error: ({feedback}), skipping.\")\n",
        "                    continue\n",
        "            \n",
        "            except Exception as e:\n",
        "                total_loops += 1\n",
        "                candidate_scores.append(100)\n",
        "                candidate_paths.append(\"\")\n",
        "                # print(f\"\\t\\tError processing Layer {layer}, Head {head}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # select and save best candidate program to master list\n",
        "        best_candidate = candidate_paths[np.argmin(candidate_scores)]\n",
        "        best_score = np.min(candidate_scores)\n",
        "        new_program = make_program_executable(open(best_candidate).read())\n",
        "        \n",
        "        print(f\"Best candidate program {new_program.__name__} added | solo score {best_score:.4f}.\")\n",
        "\n",
        "        if best_score < 0.45:\n",
        "            spec = importlib.util.spec_from_file_location(\"loaded_program\", best_candidate)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    new_program = attr\n",
        "\n",
        "            new_pattern_name = new_program.__name__\n",
        "            with open(os.path.join(folder, \"master_list\", f\"{new_pattern_name}.py\"), \"w\") as f:\n",
        "                f.write(inspect.getsource(new_program))\n",
        "\n",
        "            patterns.append(new_program)\n",
        "            new_patterns += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        total_loops += 1\n",
        "        print(f\"Error in main loop: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "322a632c",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences\n",
        "sentences_new = []\n",
        "# get rid of all the sentences with too many special characters, mathbf stuff, latex, delete all backslashes in each sentence\n",
        "for sentence in sentences:\n",
        "    if re.search(r'[{}\\\\]', sentence):\n",
        "        continue\n",
        "    if re.search(r'\\$.*\\$', sentence):\n",
        "        continue\n",
        "    sentences_new.append(sentence)\n",
        "# sentences = sentences_new\n",
        "print(f\"Filtered sentences count: {len(sentences)}\")\n",
        "print(sentences_new[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0230175",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(sentences_new):\n",
        "    print(i, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f428d4a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "del sentences_new[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc0d60a",
      "metadata": {},
      "outputs": [],
      "source": [
        "deleting = [21, 22, s]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aa4d581",
      "metadata": {},
      "outputs": [],
      "source": [
        "del sentences[21]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7b32b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def0137e",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_candidate = r\"automation_refinement_gpt2_try2/candidate/layer2_head2_code.py\"\n",
        "make_program_executable(open(best_candidate).read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659ac533",
      "metadata": {},
      "outputs": [],
      "source": [
        "for pattern in patterns:\n",
        "    try:\n",
        "        instance = pattern(sentence, tokenizer)[1].flatten()\n",
        "        # check if nan in instance:\n",
        "        if np.isnan(instance).any():\n",
        "            del patterns[patterns.index(pattern)]\n",
        "            print(f\"\\t\\tRemoved pattern {pattern.__name__} due to NaN values.\")\n",
        "            continue\n",
        "        print(f\"Tested pattern: {pattern.__name__} successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error testing pattern {pattern.__name__}: {e}\")\n",
        "        del patterns[patterns.index(pattern)]\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488ac196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get all programs from automation_refinement/master_list and load these python functions as patterns = [executable functions]\n",
        "\n",
        "import os\n",
        "import importlib.util\n",
        "import types\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import regex as re\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
        "from typing import Optional, Tuple, Callable\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "folder = \"automation_refinement_gpt2/master_list\"\n",
        "patterns = []\n",
        "\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\".py\"):\n",
        "        code_path = os.path.join(folder, filename)\n",
        "        spec = importlib.util.spec_from_file_location(f\"module_{filename[:-3]}\", code_path)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        # get pretrainedtokenizerbase, from typing import Optional, Tuple, Callable\n",
        "        module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
        "        module.__dict__['Optional'] = Optional\n",
        "        module.__dict__['Tuple'] = Tuple\n",
        "        module.__dict__['Callable'] = Callable\n",
        "        module.__dict__['spacy'] = spacy\n",
        "        \n",
        "        try:\n",
        "            spec.loader.exec_module(module)\n",
        "            for attr_name in dir(module):\n",
        "                attr = getattr(module, attr_name)\n",
        "                if isinstance(attr, types.FunctionType):\n",
        "                    patterns.append(attr)\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading program from {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "print(f\"Loaded {len(patterns)} patterns.\")\n",
        "for i, prog in enumerate(patterns):\n",
        "    print(f\"{i}: {prog.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890adf84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# go through patterns and check if they need nlp or defaultdict, if they don't need nlp library from spacy, add to patterns_strong\n",
        "patterns_strong = []\n",
        "for pattern in patterns:\n",
        "    if not any(kw in pattern.__code__.co_names for kw in ['nlp', 'spacy', 'defaultdict']):\n",
        "        patterns_strong.append(pattern)\n",
        "print(f\"Filtered to {len(patterns_strong)} strong patterns.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6879e81f",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sentences = sentences[135:200]\n",
        "# gpt2\n",
        "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "scores = np.zeros((model.config.num_hidden_layers, model.config.num_attention_heads))\n",
        "top_k = 3\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    for head in range(num_heads):\n",
        "        print(f\"Analyzing Layer {layer}, Head {head}...\")\n",
        "        sentence_scores = []\n",
        "        for sentence in test_sentences:\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            attention = outputs.attentions[layer][0, head].detach().numpy()\n",
        "            y = attention.flatten()\n",
        "\n",
        "            X = []\n",
        "            for pattern in patterns_strong:\n",
        "                X.append(pattern(sentence, tokenizer)[1].flatten())\n",
        "            X_n = np.array(X).T\n",
        "            y = y.flatten()\n",
        "\n",
        "            # avoid ValueError: Input X contains NaN.\n",
        "            X_n = np.nan_to_num(X_n)\n",
        "            y = np.nan_to_num(y)\n",
        "\n",
        "            reg = LinearRegression().fit(X_n, y)\n",
        "            side_length = int(np.sqrt(len(y)))\n",
        "            y = y.reshape((side_length, side_length))\n",
        "\n",
        "            top_indices = np.argsort(np.abs(reg.coef_))[-top_k:]\n",
        "            pred_att = reg.intercept_ + sum(reg.coef_[i] * X[i] for i in top_indices)\n",
        "            pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "            if top_k == 1:\n",
        "                #pred_att is just the single pattern with highest coef, it isn't equal to a sum at all\n",
        "                fn_highest_coeff = patterns_strong[np.argmax(np.abs(reg.coef_))]\n",
        "                pred_att = fn_highest_coeff(sentence, tokenizer)[1]\n",
        "                print(f\"\\tUsing pattern: {fn_highest_coeff.__name__}\")\n",
        "\n",
        "            jensonshannon_distances = []\n",
        "            for row_att, row_out in zip(y, pred_att):\n",
        "                jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "            score = np.mean(jensonshannon_distances)\n",
        "            sentence_scores.append(score)\n",
        "        \n",
        "        scores[layer, head] = np.mean(sentence_scores)\n",
        "        print(f\"\\tScore for Layer {layer}, Head {head}: {scores[layer, head]}\")\n",
        "\n",
        "np.save(\"gpt2_k1_automation_scores.npy\", scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5396e98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONDUCT MODEL LEVEL ANALYSIS / GET SUMMARY SCORE FOR WHOLE MODEL\n",
        "\n",
        "def classify_model(method, sentences, torch_model, torch_tokenizer):\n",
        "    if method == \"linear_fit\":\n",
        "        patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment]\n",
        "    elif method == \"best_fit\":\n",
        "        saved_file = pd.read_csv('data/best_fit_t5.csv')\n",
        "    elif method == \"automation\":\n",
        "        from automation_helper import generate_prompt, parse_llm_idea, validate_program\n",
        "\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    all_scores = []\n",
        "    final_scores = []\n",
        "    \n",
        "    if method != \"automation\":\n",
        "        for idx, sentence in enumerate(sentences):\n",
        "            scores = np.zeros((num_layers, num_heads))\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    layer, head = i, j\n",
        "                    inputs = torch_tokenizer(sentence, return_tensors=\"pt\")\n",
        "                    len_seq = len(torch_tokenizer([sentence], return_tensors=\"pt\").input_ids[0])\n",
        "\n",
        "                    X = []\n",
        "                    # y =  torch_model(**inputs, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "                    decoder_input_ids = tokens[\"input_ids\"]\n",
        "                    outputs = torch_model(input_ids=inputs[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "                    y = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "                    if method == \"random_baseline\":\n",
        "                        pred_att = np.zeros((len_seq, len_seq))\n",
        "                        pred_att[:, -1] = 1.0\n",
        "\n",
        "                    elif method == \"best_fit\":\n",
        "                        matching_rows = saved_file[(saved_file['i'] == i) & (saved_file['j'] == j)]\n",
        "                        if not matching_rows.empty:\n",
        "                            best_pattern = matching_rows.loc[matching_rows['Score'].idxmax(), 'Pattern']\n",
        "                            func = globals()[best_pattern]\n",
        "                            _, pred_att = func(sentence, tokenizer)\n",
        "                        else:\n",
        "                            out = np.random.rand(len_seq, len_seq)\n",
        "                            pred_att =  out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "                    elif method == \"linear_fit\":\n",
        "                        for pattern in patterns:\n",
        "                            X.append(pattern(sentence, torch_tokenizer)[1].flatten())\n",
        "                        X_n = np.array(X).T\n",
        "                        y = y.flatten()\n",
        "\n",
        "                        reg = LinearRegression().fit(X_n, y)\n",
        "                        side_length = int(np.sqrt(len(y)))\n",
        "                        y = y.reshape((side_length, side_length))\n",
        "\n",
        "                        pred_att = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "                        pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "                    jensonshannon_distances = []\n",
        "                    for row_att, row_out in zip(y, pred_att):\n",
        "                        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "                    score = np.mean(jensonshannon_distances)\n",
        "                    scores[layer, head] = score\n",
        "                    \n",
        "            all_scores.append(scores)\n",
        "            final_scores.append(np.sum(scores))\n",
        "            print(f\"Processed sentence #{idx}/{len(sentences)}: Score: {np.sum(scores):.2f}\\n\\t->'{sentence}'\")\n",
        "        \n",
        "    elif method == \"automation\":\n",
        "        \n",
        "        prompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "        hypothesis, program = parse_llm_idea(prompt, config=config, verbalize=False)\n",
        "        python_path = f\"{program_path}/{head}_output.py\"\n",
        "        feedback = validate_program(python_path, model, tokenizer, layer, head, sentences)\n",
        "        scores[layer, head] = feedback\n",
        "            \n",
        "\n",
        "    print(f\"Final Score: {sum(final_scores) / len(final_scores)}\")\n",
        "    return all_scores, final_scores\n",
        "\n",
        "methods = [\"random_baseline\", \"best_fit\", \"linear_fit\"]\n",
        "final_scores = {\"random_baseline\": 0, \"best_fit\": 0, \"linear_fit\": 0}\n",
        "\n",
        "for method in methods:\n",
        "    print(f\"\\nAnalyzing method: {method}\")\n",
        "    _, scores = classify_model(method, sentences[:10], model, tokenizer)\n",
        "    final_scores[method] = np.mean(scores)\n",
        "\n",
        "print(\"\\n\",final_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0989da5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PLOT DIFFERENT SUMMARY SCORES FOR THE MODEL\n",
        "\n",
        "max_score = model.config.num_hidden_layers * model.config.num_attention_heads\n",
        "raw_scores = [111, 92, 62, 65, 56]\n",
        "labels = ['Random \\nToken Baseline', 'Automatic\\nPrograms', 'K=1Refined\\nPrograms', 'Best Fit\\nPrograms', 'Linear Weight\\nPrograms']\n",
        "colors = ['darkred', 'darkblue', '#6aa84f', '#800080']\n",
        "\n",
        "# Normalize scores: lower scores become higher bars\n",
        "scores = [(score / max_score) for score in raw_scores]\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "bars = plt.bar(labels, scores, color=colors, width=0.6)\n",
        "\n",
        "# Add text labels on top of bars\n",
        "for bar, raw, norm in zip(bars, raw_scores, scores):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
        "             f'{norm:.2f}\\n[ {int(raw)} / {max_score} ]', ha='center', va='bottom', fontsize=14)\n",
        "ax.set_facecolor('#F5F5F5')\n",
        "\n",
        "plt.ylim(0, 1.0)\n",
        "# plt.title('Normalized Error (1 - Score / Max Score)')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.text(plt.xlim()[0]-0.7, plt.ylim()[1]+0.05, '[bad hypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
        "plt.text(plt.xlim()[0]-0.7, plt.ylim()[0]-0.13, '[well-fitting\\nhypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
        "plt.ylabel('Normalized Model Scores', fontsize=16, labelpad=20)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fdf02a",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Program Automation Efforts\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5510c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# AUTOMATICALLY GENERATE PATTERNS FOR A HEAD & GENERATE LLM PROMPT\n",
        "\n",
        "example_program_one = \"\"\"\n",
        "def dependencies(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
        "    len_seq = len(toks.input_ids[0]) /\n",
        "    out = np.zeros((len_seq, len_seq)) /\n",
        "    words = sentence.split() /\n",
        "    doc = nlp(\" \".join(words)) /\n",
        "    for stok in doc: /\n",
        "        parent_index = stok.i /\n",
        "        for child_stok in stok.children: /\n",
        "            child_index = child_stok.i /\n",
        "            out[parent_index+1, child_index+1] = 1 /\n",
        "            out[child_index+1, parent_index+1] = 1 /\n",
        "    out[0, 0] = 1 /\n",
        "    out[-1, 0] = 1 /\n",
        "    out += 1e-4 /\n",
        "    out = out / out.sum(axis=1, keepdims=True) /\n",
        "    return \"Dependency Parsing Pattern\", out /\n",
        "\"\"\"\n",
        "example_program_two = \"\"\"\n",
        "def same_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Same Token Pattern\", out\n",
        "\"\"\"\n",
        "example_program_three = \"\"\"\n",
        "def pos_alignment(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
        "    len_seq = len(toks.input_ids[0]) /\n",
        "    out = np.zeros((len_seq, len_seq)) /\n",
        "    # assign toks, input_ids, word_ids, len_seq, out, doc /\n",
        "    # use spacey to get pos_tags for tokens in docs [token.pos_ for token in doc] /\n",
        "    # for token in pos_tags: /\n",
        "    # loop through pos_tags and increment out[i,j] when pos_tags match /\n",
        "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention /\n",
        "    # Normalize out matrix by row (results in uniform attention) and return out /\n",
        "    # return 'Part of Speech Implementation 1', out /\n",
        "\"\"\"\n",
        "\n",
        "def generate_prompt(sentences, model, tokenizer, head_loc, top_k_ratio=0.1):\n",
        "    layer, head = head_loc\n",
        "    data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model.config.architectures[0],\n",
        "        \"examples\": []\n",
        "    }\n",
        "\n",
        "    def handle_score(score):\n",
        "        # convert to percentage with 0 decimal places\n",
        "        return \"{:.0f}\".format(score * 100)\n",
        "        \n",
        "    def scrape_head(att, tokens, top_k_ratio, ignore_special=True):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append(f\"[{str(tokens[i])}|{str(tokens[j])}:{handle_score(score)}]\")\n",
        "        top_activations_str = \" \".join(top_activations).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "        return top_activations_str\n",
        "    \n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=top_k_ratio)\n",
        "        item = {f\"sentence {idx}\": \" \".join(tokens), \"sentence attention\": top_activations}\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "    data = json.dumps(data, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    Using the following pieces of data based on {len(sentences)} sentences, generate three hypothesises about the linguistic role the following head is responsible for based on patterns\n",
        "    in the {model.config.architectures[0]} activations.  Then, choose the most fitting hypothesis for the head function using examples from the data. Finally, using the linguistic hypothesis you determine, \n",
        "    write a python function which takes in a sentence and tokenizer as parameters and outputs the name of the pattern you hypothesize along with a predicted_matrix (size: token_len * token_len), which is the \n",
        "    rule encoded matrix mirroring attention patterns you'd predict for any given sentence for Layer {layer}, Head {head}. Feel free to encode complex functions but write the simplest algorithm that captures your \n",
        "    observed pattern. You must respond to this prompt in JSON in the form \"{{\"hypothesis\": \"...\", \"program\": \"...\"}} with your chosen hypothesis. Think carefully before generating any code.\n",
        "    The first portion of your response has key \"hypothesis\" with the title of the hypothesis and the second portion of your response with key \"program\" should have valid python code starting with ```python and including imports. These patterns can be simple or \n",
        "    complex.  For uniformity, the first three lines of your function should be 'toks = tokenizer([sentence], return_tensors=\"pt\") len_seq = len(toks.input_ids[0]) out = np.zeros((len_seq, len_seq))'.\n",
        "    Make sure the token sequences from your tokenizer and spaCy (if you must use spaCy) are aligned via a dictionary if necessary, because they split text differently. Make sure you generalize your hypothesis pattern to any sentence. Functions can almost \n",
        "    always be expressed in fewer than 50 lines of code. As examples, it has been discovered one head is responsible for the complex task of dependency parsing. It's simplistic predicted pseudocode looks like: \n",
        "    {example_program_one}. Example 2: '''{example_program_two}''' Example 3: '''{example_program_three}'''. DATA: {data}\"\"\"\n",
        "    return ' '.join(prompt.strip().split())\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "layer, head = 5, 7\n",
        "prompt = generate_prompt(sentences[:25], model, tokenizer, (layer, head), 0.025)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5704e094",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using Gemini, GPT-4o, Claude, Deepseek\n",
        "# API needs long contexts and free access\n",
        "# Source to get API keys is \"usage\" key\n",
        "\n",
        "load_dotenv()\n",
        "API_CONFIGS = {\n",
        "    \"gemini\": {\n",
        "        \"model\": \"gemini\",\n",
        "        \"url\": \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\",\n",
        "        \"key\": os.getenv(\"GEMINI\"),\n",
        "        \"headers_fn\": lambda key: {\"Content-Type\": \"application/json\", \"X-goog-api-key\": key},\n",
        "        \"payload_fn\": lambda prompt: {\n",
        "            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "            \"generationConfig\": {\"response_mime_type\": \"application/json\"}\n",
        "        },\n",
        "        \"usage\": \"https://aistudio.google.com/apikey\"\n",
        "    },\n",
        "    \"openai\": {\n",
        "        \"model\": \"openai\",\n",
        "        \"url\": \"https://api.openai.com/v1/responses\",\n",
        "        \"key\": os.getenv(\"OPENAI\"),\n",
        "        \"headers_fn\": lambda key: {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\": \"gpt-4.1\", \"input\": prompt},\n",
        "        \"usage\": \"https://platform.openai.com/account/api-keys\"\n",
        "    },\n",
        "    \"claude\": {\n",
        "        \"model\": \"claude\",\n",
        "        \"url\": \"https://api.anthropic.com/v1/messages\",\n",
        "        \"key\": os.getenv(\"CLAUDE\"),\n",
        "        \"headers_fn\": lambda key: {\"x-api-key\": key, \"Content-Type\": \"application/json\", \"Anthropic-Version\":\"2023-06-01\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\":\"claude-sonnet-4-20250514\", \"messages\":[{\"role\":\"user\",\"content\":prompt}]},\n",
        "        \"usage\": \"https://platform.claude.com/api_keys\"\n",
        "    },\n",
        "    \"deepseek\": {\n",
        "        \"model\": \"deepseek\",\n",
        "        \"url\": \"https://api.deepseek.com/chat/completions\",\n",
        "        \"key\": os.getenv(\"DEEPSEEK\"),\n",
        "        \"headers_fn\": lambda key: {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\": \"deepseek-chat\", \"input\": prompt, \"max_tokens\": 1000},\n",
        "        \"usage\": \"https://platform.deepseek.com/api_keys\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2112fba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAVE AUTOMATED HYPOTHESIS + VALIDATE GENERATED PROGRAM SYNTHESIS CODE\n",
        "\n",
        "def parse_llm_idea(prompt, config=\"YOUR_API_CONFIG\", verbalize=True):\n",
        "    def make_request():\n",
        "        headers = config[\"headers_fn\"](config[\"key\"])\n",
        "        payload = config[\"payload_fn\"](prompt)\n",
        "        response = requests.post(config[\"url\"], headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if config[\"model\"] == \"gemini\":\n",
        "            data = response.json()\n",
        "            output = data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        if config[\"model\"] == \"openai\":\n",
        "            pass\n",
        "        if config[\"model\"] == \"claude\":\n",
        "            data = response.json()\n",
        "            output = data[\"content\"][\"text\"]\n",
        "        if config[\"model\"] == \"deepseek\":\n",
        "            pass\n",
        "\n",
        "        return output\n",
        "    \n",
        "    output = make_request()\n",
        "\n",
        "    try:\n",
        "        result = json.loads(output)\n",
        "\n",
        "        if type(result) is list: result = result[0]\n",
        "        hypothesis = result.get(\"hypothesis\", \"\")\n",
        "        program = result.get(\"program\", \"\")\n",
        "\n",
        "        if program.startswith(\"```python\"): program = program[9:]\n",
        "        if program.endswith(\"```\"): program = program[:-3]\n",
        "        program = program.strip()\n",
        "\n",
        "        if verbalize: print(\"Hypothesis, Explanation & Program successfully parsed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Parsing API failed: {str(e)}\")\n",
        "        return str(e)\n",
        "\n",
        "    return hypothesis, program\n",
        "\n",
        "config = API_CONFIGS[\"gemini\"] \n",
        "parse_llm_idea(prompt, config=config, verbalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be7aad1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SIMPLE AUTOMATION OF PIPELINE FOR ANALYZING ALL HEADS & WRITING/SAVING PROGRAMS\n",
        "\n",
        "def automation_pipeline(model, tokenizer, sentences, API_KEY, save_data=True, evaluate=False):\n",
        "    heads = model.config.num_attention_heads\n",
        "    layers = model.config.num_hidden_layers\n",
        "    prompts, programs = [], []\n",
        "\n",
        "    for layer in range(layers):\n",
        "        # if layer == 0: continue\n",
        "        if save_data:\n",
        "            # save prompts:\n",
        "            prompt_path = f\"automation_2/prompts/{layer}/\"\n",
        "            os.makedirs(prompt_path, exist_ok=True)\n",
        "\n",
        "            # save programs:\n",
        "            program_path = f\"automation_2/llm_code/code_layer_{layer}/\"\n",
        "            os.makedirs(program_path, exist_ok=True)\n",
        "\n",
        "            # save scores:\n",
        "            if evaluate:\n",
        "                score_path = f\"automation_2/scores/{layer}/\"\n",
        "                os.makedirs(score_path, exist_ok=True)\n",
        "\n",
        "        for head in range(heads):\n",
        "            # if head < 9: continue\n",
        "            if (layer, head) not in failed_programs:\n",
        "                continue\n",
        "            prompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.1)\n",
        "            hypothesis, explanation, program = parse_llm_idea(prompt, API_KEY, output=False)\n",
        "            print(f\"Analyzed Layer {layer}, Head {head} | Hypothesis ~ {hypothesis} \")\n",
        "\n",
        "            prompts.append(prompt)\n",
        "            programs.append(program)\n",
        "\n",
        "            if save_data:\n",
        "                with open(f\"{prompt_path}/{layer}_{head}_prompt.txt\", \"w\") as f: f.write(prompt)\n",
        "                with open(f\"{program_path}/{head}_output.py\", \"w\") as f: f.write(program)\n",
        "\n",
        "        if evaluate: \n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "automation_pipeline(model, tokenizer, generic_sentences[:10], API_KEY=API_KEY, save_data=True, evaluate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e9c1ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CALCULATE AND SAVE SCORES FOR AUTOMATICALLY GENERATED PROGRAMS\n",
        "\n",
        "import importlib.util\n",
        "import types\n",
        "\n",
        "scores = []\n",
        "failed_programs = []\n",
        "for layer in range(12):\n",
        "    # if layer != 11: continue\n",
        "    code_path = f\"automation_2/llm_code/code_layer_{layer}/\"\n",
        "    for j in range(12):\n",
        "        # if j != 11: continue\n",
        "        filename = f\"{j}_output.py\"\n",
        "        program_path = os.path.join(code_path, filename)\n",
        "        if not os.path.exists(program_path): continue\n",
        "        score_path = f\"automation_2/scores/{layer}_{j}_score.txt\"\n",
        "        os.makedirs(os.path.dirname(score_path), exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_j{j}\", program_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "        except Exception as e:\n",
        "            print(f\"Layer {layer}, Head {j} - Error: {e}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"Error loading module: {e}\")\n",
        "            failed_programs.append((layer, j))\n",
        "            scores.append(-1)\n",
        "            continue\n",
        "\n",
        "        for attr_name in dir(module):\n",
        "            attr = getattr(module, attr_name)\n",
        "            if isinstance(attr, types.FunctionType):\n",
        "                program_to_test = attr\n",
        "                break\n",
        "\n",
        "        try:\n",
        "            score = score_prediction(model, tokenizer, (layer, j), program_to_test, generic_sentences[0], distance=\"jsd\", output=False)\n",
        "            print(f\"Layer {layer}, Head {j} - Score: {score:.2f}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"{score:.2f}\")\n",
        "            scores.append(score)\n",
        "        except Exception as e:\n",
        "            print(f\"Layer {layer}, Head {j} - Error: {e}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"Error during scoring: {e}\")\n",
        "            failed_programs.append((layer, j))\n",
        "            scores.append(-1)\n",
        "            continue\n",
        "\n",
        "num_scored = len([s for s in scores if s != -1 and not np.isnan(s)])\n",
        "print(f\"Number of Successfully Scored Heads: {num_scored} out of {len(scores)}\")\n",
        "\n",
        "avg_score = sum([s for s in scores if s != -1 and not np.isnan(s)]) / len([s for s in scores if s != -1 and not np.isnan(s)])\n",
        "print(f\"Average Score (excluding errors): {avg_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48c76a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# make scores into a matrix (12,12) with -1 for failed programs\n",
        "\n",
        "sq_score = np.full((12, 12), -1.0)\n",
        "for idx, score in enumerate(scores):\n",
        "    layer = idx // 12\n",
        "    head = idx % 12\n",
        "    sq_score[layer, head] = score\n",
        "\n",
        "sq_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a61ba7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# loop through bert and bert2 scores, take best score and build sq_Score matrix\n",
        "\n",
        "scores_1 = np.array([])\n",
        "scores_2 = np.array([])\n",
        "scores_min = np.array([])\n",
        "\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        score1_path = os.path.join(\"automation_results_bert\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        score2_path = os.path.join(\"automation_results_bert_2\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        if os.path.exists(score1_path):\n",
        "            with open(score1_path, \"r\") as f:\n",
        "                try:\n",
        "                    score1 = round(float(f.read().strip()), 2)\n",
        "                    scores_1 = np.append(scores_1, score1)\n",
        "                except:\n",
        "                    score1 = 1\n",
        "        # if os.path.exists(score2_path):\n",
        "        try:\n",
        "            with open(score2_path, \"r\") as f:\n",
        "                try:\n",
        "                    score2 = round(float(f.read().strip()), 2)\n",
        "                    scores_2 = np.append(scores_2, score2)\n",
        "                except:\n",
        "                    score2 = 1\n",
        "                    scores_2 = np.append(scores_2, score2)\n",
        "        except:\n",
        "            scores_2 = np.append(scores_2, 1)\n",
        "\n",
        "        best_score = min(score1, score2)\n",
        "        scores_min = np.append(scores_min, best_score)\n",
        "\n",
        "sq_scores = scores_min.reshape((12, 12))\n",
        "np.save(\"bert_scores_1.npy\", scores_1.reshape((12, 12)))\n",
        "np.save(\"bert_scores_2.npy\", scores_2.reshape((12, 12)))\n",
        "np.save(\"bert_headscores.npy\", scores_min.reshape((12, 12)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9020344",
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate through bert heads, check if corresponding score in automation_results_bert/scores and automation_results_bert_2/scores. save the higher score\n",
        "scores = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        score1_path = os.path.join(\"automation_results_bert\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        score2_path = os.path.join(\"automation_results_bert_3\", \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        score1, score2 = None, None\n",
        "        if os.path.exists(score1_path):\n",
        "            with open(score1_path, 'r') as f:\n",
        "                score1 = float(f.read().strip())\n",
        "            \n",
        "        if os.path.exists(score2_path):\n",
        "            with open(score2_path, 'r') as f:\n",
        "                try:\n",
        "                    score2 = float(f.read().strip())\n",
        "                except ValueError:\n",
        "                    print(f\"Invalid score in {score2_path}\")\n",
        "        \n",
        "        if score1 is not None and score2 is not None:\n",
        "            # print(\"score1:\", score1, \"score2:\", score2, \"min:\", min(score1, score2))\n",
        "            scores.append(min(score1, score2))\n",
        "        elif score1 is not None:\n",
        "            scores.append(score1)\n",
        "        elif score2 is not None:\n",
        "            scores.append(score2)\n",
        "        else:\n",
        "            scores.append(None)\n",
        "\n",
        "scores = np.array(scores).reshape(12, 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5478a1e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ordinary score plot\n",
        "\n",
        "colors = \"Grays\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "im2 = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im2.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im2, ax=ax)\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
        "ax.set_yticklabels([i for i in range(12)])\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "# put a space element in between automation and scores in text\n",
        "title = (\n",
        "    r'$\\mathbf{Automation \\ Scores}$'  # \\mathbf makes the text bold\n",
        "    '\\n\\nMethod: No Refinement'\n",
        "    f'\\nScores | {model.config.architectures[0]}'  # Example: replaced model.config...\n",
        ")\n",
        "plt.title(f\"{title}\\n\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa549158",
      "metadata": {},
      "outputs": [],
      "source": [
        "scores = np.load(\"data/k62_bert_automation_scores.npy\")\n",
        "sq_score = np.reshape(scores, (12, 12))\n",
        "\n",
        "# Mask invalid entries\n",
        "masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
        "\n",
        "# Setup base heatmap\n",
        "colors = \"Greys\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "\n",
        "# --- Base image ---\n",
        "im = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im, ax=ax)\n",
        "\n",
        "# --- Overlay red cells (< 0.4) ---\n",
        "threshold_value = 0.2\n",
        "highlight_mask = sq_score < threshold_value\n",
        "red_overlay = np.zeros((*sq_score.shape, 4))\n",
        "red_overlay[highlight_mask] = [0.9, 0, 0.2, 1]  # nice blue RGBA color: \n",
        "\n",
        "# Plot on top (no transparency, exact green)\n",
        "ax.imshow(red_overlay, aspect='auto')\n",
        "\n",
        "# Axes\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels(range(12), rotation=90)\n",
        "ax.set_yticklabels(range(12))\n",
        "\n",
        "# Title\n",
        "title = (\n",
        "    r'$\\mathbf{BERT \\ Automation \\ Scores}$'\n",
        "    f'\\nHighlights for Great Scores | Colorful if score < {threshold_value}'\n",
        "    '\\n\\nMethod: Auto (K=N)'\n",
        "    f'\\n{np.sum(highlight_mask)} highlighted scores ({100 * np.sum(highlight_mask) / highlight_mask.size:.1f}%) | Avg_Score: {np.mean(sq_score):.3f}'\n",
        ")\n",
        "plt.title(f\"{title}\\n\")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257999d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = \"Grays_r\"\n",
        "plt.rcParams['text.usetex'] = False\n",
        "\n",
        "score_threshold = 0.4\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "highlighted_sq = np.where(sq_score < score_threshold, sq_score, np.nan)\n",
        "# make all non-highlighted values white (1.0)\n",
        "highlighted_sq = np.where(np.isnan(highlighted_sq), 1.0, highlighted_sq)\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "im2 = ax.imshow(highlighted_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im2.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im2, ax=ax)\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
        "ax.set_yticklabels([i for i in range(12)])\n",
        "import matplotlib\n",
        "print(\"usetex:\", matplotlib.rcParams['text.usetex'])\n",
        "plt.rcParams['text.usetex'] = False\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "# make automation bold\n",
        "# get number of highlighted scores\n",
        "num_highlighted = np.sum(sq_score < score_threshold)\n",
        "title = (\n",
        "    r'$\\mathbf{Highlighted\\ Scores}$'\n",
        "    '\\n\\nMethod: No Refinement'\n",
        "    f'\\n {num_highlighted} scores < {score_threshold} ({num_highlighted/(len(sq_score)**2)*100:.0f}%) | {model.config.architectures[0]}\\n')\n",
        "# title = \"Automation Scores\\n\"\n",
        "plt.title(title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4273dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# go through every head, get score for each pattern in patterns, save the lowest ones in scores\n",
        "\n",
        "scores = []\n",
        "# bert\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "        head_scores = []\n",
        "        for pattern in patterns:\n",
        "            try:\n",
        "                sentence_collector = []\n",
        "                for sentence in sentences[:5]:\n",
        "                    score = score_prediction(model, tokenizer, (layer, head), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                    sentence_collector.append((sentence, score))\n",
        "                head_score = np.mean([s[1] for s in sentence_collector])\n",
        "                head_scores.append((pattern.__name__, head_score))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error scoring Layer {layer}, Head {head} with pattern {pattern.__name__}: {e}\")\n",
        "                head_scores.append((pattern.__name__, float('inf')))\n",
        "        \n",
        "        # get pattern with lowest score\n",
        "        best_pattern, best_score = min(head_scores, key=lambda x: x[1])\n",
        "        scores.append((layer, head, best_pattern, best_score))\n",
        "        print(f\"Layer {layer}, Head {head} - Best Pattern: {best_pattern} | Score: {best_score:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "4780a4d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "patterns = [previous_attention, same_attention, cls_attention]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae3381ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sentences = sentences[135:150]\n",
        "# scores = np.zeros((model.config.num_hidden_layers, model.config.num_attention_heads))\n",
        "top_k = -1\n",
        "\n",
        "# model is bert\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# torch_model = model\n",
        "# torch_tokenizer = tokenizer \n",
        "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
        "\n",
        "# model is gpt-2\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "torch_model = model\n",
        "torch_tokenizer = tokenizer\n",
        "\n",
        "scores = np.zeros((model.config.num_hidden_layers, model.config.num_attention_heads))\n",
        "num_layers = torch_model.config.num_hidden_layers\n",
        "num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "for layer in range(num_layers):\n",
        "    # if layer < 8: continue\n",
        "    for head in range(num_heads):\n",
        "        # if layer < 8 and head < 4: continue\n",
        "        print(f\"Analyzing Layer {layer}, Head {head}...\")\n",
        "        sentence_scores = []\n",
        "        for sentence in test_sentences:\n",
        "            # inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "            # outputs = model(**inputs, output_attentions=True)\n",
        "            # attention = outputs.attentions[layer][0, head].detach().numpy()\n",
        "            # y = attention.flatten()\n",
        "            # num_tokens = len(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
        "\n",
        "            toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "            len_seq = len(toks.input_ids[0])\n",
        "            # assign full attention to one random index in each row\n",
        "            attention = np.zeros((len_seq, len_seq))\n",
        "            for i in range(len_seq):\n",
        "                attention[i, np.random.randint(0, len_seq)] = 1\n",
        "\n",
        "            # attention = np.random.rand(len_seq, len_seq)\n",
        "            attention = attention / attention.sum(axis=1, keepdims=True)\n",
        "            y = attention.flatten()\n",
        "\n",
        "            X = []\n",
        "            for pattern in patterns:\n",
        "                try:\n",
        "                    instance = pattern(sentence, tokenizer)[1].flatten()\n",
        "                    # asssert that instance is a 2d array\n",
        "                    if instance.ndim != 1:\n",
        "                        print(f\"\\t\\tRemoved pattern {pattern.__name__} due to incorrect output dimensions.\")\n",
        "                        del patterns[patterns.index(pattern)]\n",
        "                        continue\n",
        "\n",
        "\n",
        "                    # check if nan in instance:\n",
        "                    if np.isnan(instance).any():\n",
        "                        del patterns[patterns.index(pattern)]\n",
        "                        print(f\"\\t\\tRemoved pattern {pattern.__name__} due to NaN values.\")\n",
        "                        continue\n",
        "                    X.append(instance)\n",
        "                    # print(f\"Tested pattern: {pattern.__name__} successfully\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    # print(f\"Error testing pattern {pattern.__name__}: {e}\")\n",
        "                    print(f\"\\t\\tRemoved pattern {pattern.__name__} due to error during testing.\")\n",
        "                    del patterns[patterns.index(pattern)]\n",
        "                    continue\n",
        "\n",
        "            X_n = np.array(X).T\n",
        "            y = np.nan_to_num(y.flatten())\n",
        "\n",
        "            # methods = [\"lasso\", \"l1\", \"ridge\", \"linear\"]\n",
        "            # method = \"lasso\"\n",
        "\n",
        "            # if method == \"lasso\":\n",
        "            #     reg = Lasso(alpha=0.001).fit(X_n, y)\n",
        "            # elif method == \"l1\":\n",
        "            #     reg = ElasticNet(alpha=0.001, l1_ratio=1.0).fit(X_n, y)\n",
        "            # elif method == \"ridge\":\n",
        "            #     reg = Ridge(alpha=0.001).fit(X_n, y)\n",
        "            # else:\n",
        "            #     reg = LinearRegression().fit(X_n, y)\n",
        "\n",
        "            # reg = Lasso(alpha=0.01).fit(X_n, y)\n",
        "            # reg = Ridge(alpha=0.001).fit(X_n, y)\n",
        "            reg = LinearRegression().fit(X_n, y)\n",
        "            side_length = int(np.sqrt(len(y)))\n",
        "            y = y.reshape((side_length, side_length))\n",
        "            top_indices = np.argsort(np.abs(reg.coef_))[-top_k:]\n",
        "\n",
        "            if top_k == 1:\n",
        "                pred_atts = []\n",
        "                fn_to_trys = []\n",
        "\n",
        "                for i in range(5):\n",
        "                    fn_to_try = patterns[np.argsort(np.abs(reg.coef_))[-1 - i]]\n",
        "                    pred_att = fn_to_try(sentence, torch_tokenizer)[1]\n",
        "                    pred_atts.append(pred_att)\n",
        "                    fn_to_trys.append(fn_to_try)\n",
        "\n",
        "                scores_five = []\n",
        "                for pred_att in pred_atts:\n",
        "                    jensonshannon_distances = []\n",
        "                    for row_att, row_out in zip(y, pred_att):\n",
        "                        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "                    try_score = np.mean(jensonshannon_distances)\n",
        "                    scores_five.append(try_score)\n",
        "\n",
        "                fn_winner = fn_to_trys[np.argmin(scores_five)]\n",
        "                pred_att = pred_atts[np.argmin(scores_five)]\n",
        "\n",
        "                print(f\"\\tUsing pattern: {fn_winner.__name__}\")\n",
        "\n",
        "            elif top_k == -1:\n",
        "                pred_att = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "                pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "            else:\n",
        "                pred_att = reg.intercept_ + sum(reg.coef_[i] * X[i] for i in top_indices)\n",
        "                pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "            jensonshannon_distances = []\n",
        "            for row_att, row_out in zip(y, pred_att):\n",
        "                jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "            score = np.mean(jensonshannon_distances)\n",
        "            sentence_scores.append(score)\n",
        "        \n",
        "        # scores[layer, head] = np.mean(sentence_scores)\n",
        "        scores[layer, head] = np.mean(sentence_scores)\n",
        "        print(f\"\\tScore for Layer {layer}, Head {head}: {np.mean(sentence_scores):.2f}\\n\")\n",
        "\n",
        "# for each head: do linear interpolation on patterns, set pred_att to sum with top_k hypotheses based on parameter magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fac3928",
      "metadata": {},
      "outputs": [],
      "source": [
        "# scores nan to mean\n",
        "scores = np.nan_to_num(scores, nan=np.nanmean(scores))\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "id": "fdfd9a1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8235349756887738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:1: SyntaxWarning: invalid escape sequence '\\k'\n",
            "<>:1: SyntaxWarning: invalid escape sequence '\\k'\n",
            "C:\\Users\\amkah\\AppData\\Local\\Temp\\ipykernel_20764\\1961385896.py:1: SyntaxWarning: invalid escape sequence '\\k'\n",
            "  np_saved_matrix = \"data\\k33_gpt2_automation_scores.npy\"\n"
          ]
        }
      ],
      "source": [
        "np_saved_matrix = \"data\\k33_gpt2_automation_scores.npy\"\n",
        "saved_scores = np.load(np_saved_matrix)\n",
        "print(1-np.mean(saved_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e3b5d3b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.523, 0.581, 0.527, 0.492, 0.722, 0.61, 0.535, 0.683, 0.587, 0.694, 0.622, 0.594],\n",
            "[0.628, 0.597, 0.668, 0.65, 0.621, 0.653, 0.845, 0.661, 0.739, 0.769, 0.623, 0.539],\n",
            "[0.981, 0.547, 0.629, 0.699, 0.637, 0.69, 0.547, 0.652, 0.62, 0.968, 0.614, 0.613],\n",
            "[0.548, 0.654, 0.585, 0.558, 0.654, 0.691, 0.652, 0.692, 0.662, 0.486, 0.589, 0.551],\n",
            "[0.563, 0.555, 0.612, 0.707, 0.605, 0.54, 0.552, 0.629, 0.585, 0.66, 0.554, 0.586],\n",
            "[0.585, 0.774, 0.623, 0.574, 0.586, 0.69, 0.681, 0.622, 0.572, 0.556, 0.513, 0.626],\n",
            "[0.663, 0.58, 0.558, 0.679, 0.609, 0.527, 0.577, 0.594, 0.613, 0.526, 0.604, 0.517],\n",
            "[0.548, 0.558, 0.576, 0.674, 0.507, 0.541, 0.72, 0.7, 0.565, 0.584, 0.558, 0.644],\n",
            "[0.625, 0.634, 0.738, 0.594, 0.607, 0.671, 0.716, 0.595, 0.591, 0.609, 0.528, 0.565],\n",
            "[0.609, 0.607, 0.621, 0.617, 0.594, 0.563, 0.654, 0.639, 0.592, 0.718, 0.609, 0.621],\n",
            "[0.594, 0.549, 0.561, 0.666, 0.55, 0.556, 0.534, 0.614, 0.572, 0.522, 0.527, 0.533],\n",
            "[0.636, 0.625, 0.606, 0.612, 0.608, 0.575, 0.63, 0.635, 0.491, 0.583, 0.601, 0.611],\n"
          ]
        }
      ],
      "source": [
        "encoder_data = [[0.47660303718148705, 0.41882330851204436, 0.473115205363275, 0.507762038411257, 0.27761188740136067, 0.39020215931501745, 0.46525811458612265, 0.31704272964649716, 0.41316023998342083, 0.3055237002763774, 0.3784984333398841, 0.4057223145626849],\n",
        " [0.3717216773426008, 0.40327361009981344, 0.33245709409176044, 0.3499413011242562, 0.37894292747891417, 0.3466894808157926, 0.15513592187403683, 0.33891702169683596, 0.2605724510577071, 0.2307263012613643, 0.37716567282808905, 0.46068018620922524],\n",
        " [0.01890994824984555, 0.45321934810789766, 0.37074754622960815, 0.3005937176085346, 0.363485807195348, 0.3095568762016904, 0.4533234245886359, 0.3479424659074174, 0.380367299647174, 0.032195955038558134, 0.3861719758794779, 0.3869485260919198],\n",
        " [0.4521484572138414, 0.34571920835571496, 0.4148225877422708, 0.4416678752238985, 0.34603159429317437, 0.3087678373077368, 0.3484270700083202, 0.3075801186295602, 0.33805974504108605, 0.5142841745565918, 0.41130020025752256, 0.44898115243392284],\n",
        " [0.43652908670251006, 0.44505118133039206, 0.3878856924621522, 0.2933945065442124, 0.3947621498483996, 0.4604165634213228, 0.447609729042168, 0.37057341137053273, 0.4153138026539302, 0.33978374360702973, 0.4462459134644406, 0.41435801188750276],\n",
        " [0.41503355042501117, 0.22589023986983126, 0.3771116441656409, 0.42634068362285543, 0.4139426738155645, 0.3098452545127487, 0.31888623382172426, 0.378209224041549, 0.4284631385756788, 0.4437722083035277, 0.48695804050648533, 0.37437809662668126],\n",
        " [0.33747982645306845, 0.4204785892579204, 0.44202844360392085, 0.3210594022543019, 0.39079939483254705, 0.47255183094371883, 0.4225911140254576, 0.4057087246176945, 0.386976668358231, 0.47436804931254073, 0.39614855628605383, 0.48302074997108363],\n",
        " [0.45197645733715247, 0.44231642081647404, 0.42387777151618816, 0.3258729250504649, 0.4926125169479803, 0.4589200074691059, 0.27958161281743604, 0.3004697768097369, 0.43546315357465787, 0.4156273130082904, 0.4424471488591582, 0.355554771006258],\n",
        " [0.3748680342211462, 0.3658219710096527, 0.2617398391715175, 0.4057259341593981, 0.393141121243347, 0.32908020976301583, 0.2844711734759256, 0.40502342603652336, 0.40949547352709437, 0.391150503634955, 0.47234460764379665, 0.43515140114018525],\n",
        " [0.3910421737956252, 0.39274265196759006, 0.3791831618878873, 0.383494839754335, 0.40589505495855005, 0.4368577152666501, 0.34563235480316223, 0.3613787782906872, 0.4076182835549406, 0.2818122739701554, 0.3907679257801245, 0.3791242605578172],\n",
        " [0.4057048395051411, 0.4509513064465434, 0.43886444026255217, 0.33411714143267773, 0.4502488140410839, 0.44422135781704525, 0.46579982185557267, 0.3857856438839021, 0.42820464733347896, 0.4779629492486736, 0.4734418698935178, 0.4670072008246032],\n",
        " [0.3635157709512875, 0.37486153895926055, 0.39358054573166523, 0.38764816691278914, 0.392492241476459, 0.42453709710523274, 0.36983162250629154, 0.365027071700661, 0.5093410236458151, 0.4165531593736596, 0.3987662986381564, 0.3893409899840961]]\n",
        "\n",
        "# loop through and subtract each value from 1.0, round to 3 decimal places\n",
        "encoder_data_processed = [[round(1.0 - value, 3) for value in row] for row in encoder_data]\n",
        "for row in encoder_data_processed:\n",
        "    print(f\"{row},\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "id": "ce20f18a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.543, 0.82, 0.549, 0.354, 0.445, 0.424, 0.534, 0.473, 0.492, 0.57, 0.532, 0.508],\n",
            "[0.51, 0.541, 0.534, 0.604, 0.612, 0.579, 0.63, 0.621, 0.62, 0.681, 0.369, 0.455],\n",
            "[0.564, 0.731, 0.434, 0.517, 0.543, 0.495, 0.687, 0.516, 0.481, 0.451, 0.538, 0.717],\n",
            "[0.893, 0.654, 0.469, 0.658, 0.893, 0.665, 0.482, 0.502, 0.529, 0.57, 0.729, 0.5],\n",
            "[0.604, 0.612, 0.763, 0.539, 0.773, 0.665, 0.668, 0.515, 0.785, 0.597, 0.842, 0.506],\n",
            "[0.892, 0.979, 0.64, 0.604, 0.642, 0.902, 0.744, 0.701, 0.826, 0.857, 0.681, 0.78],\n",
            "[0.57, 0.793, 0.81, 0.753, 0.655, 0.718, 0.824, 0.673, 0.627, 0.955, 0.884, 0.631],\n",
            "[0.697, 0.854, 0.966, 0.712, 0.779, 0.685, 0.824, 0.903, 0.664, 0.717, 0.945, 0.916],\n",
            "[0.787, 0.919, 0.711, 0.798, 0.69, 0.621, 0.825, 0.677, 0.699, 0.769, 0.734, 0.816],\n",
            "[0.773, 0.897, 0.792, 0.612, 0.867, 0.8, 0.92, 0.77, 0.812, 0.925, 0.689, 0.931],\n",
            "[0.824, 0.888, 0.88, 0.808, 0.764, 0.745, 0.863, 0.791, 0.907, 0.694, 0.898, 0.783],\n",
            "[0.503, 0.785, 0.856, 0.644, 0.755, 0.859, 0.882, 0.799, 0.344, 0.879, 0.698, 0.704],\n"
          ]
        }
      ],
      "source": [
        "decoder_data = [[0.45679457795459544, 0.17962467884924407, 0.45058006762214403, 0.6461168076901916, 0.554758016095591, 0.5760074161269119, 0.46564047160081057, 0.5271692754565496, 0.5077610251527572, 0.4298279392057727, 0.4678102187469942, 0.49150130481798965],\n",
        " [0.4902322598095902, 0.4587733605931322, 0.46585821014729784, 0.3956402317401599, 0.3875658003018627, 0.4210522738849718, 0.3697133529507666, 0.3785108445041003, 0.37958842865009157, 0.3189244927293452, 0.6309630318977443, 0.5445086471532767],\n",
        " [0.435963940628892, 0.2685352775467594, 0.5660927443777496, 0.48318022123903503, 0.4568051476346893, 0.5052366933627256, 0.3125465825875403, 0.4835423361844562, 0.51931158966788, 0.5486600597915955, 0.46197700799720665, 0.2827341051881181],\n",
        " [0.10655174186303483, 0.34600680188858934, 0.5307256174510299, 0.34212005369298015, 0.10731824001613545, 0.3352640301642127, 0.5183676594635676, 0.49780862348982974, 0.47079490725621037, 0.4299753183643346, 0.27146791233730627, 0.500036096097296],\n",
        " [0.3963153180839415, 0.3876992317861739, 0.23653463622818754, 0.4612356706154476, 0.2274082528952396, 0.3352573665697178, 0.33232795271175264, 0.48491363394951686, 0.21489672223364759, 0.40349787029703504, 0.15809137439636184, 0.4943353079608847],\n",
        " [0.10807557058373614, 0.020509790710391475, 0.35985563084384126, 0.39606579875542025, 0.35762480520474127, 0.09753043616918168, 0.2564631403914261, 0.29911300228340554, 0.17407131805640313, 0.14296571971230781, 0.31875795585270406, 0.2204497299315993],\n",
        " [0.42951732000288334, 0.20658901936390517, 0.19043774684157866, 0.24666305172061934, 0.34519427437015077, 0.28153179210076923, 0.17570472734785383, 0.32717782380580984, 0.37258123225114387, 0.0452927488132334, 0.11568179254001729, 0.36913362701183466],\n",
        " [0.303184715789751, 0.14569921687720253, 0.034382567087374645, 0.28809111242379, 0.22140912971774457, 0.3154353267618083, 0.17618527288539249, 0.09701315525035256, 0.3362591226686061, 0.28296596233511695, 0.055467934614817645, 0.08420712931980762],\n",
        " [0.2128148507673627, 0.08083832849542337, 0.2891984671954332, 0.2024974144392996, 0.3100916019549652, 0.3793483992198359, 0.17519492740307333, 0.3225487694896923, 0.3005239551225001, 0.2311120267821574, 0.26648988180586664, 0.18409647345733365],\n",
        " [0.22654577352150562, 0.10344678870526429, 0.2078769195050633, 0.3878228880914839, 0.13250187067227603, 0.20033299978210398, 0.08048738961147386, 0.23024111289347662, 0.18807240975913242, 0.07454819421453217, 0.3111676159304801, 0.06907840214509232],\n",
        " [0.17607068532007877, 0.11206966476580618, 0.1203573827909147, 0.1921346860255035, 0.2361864299166741, 0.25475561583898326, 0.13749060399825905, 0.20925103462149283, 0.09256131678051546, 0.30560110136357727, 0.10199879483392073, 0.21694763981599716],\n",
        " [0.4971271007522499, 0.21531702697394842, 0.14397045012432935, 0.3558795725990279, 0.24475989652231295, 0.14052518978157583, 0.1177651525874369, 0.2010258620992226, 0.6555673379301096, 0.12107359125505912, 0.30178076817592514, 0.29587148522537965]]\n",
        "\n",
        "decoder_data_processed = [[round(1.0 - value, 3) for value in row] for row in decoder_data]\n",
        "for row in decoder_data_processed:\n",
        "    print(f\"{row},\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "id": "29076dac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n",
            "['adverbial_modulation', 'appositive_phrase_attention', 'cls_attention', 'complement_adjunct_relationship', 'compound_element_association', 'compound_modifier_attention', 'compound_word_attention_pattern', 'conjunction_based_grouping', 'conjunction_resolution', 'contextual_anchoring', 'coord_and_verb_dependency', 'coordination_attention', 'coreference_resolution', 'dependencies', 'dominant_subject_attention', 'emphasize_verbs_and_objects', 'eos_attention', 'first_token_dominance', 'first_token_domination', 'first_token_emphasis', 'head_initial_token_emphasis', 'high_saliency_relationship_detection', 'initial_contextual_attention', 'initial_element_reinforcement', 'initial_phrase_contextualization', 'initial_phrase_dominance', 'initial_reference_attention', 'initial_token_anchoring', 'initial_token_attachment', 'initial_token_attention', 'initial_token_centralization', 'initial_token_dominance', 'initial_token_emphasis', 'initial_token_reference_attention', 'initial_word_attention', 'last_token_attention', 'leading_contextual_emphasis', 'lexical_diversity_focus', 'main_subject_attention', 'negation_attention', 'next_attention', 'parenthetical_attention', 'parenthetical_insertion', 'pos_alignment', 'previous_attention', 'pronoun_reference', 'punctuation_attention', 'quotation_association', 'rare_word_dominance', 'relative_position_attention', 'repeated_attention', 'same_attention', 'semantic_grouping', 'sentence_beginning_attention_pattern', 'sentence_beginning_emphasis', 'sentence_beginning_salience', 'sentence_boundary_focus', 'sentence_initial_dominance', 'sentence_initiation_emphasis', 'sentence_level_attention', 'sentence_level_initial_token_repetition', 'sentence_opening_salience', 'sentence_position_preference', 'sentence_start_attention', 'sentence_start_dominance', 'sentence_start_emphasis', 'special_token_attention', 'token_emphasis_subsequent_dominance', 'token_reinforcement', 'uniform_attention']\n"
          ]
        }
      ],
      "source": [
        "# read csv and list out set of function names: C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\data\\best_fit_refinement_bert.csv\n",
        "import pandas as pd\n",
        "# df = pd.read_csv(\"data/best_fit_refinement_gpt2_new.csv\")\n",
        "# function_names += df['Pattern'].unique().tolist()\n",
        "\n",
        "function_names = []\n",
        "\n",
        "nonbert_names = [\n",
        "    \"data/best_fit_refinement_gpt2_new.csv\",\n",
        "    # \"data/best_fit_refinement_bert.csv\",\n",
        "    # \"data/best_fit_roberta.csv\",\n",
        "    # \"data/best_fit_3.csv\",\n",
        "    \"data/best_fit_gpt2.csv\",\n",
        "    \"data/best_fit_refinement_gpt2.csv\",\n",
        "#     \"data/best_fit_t5.csv\",\n",
        "#     \"data/best_fit_tinyllama.csv\"\n",
        "]\n",
        "\n",
        "bert_names = [\n",
        "    \"data/best_fit_3.csv\",\n",
        "    \"data/best_fit_refinement_bert.csv\",\n",
        "]\n",
        "\n",
        "# assign each head the function name with the lowest score from the csvs\n",
        "best_fits_bert = [None] * (12 * 12)\n",
        "for name in bert_names:\n",
        "    try:\n",
        "        df = pd.read_csv(name)\n",
        "        function_names += df['Pattern'].unique().tolist()\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            layer = row['i']\n",
        "            head = row['j']\n",
        "            function_name = row['Pattern']\n",
        "            score = row['Score']\n",
        "\n",
        "            # print(layer, head, function_name, score)\n",
        "\n",
        "            idx = layer * 12 + head\n",
        "            if best_fits_bert[idx] is None:\n",
        "                best_fits_bert[idx] = (function_name, score)\n",
        "            else:\n",
        "                existing_score = best_fits_bert[idx][1]\n",
        "                if score < existing_score:\n",
        "                    best_fits_bert[idx] = (function_name, score)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {name}: {e}\")\n",
        "        continue\n",
        "\n",
        "best_fits_gpt = [None] * (12 * 12)\n",
        "for name in nonbert_names:\n",
        "    try:\n",
        "        df = pd.read_csv(name)\n",
        "        function_names += df['Pattern'].unique().tolist()\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            layer = row['i']\n",
        "            head = row['j']\n",
        "            function_name = row['Pattern']\n",
        "            score = row['Score']\n",
        "\n",
        "            # print(layer, head, function_name, score)\n",
        "\n",
        "            idx = layer * 12 + head\n",
        "            if best_fits_gpt[idx] is None:\n",
        "                best_fits_gpt[idx] = (function_name, score)\n",
        "            else:\n",
        "                existing_score = best_fits_gpt[idx][1]\n",
        "                if score < existing_score:\n",
        "                    best_fits_gpt[idx] = (function_name, score)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {name}: {e}\")\n",
        "        continue\n",
        "\n",
        "function_names = set(function_names)\n",
        "function_names.remove('initial_token_emphasis_subsequent_dominance')\n",
        "function_names.add('token_emphasis_subsequent_dominance')\n",
        "function_names = list(function_names)\n",
        "function_names.sort()\n",
        "print(len(function_names))\n",
        "print(function_names)\n",
        "\n",
        "# loop through best_fits_bert and attach index of function_names\n",
        "for i in range(len(best_fits_bert)):\n",
        "    if best_fits_bert[i] is not None:\n",
        "        function_name, score = best_fits_bert[i]\n",
        "        function_index = function_names.index(function_name)\n",
        "        best_fits_bert[i] = (function_index, function_name, score)\n",
        "    else:\n",
        "        best_fits_bert[i] = (None, None, None)\n",
        "\n",
        "# loop through best_fits_gpt and attach index of function_names\n",
        "for i in range(len(best_fits_gpt)):\n",
        "    if best_fits_gpt[i] is not None:\n",
        "        function_name, score = best_fits_gpt[i]\n",
        "        function_index = function_names.index(function_name)\n",
        "        best_fits_gpt[i] = (function_index, function_name, score)\n",
        "    else:\n",
        "        best_fits_gpt[i] = (None, None, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "id": "17a14401",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(62, 'sentence_position_preference', 0.2914884341553049),\n",
              " (37, 'lexical_diversity_focus', 0.1329117627744686),\n",
              " (62, 'sentence_position_preference', 0.3004906719283918),\n",
              " (51, 'same_attention', 0.2536708417295858),\n",
              " (51, 'same_attention', 0.4630519576815052),\n",
              " (37, 'lexical_diversity_focus', 0.2297301121221685),\n",
              " (62, 'sentence_position_preference', 0.335115775932713),\n",
              " (9, 'contextual_anchoring', 0.415118853582096),\n",
              " (9, 'contextual_anchoring', 0.3463049901105369),\n",
              " (62, 'sentence_position_preference', 0.2260690974734531),\n",
              " (9, 'contextual_anchoring', 0.3294146903362529),\n",
              " (9, 'contextual_anchoring', 0.2606255529113294),\n",
              " (62, 'sentence_position_preference', 0.4031819855104725),\n",
              " (9, 'contextual_anchoring', 0.3653769563361396),\n",
              " (62, 'sentence_position_preference', 0.3093851231838089),\n",
              " (57, 'sentence_initial_dominance', 0.3659560466167397),\n",
              " (62, 'sentence_position_preference', 0.3181835443514528),\n",
              " (62, 'sentence_position_preference', 0.4012687165249795),\n",
              " (62, 'sentence_position_preference', 0.3449164776194851),\n",
              " (62, 'sentence_position_preference', 0.2961582511321428),\n",
              " (62, 'sentence_position_preference', 0.306940476173121),\n",
              " (62, 'sentence_position_preference', 0.3447064823561004),\n",
              " (9, 'contextual_anchoring', 0.3423282871215644),\n",
              " (37, 'lexical_diversity_focus', 0.3207622978597119),\n",
              " (62, 'sentence_position_preference', 0.3722101821465495),\n",
              " (57, 'sentence_initial_dominance', 0.2977809251850797),\n",
              " (44, 'previous_attention', 0.3967831138356478),\n",
              " (62, 'sentence_position_preference', 0.423782901657195),\n",
              " (44, 'previous_attention', 0.5292600091283675),\n",
              " (62, 'sentence_position_preference', 0.4460811018524942),\n",
              " (63, 'sentence_start_attention', 0.3719683200488234),\n",
              " (9, 'contextual_anchoring', 0.3424108647624584),\n",
              " (62, 'sentence_position_preference', 0.4348656325835338),\n",
              " (62, 'sentence_position_preference', 0.4460266409117334),\n",
              " (62, 'sentence_position_preference', 0.3331010577707046),\n",
              " (63, 'sentence_start_attention', 0.2938262147521598),\n",
              " (29, 'initial_token_attention', 0.1080639091187956),\n",
              " (63, 'sentence_start_attention', 0.3847544925534193),\n",
              " (44, 'previous_attention', 0.4934090860803749),\n",
              " (63, 'sentence_start_attention', 0.3867980974200595),\n",
              " (55, 'sentence_beginning_salience', 0.1509321345851125),\n",
              " (57, 'sentence_initial_dominance', 0.3617767573832157),\n",
              " (62, 'sentence_position_preference', 0.4486941484346213),\n",
              " (44, 'previous_attention', 0.5065180575053695),\n",
              " (44, 'previous_attention', 0.5436016383630975),\n",
              " (2, 'cls_attention', 0.4921965488182295),\n",
              " (33, 'initial_token_reference_attention', 0.276627239770175),\n",
              " (62, 'sentence_position_preference', 0.4269676670055858),\n",
              " (63, 'sentence_start_attention', 0.4473045610852385),\n",
              " (63, 'sentence_start_attention', 0.4438541918044546),\n",
              " (57, 'sentence_initial_dominance', 0.237554894796999),\n",
              " (None, None, None),\n",
              " (63, 'sentence_start_attention', 0.2494428868748612),\n",
              " (63, 'sentence_start_attention', 0.383629971877277),\n",
              " (57, 'sentence_initial_dominance', 0.3717556567697547),\n",
              " (17, 'first_token_dominance', 0.3608984786591979),\n",
              " (57, 'sentence_initial_dominance', 0.2455805573623935),\n",
              " (62, 'sentence_position_preference', 0.383406324422565),\n",
              " (29, 'initial_token_attention', 0.1864334192961182),\n",
              " (44, 'previous_attention', 0.0437074035846249),\n",
              " (29, 'initial_token_attention', 0.1585982680865517),\n",
              " (2, 'cls_attention', 0.0307966296468495),\n",
              " (57, 'sentence_initial_dominance', 0.4275360659674973),\n",
              " (62, 'sentence_position_preference', 0.3563033453659633),\n",
              " (63, 'sentence_start_attention', 0.3998201058737868),\n",
              " (29, 'initial_token_attention', 0.127452092152257),\n",
              " (29, 'initial_token_attention', 0.2906156277547844),\n",
              " (63, 'sentence_start_attention', 0.3228344980565573),\n",
              " (29, 'initial_token_attention', 0.2504397825572908),\n",
              " (57, 'sentence_initial_dominance', 0.1676847526089761),\n",
              " (57, 'sentence_initial_dominance', 0.3513798383287888),\n",
              " (63, 'sentence_start_attention', 0.2511496232682226),\n",
              " (62, 'sentence_position_preference', 0.4073540779773343),\n",
              " (29, 'initial_token_attention', 0.2791963636289706),\n",
              " (57, 'sentence_initial_dominance', 0.2224797070984331),\n",
              " (57, 'sentence_initial_dominance', 0.2692157752765105),\n",
              " (57, 'sentence_initial_dominance', 0.3248577558212199),\n",
              " (63, 'sentence_start_attention', 0.3079212098493306),\n",
              " (63, 'sentence_start_attention', 0.1818505349268361),\n",
              " (57, 'sentence_initial_dominance', 0.3750384803435289),\n",
              " (57, 'sentence_initial_dominance', 0.4288196877280613),\n",
              " (2, 'cls_attention', 0.067090146229909),\n",
              " (29, 'initial_token_attention', 0.1431834769032015),\n",
              " (63, 'sentence_start_attention', 0.401424012228409),\n",
              " (29, 'initial_token_attention', 0.3862183557064797),\n",
              " (29, 'initial_token_attention', 0.1647339072349563),\n",
              " (2, 'cls_attention', 0.0522305542941697),\n",
              " (57, 'sentence_initial_dominance', 0.3174508728557639),\n",
              " (57, 'sentence_initial_dominance', 0.2735308763001257),\n",
              " (57, 'sentence_initial_dominance', 0.3208056351580075),\n",
              " (57, 'sentence_initial_dominance', 0.211498096576139),\n",
              " (29, 'initial_token_attention', 0.1511048961730078),\n",
              " (57, 'sentence_initial_dominance', 0.4085264146024003),\n",
              " (57, 'sentence_initial_dominance', 0.3038300452075505),\n",
              " (29, 'initial_token_attention', 0.0825894546573127),\n",
              " (29, 'initial_token_attention', 0.1324738611655243),\n",
              " (57, 'sentence_initial_dominance', 0.2255889075931827),\n",
              " (29, 'initial_token_attention', 0.1078222327620379),\n",
              " (57, 'sentence_initial_dominance', 0.3247607600994592),\n",
              " (29, 'initial_token_attention', 0.1998049348924919),\n",
              " (63, 'sentence_start_attention', 0.3205083971465584),\n",
              " (62, 'sentence_position_preference', 0.3887177144253593),\n",
              " (29, 'initial_token_attention', 0.2436229480873663),\n",
              " (68, 'token_reinforcement', 0.3698229306541032),\n",
              " (57, 'sentence_initial_dominance', 0.332380070480717),\n",
              " (29, 'initial_token_attention', 0.2471635171258284),\n",
              " (63, 'sentence_start_attention', 0.2847343998655379),\n",
              " (57, 'sentence_initial_dominance', 0.2128985138767071),\n",
              " (57, 'sentence_initial_dominance', 0.2787755712982808),\n",
              " (29, 'initial_token_attention', 0.1314838437894311),\n",
              " (29, 'initial_token_attention', 0.2479079944741037),\n",
              " (63, 'sentence_start_attention', 0.4224550447517201),\n",
              " (29, 'initial_token_attention', 0.1241508600219557),\n",
              " (57, 'sentence_initial_dominance', 0.2248967160138881),\n",
              " (55, 'sentence_beginning_salience', 0.0894682433938373),\n",
              " (57, 'sentence_initial_dominance', 0.2545525442952348),\n",
              " (57, 'sentence_initial_dominance', 0.1991169027961679),\n",
              " (55, 'sentence_beginning_salience', 0.0714835655208253),\n",
              " (63, 'sentence_start_attention', 0.3395318089948592),\n",
              " (55, 'sentence_beginning_salience', 0.1077038776010236),\n",
              " (29, 'initial_token_attention', 0.1948507483076471),\n",
              " (29, 'initial_token_attention', 0.11894959085727),\n",
              " (29, 'initial_token_attention', 0.1219128801095612),\n",
              " (57, 'sentence_initial_dominance', 0.221458127907947),\n",
              " (57, 'sentence_initial_dominance', 0.3272238578270949),\n",
              " (57, 'sentence_initial_dominance', 0.2856583540549461),\n",
              " (29, 'initial_token_attention', 0.1144459720401139),\n",
              " (63, 'sentence_start_attention', 0.266277493584839),\n",
              " (29, 'initial_token_attention', 0.1038358649528495),\n",
              " (63, 'sentence_start_attention', 0.3418683330753586),\n",
              " (55, 'sentence_beginning_salience', 0.1012886946707291),\n",
              " (29, 'initial_token_attention', 0.239546029895825),\n",
              " (9, 'contextual_anchoring', 0.3113364437936279),\n",
              " (57, 'sentence_initial_dominance', 0.2205793787672746),\n",
              " (29, 'initial_token_attention', 0.1450168694812143),\n",
              " (57, 'sentence_initial_dominance', 0.3380040948459595),\n",
              " (57, 'sentence_initial_dominance', 0.2505720366738785),\n",
              " (29, 'initial_token_attention', 0.1561793692367714),\n",
              " (55, 'sentence_beginning_salience', 0.1183617685886583),\n",
              " (29, 'initial_token_attention', 0.2281710396092278),\n",
              " (9, 'contextual_anchoring', 0.4326743689830434),\n",
              " (29, 'initial_token_attention', 0.1273877076332941),\n",
              " (63, 'sentence_start_attention', 0.3424803209437157),\n",
              " (63, 'sentence_start_attention', 0.3435287563563883)]"
            ]
          },
          "execution_count": 278,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_fits_gpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8216bd55",
      "metadata": {},
      "outputs": [],
      "source": [
        "list_ex = [1,2,3]\n",
        "np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aeb71ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"data/interpolation_experiment_gpt2_lasso_a=0.01_k=N.npy\", scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd2b457",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = []\n",
        "for pattern in patterns:\n",
        "    try:\n",
        "        instance = pattern(sentence, tokenizer)[1].flatten()\n",
        "        # asssert that instance is a 2d array\n",
        "        if instance.ndim != 1:\n",
        "            print(f\"\\t\\tRemoved pattern {pattern.__name__} due to incorrect output dimensions.\")\n",
        "            del patterns[patterns.index(pattern)]\n",
        "            continue\n",
        "\n",
        "\n",
        "        # check if nan in instance:\n",
        "        if np.isnan(instance).any():\n",
        "            del patterns[patterns.index(pattern)]\n",
        "            print(f\"\\t\\tRemoved pattern {pattern.__name__} due to NaN values.\")\n",
        "            continue\n",
        "        X.append(instance)\n",
        "        # print(instance.shape)\n",
        "        # print(f\"Tested pattern: {pattern.__name__} successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        # print(f\"Error testing pattern {pattern.__name__}: {e}\")\n",
        "        print(f\"\\t\\tRemoved pattern {pattern.__name__} due to error during testing.\")\n",
        "        del patterns[patterns.index(pattern)]\n",
        "        continue\n",
        "X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15078a0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# dict: pattern name -> number of lines\n",
        "pattern_line_dict = {}\n",
        "for pattern, num_lines in zip(patterns, number_of_lines):\n",
        "    pattern_line_dict[pattern.__name__] = num_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "009616a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "line_count = []\n",
        "# go through scores and assign the number of lines based on the patter name\n",
        "for layer, head, pattern_name, score in scores:\n",
        "    num_lines = pattern_line_dict.get(pattern_name, None)\n",
        "    line_count.append(num_lines)\n",
        "print(len(line_count))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b192807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# i only need score from scores, not head and layer\n",
        "final_scores = [s[3] for s in scores]\n",
        "\n",
        "line_count_np = np.array(line_count).flatten()\n",
        "final_scores_np = np.array(final_scores).flatten()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(line_count, final_scores, color='blue')\n",
        "plt.xlabel(\"Number of Non-Comment, Non-Blank Lines\\n\")\n",
        "plt.ylabel(\"Score\")\n",
        "# model is bert\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "#bold second line in title\n",
        "second_line = f\"{model.config.architectures[0]}\"\n",
        "# how to bold text in only part of title\n",
        "plt.title(f\"Function Length vs Score | Score is from \\nAuto (K=1) Refinement\\n\\n{second_line}\\n\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2788d64",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PROGRAM COMPLEXITY ANALYSIS | FUNCTION LENGTH VS SCORE\n",
        "\n",
        "#make scatterplot, loop through functions and count number of non-comment, non-blank lines and then get scores and at the end plot all line/score pairs\n",
        "\n",
        "folder_of_interest = \"automation_results_gpt2\"\n",
        "\n",
        "complexity = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        score_path = os.path.join(folder_of_interest, \"scores\", f\"layer{layer}_head{head}_score.txt\")\n",
        "        # program_path = os.path.join(folder_of_interest, \"llm_code\", f\"code_layer_{layer}\", f\"{head}_output.py\")\n",
        "        # program example: automation_results_bert\\llm_code\\layer0_head0_code.py\n",
        "        program_path = os.path.join(folder_of_interest, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "        if not os.path.exists(score_path):\n",
        "            print(\"here\")\n",
        "            continue\n",
        "\n",
        "        with open(score_path, \"r\") as f:\n",
        "            try:\n",
        "                score = float(f.read().strip())\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        with open(program_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "            code_lines = [line for line in lines if line.strip() and not line.strip().startswith(\"#\")]\n",
        "            num_code_lines = len(code_lines)\n",
        "        \n",
        "        complexity.append((num_code_lines, score))\n",
        "        print(f\"L{layer},H{head} | Lines: {num_code_lines} | Score: {score}\")\n",
        "\n",
        "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "line_counts = [item[0] for item in complexity]\n",
        "scores = [item[1] for item in complexity]\n",
        "plt.scatter(line_counts, scores, color='blue')\n",
        "plt.xlabel(\"Number of Non-Comment, Non-Blank Lines\\n\")\n",
        "plt.ylabel(\"Score\")\n",
        "# model is bert\n",
        "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "#bold second line in title\n",
        "second_line = f\"{model.config.architectures[0]}\"\n",
        "# how to bold text in only part of title\n",
        "plt.title(f\"Function Length vs Score | Score is from \\nNo Refinement\\n\\n{second_line}\\n\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3a0037",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Hypothesis Structure Experiments\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e73244",
      "metadata": {},
      "outputs": [],
      "source": [
        "programs = [next_attention, previous_attention, same_attention, punctuation_attention, dependencies,\n",
        "            last_token_attention, uniform_attention, cls_attention, eos_attention, pos_alignment,\n",
        "            special_token_attention, repeated_attention, noun_modifier_attention, pronoun_attention,\n",
        "            single_token_attention, root_cluster_attention]\n",
        "programs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "id": "7ac957f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n",
            "['adverbial_modulation', 'appositive_phrase_attention', 'cls_attention', 'complement_adjunct_relationship', 'compound_element_association', 'compound_modifier_attention', 'compound_word_attention_pattern', 'conjunction_based_grouping', 'conjunction_resolution', 'contextual_anchoring', 'coord_and_verb_dependency', 'coordination_attention', 'coreference_resolution', 'dependencies', 'dominant_subject_attention', 'emphasize_verbs_and_objects', 'eos_attention', 'first_token_dominance', 'first_token_domination', 'first_token_emphasis', 'head_initial_token_emphasis', 'high_saliency_relationship_detection', 'initial_contextual_attention', 'initial_element_reinforcement', 'initial_phrase_contextualization', 'initial_phrase_dominance', 'initial_reference_attention', 'initial_token_anchoring', 'initial_token_attachment', 'initial_token_attention', 'initial_token_centralization', 'initial_token_dominance', 'initial_token_emphasis', 'initial_token_reference_attention', 'initial_word_attention', 'last_token_attention', 'leading_contextual_emphasis', 'lexical_diversity_focus', 'main_subject_attention', 'negation_attention', 'next_attention', 'parenthetical_attention', 'parenthetical_insertion', 'pos_alignment', 'previous_attention', 'pronoun_reference', 'punctuation_attention', 'quotation_association', 'rare_word_dominance', 'relative_position_attention', 'repeated_attention', 'same_attention', 'semantic_grouping', 'sentence_beginning_attention_pattern', 'sentence_beginning_emphasis', 'sentence_beginning_salience', 'sentence_boundary_focus', 'sentence_initial_dominance', 'sentence_initiation_emphasis', 'sentence_level_attention', 'sentence_level_initial_token_repetition', 'sentence_opening_salience', 'sentence_position_preference', 'sentence_start_attention', 'sentence_start_dominance', 'sentence_start_emphasis', 'special_token_attention', 'token_emphasis_subsequent_dominance', 'token_reinforcement', 'uniform_attention']\n"
          ]
        }
      ],
      "source": [
        "# read csv and list out set of function names: C:\\Users\\amkah\\OneDrive\\Documents\\GitHub\\LLM-Interpretability\\data\\best_fit_refinement_bert.csv\n",
        "import pandas as pd\n",
        "# df = pd.read_csv(\"data/best_fit_refinement_gpt2_new.csv\")\n",
        "# function_names += df['Pattern'].unique().tolist()\n",
        "\n",
        "function_names = []\n",
        "\n",
        "nonbert_names = [\n",
        "    \"data/best_fit_refinement_gpt2_new.csv\",\n",
        "    # \"data/best_fit_refinement_bert.csv\",\n",
        "    # \"data/best_fit_roberta.csv\",\n",
        "    # \"data/best_fit_3.csv\",\n",
        "    \"data/best_fit_gpt2.csv\",\n",
        "    \"data/best_fit_refinement_gpt2.csv\",\n",
        "#     \"data/best_fit_t5.csv\",\n",
        "#     \"data/best_fit_tinyllama.csv\"\n",
        "]\n",
        "\n",
        "bert_names = [\n",
        "    \"data/best_fit_3.csv\",\n",
        "    \"data/best_fit_refinement_bert.csv\",\n",
        "]\n",
        "\n",
        "# assign each head the function name with the lowest score from the csvs\n",
        "best_fits_bert = [None] * (12 * 12)\n",
        "for name in bert_names:\n",
        "    try:\n",
        "        df = pd.read_csv(name)\n",
        "        function_names += df['Pattern'].unique().tolist()\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            layer = row['i']\n",
        "            head = row['j']\n",
        "            function_name = row['Pattern']\n",
        "            score = row['Score']\n",
        "\n",
        "            # print(layer, head, function_name, score)\n",
        "\n",
        "            idx = layer * 12 + head\n",
        "            if best_fits_bert[idx] is None:\n",
        "                best_fits_bert[idx] = (function_name, score)\n",
        "            else:\n",
        "                existing_score = best_fits_bert[idx][1]\n",
        "                if score < existing_score:\n",
        "                    best_fits_bert[idx] = (function_name, score)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {name}: {e}\")\n",
        "        continue\n",
        "\n",
        "best_fits_gpt = [None] * (12 * 12)\n",
        "for name in nonbert_names:\n",
        "    try:\n",
        "        df = pd.read_csv(name)\n",
        "        function_names += df['Pattern'].unique().tolist()\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            layer = row['i']\n",
        "            head = row['j']\n",
        "            function_name = row['Pattern']\n",
        "            score = row['Score']\n",
        "\n",
        "            # print(layer, head, function_name, score)\n",
        "\n",
        "            idx = layer * 12 + head\n",
        "            if best_fits_gpt[idx] is None:\n",
        "                best_fits_gpt[idx] = (function_name, score)\n",
        "            else:\n",
        "                existing_score = best_fits_gpt[idx][1]\n",
        "                if score < existing_score:\n",
        "                    best_fits_gpt[idx] = (function_name, score)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {name}: {e}\")\n",
        "        continue\n",
        "\n",
        "function_names = set(function_names)\n",
        "function_names.remove('initial_token_emphasis_subsequent_dominance')\n",
        "function_names.add('token_emphasis_subsequent_dominance')\n",
        "function_names = list(function_names)\n",
        "function_names.sort()\n",
        "print(len(function_names))\n",
        "print(function_names)\n",
        "\n",
        "# loop through best_fits_bert and attach index of function_names\n",
        "for i in range(len(best_fits_bert)):\n",
        "    if best_fits_bert[i] is not None:\n",
        "        function_name, score = best_fits_bert[i]\n",
        "        function_index = function_names.index(function_name)\n",
        "        best_fits_bert[i] = (function_index, function_name, score)\n",
        "    else:\n",
        "        best_fits_bert[i] = (None, None, None)\n",
        "\n",
        "# loop through best_fits_gpt and attach index of function_names\n",
        "for i in range(len(best_fits_gpt)):\n",
        "    if best_fits_gpt[i] is not None:\n",
        "        function_name, score = best_fits_gpt[i]\n",
        "        function_index = function_names.index(function_name)\n",
        "        best_fits_gpt[i] = (function_index, function_name, score)\n",
        "    else:\n",
        "        best_fits_gpt[i] = (None, None, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "id": "9e8bcd3c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: Appended coreference_resolution from Layer 0, Head 0\n",
            "2: Appended coreference_resolution from Layer 0, Head 2\n",
            "3: Appended coreference_resolution from Layer 0, Head 9\n",
            "4: Appended punctuation_attention from Layer 1, Head 7\n",
            "5: Appended coreference_resolution from Layer 1, Head 8\n",
            "6: Appended pronoun_reference from Layer 4, Head 3\n",
            "7: Appended coordination_attention from Layer 5, Head 1\n",
            "8: Appended complement_adjunct_relationship from Layer 5, Head 6\n",
            "9: Appended emphasize_verbs_and_objects from Layer 6, Head 3\n",
            "10: Appended conjunction_resolution from Layer 7, Head 11\n",
            "11: Appended coreference_resolution from Layer 8, Head 0\n",
            "12: Appended coreference_resolution from Layer 8, Head 10\n",
            "13: Appended conjunction_resolution from Layer 8, Head 2\n",
            "14: Appended coord_and_verb_dependency from Layer 8, Head 5\n",
            "15: Appended coreference_resolution from Layer 9, Head 6\n",
            "16: Appended coreference_resolution from Layer 0, Head 8\n",
            "17: Appended initial_token_attention from Layer 10, Head 0\n",
            "18: Appended coreference_resolution from Layer 10, Head 1\n",
            "19: Appended coreference_resolution from Layer 1, Head 0\n",
            "20: Appended sentence_start_attention from Layer 1, Head 1\n",
            "21: Appended initial_token_attention from Layer 1, Head 2\n",
            "22: Appended initial_token_dominance from Layer 1, Head 3\n",
            "23: Appended initial_token_attention from Layer 1, Head 7\n",
            "24: Appended sentence_start_attention from Layer 3, Head 1\n",
            "25: Appended sentence_start_attention from Layer 3, Head 5\n",
            "26: Appended initial_token_dominance from Layer 4, Head 2\n",
            "27: Appended pronoun_reference from Layer 4, Head 3\n",
            "28: Appended sentence_initial_dominance from Layer 4, Head 8\n",
            "29: Appended sentence_start_emphasis from Layer 4, Head 9\n",
            "30: Appended first_token_emphasis from Layer 5, Head 1\n",
            "31: Appended coreference_resolution from Layer 6, Head 10\n",
            "32: Appended pronoun_reference from Layer 6, Head 6\n",
            "33: Appended coreference_resolution from Layer 6, Head 9\n",
            "34: Appended coreference_resolution from Layer 7, Head 10\n",
            "35: Appended coreference_resolution from Layer 7, Head 1\n",
            "36: Appended sentence_start_attention from Layer 7, Head 2\n",
            "37: Appended initial_token_attention from Layer 8, Head 0\n",
            "38: Appended sentence_start_attention from Layer 8, Head 5\n",
            "39: Appended sentence_start_attention from Layer 8, Head 7\n",
            "40: Appended initial_token_attention from Layer 9, Head 11\n",
            "41: Appended pronoun_reference from Layer 9, Head 6\n",
            "42: Appended adverbial_modulation from refinement\n",
            "43: Appended appositive_phrase_attention from refinement\n",
            "44: Appended cls_attention from refinement\n",
            "45: Appended compound_element_association from refinement\n",
            "46: Appended compound_modifier_attention from refinement\n",
            "47: Appended compound_word_attention_pattern from refinement\n",
            "48: Appended conjunction_based_grouping from refinement\n",
            "49: Appended dependencies from refinement\n",
            "50: Appended eos_attention from refinement\n",
            "51: Appended high_saliency_relationship_detection from refinement\n",
            "52: Appended last_token_attention from refinement\n",
            "53: Appended next_attention from refinement\n",
            "54: Appended parenthetical_attention from refinement\n",
            "55: Appended parenthetical_insertion from refinement\n",
            "56: Appended pos_alignment from refinement\n",
            "57: Appended previous_attention from refinement\n",
            "58: Appended punctuation_attention from refinement\n",
            "59: Appended quotation_association from refinement\n",
            "60: Appended relative_position_attention from refinement\n",
            "61: Appended repeated_attention from refinement\n",
            "62: Appended same_attention from refinement\n",
            "63: Appended special_token_attention from refinement\n",
            "64: Appended uniform_attention from refinement\n",
            "65: Appended adverbial_modulation from refinement\n",
            "66: Appended appositive_phrase_attention from refinement\n",
            "67: Appended cls_attention from refinement\n",
            "68: Appended compound_element_association from refinement\n",
            "69: Appended compound_modifier_attention from refinement\n",
            "70: Appended compound_word_attention_pattern from refinement\n",
            "71: Appended conjunction_based_grouping from refinement\n",
            "72: Appended contextual_anchoring from refinement\n",
            "73: Appended first_token_domination from refinement\n",
            "74: Appended high_saliency_relationship_detection from refinement\n",
            "75: Appended initial_contextual_attention from refinement\n",
            "76: Appended initial_element_reinforcement from refinement\n",
            "77: Appended initial_phrase_contextualization from refinement\n",
            "78: Appended initial_phrase_dominance from refinement\n",
            "79: Appended initial_token_anchoring from refinement\n",
            "80: Appended initial_token_centralization from refinement\n",
            "81: Appended initial_token_dominance from refinement\n",
            "82: Appended initial_token_reference_attention from refinement\n",
            "83: Appended initial_word_attention from refinement\n",
            "84: Appended lexical_diversity_focus from refinement\n",
            "85: Appended main_subject_attention from refinement\n",
            "86: Appended parenthetical_attention from refinement\n",
            "87: Appended parenthetical_insertion from refinement\n",
            "88: Appended pos_alignment from refinement\n",
            "89: Appended previous_attention from refinement\n",
            "90: Appended quotation_association from refinement\n",
            "91: Appended rare_word_dominance from refinement\n",
            "92: Appended relative_position_attention from refinement\n",
            "93: Appended same_attention from refinement\n",
            "94: Appended semantic_grouping from refinement\n",
            "95: Appended sentence_beginning_attention_pattern from refinement\n",
            "96: Appended sentence_beginning_salience from refinement\n",
            "97: Appended sentence_boundary_focus from refinement\n",
            "98: Appended sentence_level_attention from refinement\n",
            "99: Appended sentence_opening_salience from refinement\n",
            "100: Appended sentence_position_preference from refinement\n",
            "101: Appended sentence_start_attention from refinement\n",
            "102: Appended special_token_attention from refinement\n",
            "103: Appended token_reinforcement from refinement\n",
            "Total unique functions appended: 57\n"
          ]
        }
      ],
      "source": [
        "results_folders = [\n",
        "    \"automation_results_bert\",\n",
        "    \"automation_results_gpt2\",\n",
        "    \"automation_refinement_bert\",\n",
        "    \"automation_refinement_gpt2\",\n",
        "]\n",
        "\n",
        "destination_folder = \"golden_programs.py\"\n",
        "program_definitions = []\n",
        "file_names = []\n",
        "\n",
        "for folder in results_folders:\n",
        "    if \"results\" in folder:\n",
        "        # loop through all python files in llm_code to find matching function definitions to function_names\n",
        "        # if found, append to golden_programs.py with comment of layer and head\n",
        "\n",
        "        llm_code_folder = os.path.join(folder, \"llm_code\")\n",
        "        for filename in os.listdir(llm_code_folder):\n",
        "            if filename.endswith(\".py\"):\n",
        "                layer_head = filename.replace(\"layer\", \"\").replace(\"_head\", \"_\").replace(\"_code.py\", \"\")\n",
        "                layer, head = map(int, layer_head.split(\"_\"))\n",
        "                file_path = os.path.join(llm_code_folder, filename)\n",
        "                with open(file_path, \"r\") as f:\n",
        "                    code = f.read()\n",
        "                    for function_name in function_names:\n",
        "                        if f\"def {function_name}(\" in code:\n",
        "                            with open(destination_folder, \"a\") as dest_f:\n",
        "                                dest_f.write(f\"# Layer {layer}, Head {head}\\n\")\n",
        "                                dest_f.write(code + \"\\n\\n\")\n",
        "                            file_names.append(function_name)\n",
        "                            print(f\"{len(file_names)}: Appended {function_name} from Layer {layer}, Head {head}\")\n",
        "                        \n",
        "\n",
        "    elif \"refinement\" in folder:\n",
        "        # loop through names of all python files in master_list to find matching function_names\n",
        "        # if found, append to golden_programs.py with comment of layer and head\n",
        "        master_list_folder = os.path.join(folder, \"master_list\")\n",
        "        for filename in os.listdir(master_list_folder): # strip .py to see if its a match\n",
        "            if filename.endswith(\".py\") and filename.replace(\".py\", \"\") in function_names:\n",
        "                # read code and append to golden programs\n",
        "                file_path = os.path.join(master_list_folder, filename)\n",
        "                with open(file_path, \"r\") as f:\n",
        "                    code = f.read()\n",
        "                    with open(destination_folder, \"a\") as dest_f:\n",
        "                        dest_f.write(f\"# Refinement {filename.replace('.py', '')}\\n\")\n",
        "                        dest_f.write(code + \"\\n\\n\")\n",
        "                    file_names.append(filename.replace(\".py\", \"\"))\n",
        "                    print(f\"{len(file_names)}: Appended {filename.replace('.py', '')} from refinement\")\n",
        "\n",
        "# python_file = \"programs.py\"\n",
        "# # loop through all programs from this file, finding matching definitions to function_names, append to golden_programs.py\n",
        "# with open(python_file, \"r\") as f:\n",
        "#     code = f.read()\n",
        "#     for function_name in function_names:\n",
        "#         if f\"def {function_name}(\" in code:\n",
        "#             with open(destination_folder, \"a\") as dest_f:\n",
        "#                 dest_f.write(f\"# From {python_file}\\n\")\n",
        "#                 dest_f.write(code + \"\\n\\n\")\n",
        "#             file_names.append(function_name)\n",
        "#             print(f\"{len(file_names)}: Appended {function_name} from {python_file}\")\n",
        "\n",
        "print(f\"Total unique functions appended: {len(set(file_names))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a34e30",
      "metadata": {},
      "outputs": [],
      "source": [
        "# how many functions are in golden_programs.py\n",
        "executables = []\n",
        "\n",
        "with open(destination_folder, \"r\") as f:\n",
        "    golden_code = f.read()\n",
        "    unique_functions = set()\n",
        "    for function_name in function_names:\n",
        "        if f\"def {function_name}(\" in golden_code:\n",
        "            unique_functions.add(function_name)\n",
        "            # make it executable using function make_program_executable\n",
        "            # first get the function content of function_name as str from header to return\n",
        "            function_content = golden_code.split(f\"def {function_name}(\")[1]\n",
        "            function_content = f\"def {function_name}(\" + function_content\n",
        "\n",
        "            executables.append(make_program_executable(function_content))\n",
        "\n",
        "\n",
        "print(f\"Number of unique functions in golden_programs.py: {len(unique_functions)}\")\n",
        "# missing programs from unique functions to function_names\n",
        "\n",
        "print(\"Missing functions:\")\n",
        "for function_name in function_names:\n",
        "    if function_name not in unique_functions:\n",
        "        print(function_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 364,
      "id": "3a1b6108",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "execution_count": 364,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove \"initial_token_emphasis_subsequent_dominance\" from programs\n",
        "programs = [fn for fn in list_of_fns if fn.__name__ != \"initial_token_emphasis_subsequent_dominance\"]\n",
        "len(programs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a15438",
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect\n",
        "import golden_programs\n",
        "\n",
        "# functions_list = inspect.getmembers(golden_programs, inspect.isfunction)\n",
        "# remove \n",
        "# programs = [func for name, func in functions_list]\n",
        "\n",
        "sentence_data = sentences[:3]\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def program_similarity(att_one, att_two):\n",
        "    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "        p = np.clip(p, 1e-12, 1.0)\n",
        "        q = np.clip(q, 1e-12, 1.0)\n",
        "        p /= p.sum()\n",
        "        q /= q.sum()\n",
        "        m = 0.5 * (p + q)\n",
        "        return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
        "\n",
        "    jensonshannon_distances = []\n",
        "    for row_att, row_out in zip(att_one, att_two):\n",
        "        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "    score = np.mean(jensonshannon_distances)\n",
        "    return score\n",
        "\n",
        "x = len(programs)\n",
        "print(x)\n",
        "S = np.zeros((x, x))\n",
        "for i in range(1, x):\n",
        "    # if i in [106, 128]:\n",
        "    #     S[i, :] = 0.8\n",
        "    #     continue\n",
        "    print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
        "    for j in range(x):\n",
        "        if j % 36 == 0: print(f\"\\tinner loop {j/x:.0%}\")\n",
        "        # if j in [106, 128]:\n",
        "        #     S[i, j] = 0.8\n",
        "        #     continue\n",
        "\n",
        "        if i != j:\n",
        "            similarities = []\n",
        "            program_one = programs[i]\n",
        "            program_two = programs[j]\n",
        "\n",
        "            for sentence in sentence_data:\n",
        "                h1, activations_one = program_one(sentence, tokenizer)\n",
        "                try:\n",
        "                    h2, activations_two = program_two(sentence, tokenizer)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing sentence with programs {i} and {j}: {e}\")\n",
        "                    continue\n",
        "                similarities.append(program_similarity(activations_one, activations_two))\n",
        "        \n",
        "            S[i, j] = np.mean(similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "id": "df49721d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# set diagonal to 0\n",
        "for i in range(x):\n",
        "    S[i, i] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a059c97f",
      "metadata": {},
      "outputs": [],
      "source": [
        "for row in S:\n",
        "    print(f\"{[float(round(val, 3)) for val in row]},\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "id": "c6a62ef1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Group 1: ['adverbial_modulation', 'appositive_phrase_attention', 'cls_attention', 'complement_adjunct_relationship', 'compound_element_association', 'compound_modifier_attention', 'compound_word_attention_pattern', 'conjunction_based_grouping', 'conjunction_resolution', 'contextual_anchoring', 'coord_and_verb_dependency', 'coordination_attention', 'coreference_resolution', 'dependencies', 'dominant_subject_attention', 'emphasize_verbs_and_objects', 'eos_attention', 'first_token_dominance', 'first_token_domination', 'first_token_emphasis', 'head_initial_token_emphasis', 'high_saliency_relationship_detection', 'initial_contextual_attention', 'initial_element_reinforcement', 'initial_phrase_contextualization', 'initial_phrase_dominance', 'initial_reference_attention', 'initial_token_anchoring', 'initial_token_attachment', 'initial_token_attention', 'initial_token_centralization', 'initial_token_dominance', 'initial_token_emphasis', 'initial_token_reference_attention', 'initial_word_attention', 'last_token_attention', 'leading_contextual_emphasis', 'lexical_diversity_focus', 'main_subject_attention', 'negation_attention', 'next_attention', 'parenthetical_attention', 'parenthetical_insertion', 'pos_alignment', 'previous_attention', 'pronoun_reference', 'punctuation_attention', 'quotation_association', 'rare_word_dominance', 'relative_position_attention', 'repeated_attention', 'same_attention', 'semantic_grouping', 'sentence_beginning_attention_pattern', 'sentence_beginning_emphasis', 'sentence_beginning_salience', 'sentence_boundary_focus', 'sentence_initial_dominance', 'sentence_initiation_emphasis', 'sentence_level_attention', 'sentence_level_initial_token_repetition', 'sentence_opening_salience', 'sentence_position_preference', 'sentence_start_attention', 'sentence_start_dominance', 'sentence_start_emphasis', 'special_token_attention', 'token_emphasis_subsequent_dominance', 'token_reinforcement', 'uniform_attention']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def group_similar_programs(programs, S, threshold=0.6):\n",
        "    groups, used = [], set()\n",
        "    for i in range(len(programs)):\n",
        "        if i in used: continue\n",
        "        group = [i]\n",
        "        used.add(i)\n",
        "        \n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for group_member in group:\n",
        "                for j in range(len(programs)):\n",
        "                    if j not in used and S[group_member, j] < threshold:\n",
        "                        group.append(j)\n",
        "                        used.add(j)\n",
        "                        changed = True\n",
        "        groups.append([programs[idx].__name__ for idx in group])\n",
        "    \n",
        "    return groups\n",
        "\n",
        "groups = group_similar_programs(programs, S, threshold=0.00000000000000000000001)\n",
        "for i, group in enumerate(groups):\n",
        "    print(f\"Group {i+1}: {group}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "id": "31fc476c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Group 1: ['adverbial_modulation', 'appositive_phrase_attention', 'cls_attention', 'complement_adjunct_relationship', 'compound_element_association', 'compound_modifier_attention', 'compound_word_attention_pattern', 'conjunction_based_grouping', 'conjunction_resolution', 'contextual_anchoring', 'coord_and_verb_dependency', 'coordination_attention', 'coreference_resolution', 'dependencies', 'dominant_subject_attention', 'emphasize_verbs_and_objects', 'eos_attention', 'first_token_dominance', 'first_token_domination', 'first_token_emphasis', 'head_initial_token_emphasis', 'high_saliency_relationship_detection', 'initial_contextual_attention', 'initial_element_reinforcement', 'initial_phrase_contextualization', 'initial_phrase_dominance', 'initial_reference_attention', 'initial_token_anchoring', 'initial_token_attachment', 'initial_token_attention', 'initial_token_centralization', 'initial_token_dominance', 'initial_token_emphasis', 'initial_token_reference_attention', 'initial_word_attention', 'last_token_attention', 'leading_contextual_emphasis', 'lexical_diversity_focus', 'main_subject_attention', 'negation_attention', 'next_attention', 'parenthetical_attention', 'parenthetical_insertion', 'pos_alignment', 'previous_attention', 'pronoun_reference', 'punctuation_attention', 'quotation_association', 'rare_word_dominance', 'relative_position_attention', 'repeated_attention', 'same_attention', 'semantic_grouping', 'sentence_beginning_attention_pattern', 'sentence_beginning_emphasis', 'sentence_beginning_salience', 'sentence_boundary_focus', 'sentence_initial_dominance', 'sentence_initiation_emphasis', 'sentence_level_attention', 'sentence_level_initial_token_repetition', 'sentence_opening_salience', 'sentence_position_preference', 'sentence_start_attention', 'sentence_start_dominance', 'sentence_start_emphasis', 'special_token_attention', 'token_emphasis_subsequent_dominance', 'token_reinforcement', 'uniform_attention']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# S is your distance matrix (0 = same, 1 = different)\n",
        "# metric=\"precomputed\" tells sklearn you are providing distances, not points\n",
        "db = DBSCAN(eps=0.0000000000000000000001, min_samples=4, metric=\"precomputed\")\n",
        "labels = db.fit_predict(S)\n",
        "\n",
        "# Organize the results\n",
        "groups = {}\n",
        "for idx, label in enumerate(labels):\n",
        "    groups.setdefault(label, []).append(programs[idx].__name__)\n",
        "\n",
        "for label, members in groups.items():\n",
        "    print(f\"Group {label + 1}: {members}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 410,
      "id": "cec2a74d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 6  6  1  0  6  6  6 17  2  6  0  0  0  9  6  0  0 10  1  5  1  6  4  1\n",
            "  1  1  1  1  1  8  1  1  1  7  6 18  1  2  3  9 15  6  4 12 11 13 14  6\n",
            "  2  5  2  2  1  1  8  1  1 17  1 16  1 13 19  3  1 20  0  1  7  5]\n",
            "Group 7: ['adverbial_modulation', 'appositive_phrase_attention', 'compound_element_association', 'compound_modifier_attention', 'compound_word_attention_pattern', 'contextual_anchoring', 'dominant_subject_attention', 'high_saliency_relationship_detection', 'initial_word_attention', 'parenthetical_attention', 'quotation_association']\n",
            "Group 2: ['cls_attention', 'first_token_domination', 'head_initial_token_emphasis', 'initial_element_reinforcement', 'initial_phrase_contextualization', 'initial_phrase_dominance', 'initial_reference_attention', 'initial_token_anchoring', 'initial_token_attachment', 'initial_token_centralization', 'initial_token_dominance', 'initial_token_emphasis', 'leading_contextual_emphasis', 'semantic_grouping', 'sentence_beginning_attention_pattern', 'sentence_beginning_salience', 'sentence_boundary_focus', 'sentence_initiation_emphasis', 'sentence_level_initial_token_repetition', 'sentence_start_dominance', 'token_emphasis_subsequent_dominance']\n",
            "Group 1: ['complement_adjunct_relationship', 'coord_and_verb_dependency', 'coordination_attention', 'coreference_resolution', 'emphasize_verbs_and_objects', 'eos_attention', 'special_token_attention']\n",
            "Group 3: ['conjunction_resolution', 'lexical_diversity_focus', 'rare_word_dominance', 'repeated_attention', 'same_attention']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# 'average' linkage prevents the \"one giant group\" problem better than 'single'\n",
        "model = AgglomerativeClustering(\n",
        "    n_clusters=None, \n",
        "    distance_threshold=0.15, \n",
        "    metric='precomputed', \n",
        "    linkage='average' \n",
        ")\n",
        "\n",
        "labels = model.fit_predict(S)\n",
        "print(labels)\n",
        "\n",
        "groups = {}\n",
        "for idx, label in enumerate(labels):\n",
        "    groups.setdefault(label, []).append(programs[idx].__name__)\n",
        "\n",
        "for label, members in groups.items():\n",
        "    if len(members) > 4:\n",
        "        print(f\"Group {label + 1}: {members}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 414,
      "id": "489880c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_ast_feature_vector(source_code):\n",
        "    tree = ast.parse(source_code)\n",
        "    # Extract every node type present in the code\n",
        "    node_types = [type(node).__name__ for node in ast.walk(tree)]\n",
        "    return Counter(node_types)\n",
        "\n",
        "def build_ast_similarity_matrix(programs):\n",
        "    # 1. Extract features for all programs\n",
        "    all_counters = [get_ast_feature_vector(p) for p in programs]\n",
        "    \n",
        "    # 2. Vectorize the counters (Vocabulary of AST node types)\n",
        "    unique_nodes = sorted(list(set(node for c in all_counters for node in c)))\n",
        "    vectors = []\n",
        "    for c in all_counters:\n",
        "        vectors.append([c[node] for node in unique_nodes])\n",
        "    \n",
        "    # 3. Compute Cosine Similarity Matrix\n",
        "    return cosine_similarity(vectors)\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load UniXcoder (or CodeBERT)\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model_name = \"microsoft/unixcoder-base\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# def get_embedding(source_code):\n",
        "#     inputs = tokenizer(source_code, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#         # Use the pooler output (global representation of the code)\n",
        "#         embedding = outputs.pooler_output\n",
        "#     return embedding.cpu().numpy()\n",
        "\n",
        "# def build_semantic_similarity_matrix(programs):\n",
        "#     embeddings = np.vstack([get_embedding(p) for p in programs])\n",
        "#     # Returns matrix S where S[i,j] is the similarity between program i and j\n",
        "#     return cosine_similarity(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82ad85a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_hybrid_matrix(programs, weight=0.5):\n",
        "    S_ast = build_ast_similarity_matrix(programs)\n",
        "    # S_sem = build_semantic_similarity_matrix(programs)\n",
        "    \n",
        "    # Weighted average to get a final normalized score [0, 1]\n",
        "    # S_final = (weight * S_ast) + ((1 - weight) * S_sem)\n",
        "    return S_ast\n",
        "\n",
        "# Example Usage:\n",
        "# programs = [code_str_1, code_str_2, code_str_3]\n",
        "# S = build_hybrid_matrix(programs)\n",
        "\n",
        "str_programs = [inspect.getsource(prog) for prog in programs]\n",
        "S_code = build_hybrid_matrix(str_programs)\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# 'average' linkage prevents the \"one giant group\" problem better than 'single'\n",
        "model = AgglomerativeClustering(\n",
        "    n_clusters=None, \n",
        "    distance_threshold=0.975, \n",
        "    metric='precomputed', \n",
        "    linkage='average' \n",
        ")\n",
        "\n",
        "labels = model.fit_predict(S_code)\n",
        "print(labels)\n",
        "\n",
        "groups = {}\n",
        "for idx, label in enumerate(labels):\n",
        "    groups.setdefault(label, []).append(programs[idx].__name__)\n",
        "\n",
        "program_definitions_str = []\n",
        "for program in programs:\n",
        "    program_definitions_str.append(inspect.getsource(program))\n",
        "\n",
        "for label, members in groups.items():\n",
        "    if len(members) > 1:\n",
        "        print(f\"Group {label + 1}: {members}\")\n",
        "        for i, member in enumerate(members):\n",
        "            # print out the corresponding member using the name\n",
        "            if i < 4:\n",
        "                member_index = [prog.__name__ for prog in programs].index(member)\n",
        "                print(f\"{program_definitions_str[member_index]}\\n\")\n",
        "            \n",
        "\n",
        "print(len(set(labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940d879e",
      "metadata": {},
      "outputs": [],
      "source": [
        "for row in S_code:\n",
        "    print(f\"{[float(round(val, 3)) for val in row]},\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "id": "8fee069f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n"
          ]
        }
      ],
      "source": [
        "for group in groups:\n",
        "    print(len(groups[group]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "id": "5d4cc220",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function golden_programs.sentence_start_emphasis(sentence: str, tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase) -> Tuple[str, numpy.ndarray]>"
            ]
          },
          "execution_count": 331,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "programs[66]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "id": "d54487ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing folder: automation_results_gpt2\n",
            "bert | L2 H1 | New Function: beginning_attention | New Score: 0.25736151087808234 | Prev: 0.2977809251850797 | Old Function: sentence_initial_dominance\n",
            "bert | L6 H0 | New Function: pronoun_focus_attention | New Score: 0.39417133335665977 | Prev: 0.4073540779773343 | Old Function: sentence_position_preference\n",
            "bert | L8 H2 | New Function: pronoun_initial_focus | New Score: 0.2724249772481505 | Prev: 0.3247607600994592 | Old Function: sentence_initial_dominance\n"
          ]
        }
      ],
      "source": [
        "results_folders = [\n",
        "    # \"automation_results_bert\",\n",
        "    \"automation_results_gpt2\",\n",
        "    # \"automation_results_bert_2\",\n",
        "    # \"automation_results_bert_greedy\"\n",
        "]\n",
        "\n",
        "# reshape best_fits_bert into 12x12 array\n",
        "best_fits_bert_array = []\n",
        "for i in range(12):\n",
        "    row = []\n",
        "    for j in range(12):\n",
        "        idx = i * 12 + j\n",
        "        row.append(best_fits_gpt[idx])\n",
        "    best_fits_bert_array.append(row)\n",
        "\n",
        "for folder in results_folders:\n",
        "    print(f\"Processing folder: {folder}\")\n",
        "    # there are two subfolders: scores and llm_code\n",
        "    scores_folder = os.path.join(folder, \"scores\")\n",
        "    code_folder = os.path.join(folder, \"llm_code\")\n",
        "    # loop through each file in scores_folder, if score is lower than best_fits_bert, add to function names and print out (\"bert\", layer, head, function_name, new_score, \"prev: \", old_score)\n",
        "\n",
        "    for score_file in os.listdir(scores_folder):\n",
        "        if not score_file.endswith(\"_score.txt\"):\n",
        "            continue\n",
        "        layer_head = score_file.replace(\"layer\", \"\").replace(\"_head\", \"_\").replace(\"_score.txt\", \"\")\n",
        "        layer, head = map(int, layer_head.split(\"_\"))\n",
        "        score_path = os.path.join(scores_folder, score_file)\n",
        "        with open(score_path, \"r\") as f:\n",
        "            try:\n",
        "                new_score = float(f.read().strip())\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        old_entry = best_fits_bert_array[layer][head]\n",
        "        if old_entry[0] is not None:\n",
        "            old_function_name = old_entry[1]\n",
        "            old_score = old_entry[2]\n",
        "            if new_score < old_score:\n",
        "                if old_function_name not in function_names:\n",
        "                    function_names.append(old_function_name)\n",
        "                # get function name from code file\n",
        "                code_file = f\"layer{layer}_head{head}_code.py\"\n",
        "                code_path = os.path.join(code_folder, code_file)\n",
        "                with open(code_path, \"r\") as cf:\n",
        "                    code_contents = cf.read()\n",
        "                    # extract function name from code contents\n",
        "                    func_name_start = code_contents.find(\"def \") + 4\n",
        "                    func_name_end = code_contents.find(\"(\", func_name_start)\n",
        "                    new_function_name = code_contents[func_name_start:func_name_end].strip()\n",
        "                    print(f\"bert | L{layer} H{head} | New Function: {new_function_name} | New Score: {new_score} | Prev: {old_score} | Old Function: {old_function_name}\")\n",
        "                    if new_function_name not in function_names:\n",
        "                        function_names.append(new_function_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ebc736",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "id": "9f4ca336",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading function: adverbial_modulation from automation_refinement\\master_list\\adverbial_modulation.py\n",
            "Loading function: appositive_phrase_attention from automation_refinement\\master_list\\appositive_phrase_attention.py\n",
            "Loading function: compound_element_association from automation_refinement\\master_list\\compound_element_association.py\n",
            "Loading function: compound_modifier_attention from automation_refinement\\master_list\\compound_modifier_attention.py\n",
            "Loading function: compound_word_attention_pattern from automation_refinement\\master_list\\compound_word_attention_pattern.py\n",
            "Loading function: conjunction_based_grouping from automation_refinement\\master_list\\conjunction_based_grouping.py\n",
            "Loading function: dependencies from automation_refinement\\master_list\\dependencies.py\n",
            "Loading function: high_saliency_relationship_detection from automation_refinement\\master_list\\high_saliency_relationship_detection.py\n",
            "Loading function: next_attention from automation_refinement\\master_list\\next_attention.py\n",
            "Loading function: parenthetical_attention from automation_refinement\\master_list\\parenthetical_attention.py\n",
            "Loading function: parenthetical_insertion from automation_refinement\\master_list\\parenthetical_insertion.py\n",
            "Loading function: pos_alignment from automation_refinement\\master_list\\pos_alignment.py\n",
            "Loading function: previous_attention from automation_refinement\\master_list\\previous_attention.py\n",
            "Loading function: punctuation_attention from automation_refinement\\master_list\\punctuation_attention.py\n",
            "Loading function: quotation_association from automation_refinement\\master_list\\quotation_association.py\n",
            "Loading function: relative_position_attention from automation_refinement\\master_list\\relative_position_attention.py\n",
            "Loading function: repeated_attention from automation_refinement\\master_list\\repeated_attention.py\n",
            "Loading function: same_attention from automation_refinement\\master_list\\same_attention.py\n",
            "Loading function: adverbial_modulation from automation_refinement_bert\\master_list\\adverbial_modulation.py\n",
            "Loading function: appositive_phrase_attention from automation_refinement_bert\\master_list\\appositive_phrase_attention.py\n",
            "Loading function: cls_attention from automation_refinement_bert\\master_list\\cls_attention.py\n",
            "Loading function: compound_element_association from automation_refinement_bert\\master_list\\compound_element_association.py\n",
            "Loading function: compound_modifier_attention from automation_refinement_bert\\master_list\\compound_modifier_attention.py\n",
            "Loading function: compound_word_attention_pattern from automation_refinement_bert\\master_list\\compound_word_attention_pattern.py\n",
            "Loading function: conjunction_based_grouping from automation_refinement_bert\\master_list\\conjunction_based_grouping.py\n",
            "Loading function: dependencies from automation_refinement_bert\\master_list\\dependencies.py\n",
            "Loading function: eos_attention from automation_refinement_bert\\master_list\\eos_attention.py\n",
            "Loading function: high_saliency_relationship_detection from automation_refinement_bert\\master_list\\high_saliency_relationship_detection.py\n",
            "Loading function: last_token_attention from automation_refinement_bert\\master_list\\last_token_attention.py\n",
            "Loading function: next_attention from automation_refinement_bert\\master_list\\next_attention.py\n",
            "Loading function: parenthetical_attention from automation_refinement_bert\\master_list\\parenthetical_attention.py\n",
            "Loading function: parenthetical_insertion from automation_refinement_bert\\master_list\\parenthetical_insertion.py\n",
            "Loading function: pos_alignment from automation_refinement_bert\\master_list\\pos_alignment.py\n",
            "Loading function: previous_attention from automation_refinement_bert\\master_list\\previous_attention.py\n",
            "Loading function: punctuation_attention from automation_refinement_bert\\master_list\\punctuation_attention.py\n",
            "Loading function: quotation_association from automation_refinement_bert\\master_list\\quotation_association.py\n",
            "Loading function: relative_position_attention from automation_refinement_bert\\master_list\\relative_position_attention.py\n",
            "Loading function: repeated_attention from automation_refinement_bert\\master_list\\repeated_attention.py\n",
            "Loading function: same_attention from automation_refinement_bert\\master_list\\same_attention.py\n",
            "Loading function: special_token_attention from automation_refinement_bert\\master_list\\special_token_attention.py\n",
            "Loading function: uniform_attention from automation_refinement_bert\\master_list\\uniform_attention.py\n",
            "Loading function: adverbial_modulation from automation_refinement_gpt2\\master_list\\adverbial_modulation.py\n",
            "Loading function: appositive_phrase_attention from automation_refinement_gpt2\\master_list\\appositive_phrase_attention.py\n",
            "Loading function: cls_attention from automation_refinement_gpt2\\master_list\\cls_attention.py\n",
            "Loading function: compound_element_association from automation_refinement_gpt2\\master_list\\compound_element_association.py\n",
            "Loading function: compound_modifier_attention from automation_refinement_gpt2\\master_list\\compound_modifier_attention.py\n",
            "Loading function: compound_word_attention_pattern from automation_refinement_gpt2\\master_list\\compound_word_attention_pattern.py\n",
            "Loading function: conjunction_based_grouping from automation_refinement_gpt2\\master_list\\conjunction_based_grouping.py\n",
            "Loading function: contextual_anchoring from automation_refinement_gpt2\\master_list\\contextual_anchoring.py\n",
            "Loading function: first_token_domination from automation_refinement_gpt2\\master_list\\first_token_domination.py\n",
            "Loading function: high_saliency_relationship_detection from automation_refinement_gpt2\\master_list\\high_saliency_relationship_detection.py\n",
            "Loading function: initial_contextual_attention from automation_refinement_gpt2\\master_list\\initial_contextual_attention.py\n",
            "Loading function: initial_element_reinforcement from automation_refinement_gpt2\\master_list\\initial_element_reinforcement.py\n",
            "Loading function: initial_phrase_contextualization from automation_refinement_gpt2\\master_list\\initial_phrase_contextualization.py\n",
            "Loading function: initial_phrase_dominance from automation_refinement_gpt2\\master_list\\initial_phrase_dominance.py\n",
            "Error loading function initial_phrase_dominance from automation_refinement_gpt2\\master_list\\initial_phrase_dominance.py: 'initial_phrase_dominance'\n",
            "Loading function: initial_token_anchoring from automation_refinement_gpt2\\master_list\\initial_token_anchoring.py\n",
            "Loading function: initial_token_centralization from automation_refinement_gpt2\\master_list\\initial_token_centralization.py\n",
            "Loading function: initial_token_dominance from automation_refinement_gpt2\\master_list\\initial_token_dominance.py\n",
            "Loading function: initial_token_reference_attention from automation_refinement_gpt2\\master_list\\initial_token_reference_attention.py\n",
            "Error loading function initial_token_reference_attention from automation_refinement_gpt2\\master_list\\initial_token_reference_attention.py: 'initial_token_reference_attention'\n",
            "Loading function: initial_word_attention from automation_refinement_gpt2\\master_list\\initial_word_attention.py\n",
            "Loading function: lexical_diversity_focus from automation_refinement_gpt2\\master_list\\lexical_diversity_focus.py\n",
            "Loading function: main_subject_attention from automation_refinement_gpt2\\master_list\\main_subject_attention.py\n",
            "Error loading function main_subject_attention from automation_refinement_gpt2\\master_list\\main_subject_attention.py: 'main_subject_attention'\n",
            "Loading function: parenthetical_attention from automation_refinement_gpt2\\master_list\\parenthetical_attention.py\n",
            "Loading function: parenthetical_insertion from automation_refinement_gpt2\\master_list\\parenthetical_insertion.py\n",
            "Loading function: pos_alignment from automation_refinement_gpt2\\master_list\\pos_alignment.py\n",
            "Loading function: previous_attention from automation_refinement_gpt2\\master_list\\previous_attention.py\n",
            "Loading function: quotation_association from automation_refinement_gpt2\\master_list\\quotation_association.py\n",
            "Loading function: rare_word_dominance from automation_refinement_gpt2\\master_list\\rare_word_dominance.py\n",
            "Loading function: relative_position_attention from automation_refinement_gpt2\\master_list\\relative_position_attention.py\n",
            "Loading function: same_attention from automation_refinement_gpt2\\master_list\\same_attention.py\n",
            "Loading function: semantic_grouping from automation_refinement_gpt2\\master_list\\semantic_grouping.py\n",
            "Loading function: sentence_beginning_attention_pattern from automation_refinement_gpt2\\master_list\\sentence_beginning_attention_pattern.py\n",
            "Loading function: sentence_beginning_salience from automation_refinement_gpt2\\master_list\\sentence_beginning_salience.py\n",
            "Loading function: sentence_boundary_focus from automation_refinement_gpt2\\master_list\\sentence_boundary_focus.py\n",
            "Error loading function sentence_boundary_focus from automation_refinement_gpt2\\master_list\\sentence_boundary_focus.py: 'sentence_boundary_focus'\n",
            "Loading function: sentence_level_attention from automation_refinement_gpt2\\master_list\\sentence_level_attention.py\n",
            "Loading function: sentence_opening_salience from automation_refinement_gpt2\\master_list\\sentence_opening_salience.py\n",
            "Loading function: sentence_position_preference from automation_refinement_gpt2\\master_list\\sentence_position_preference.py\n",
            "Loading function: sentence_start_attention from automation_refinement_gpt2\\master_list\\sentence_start_attention.py\n",
            "Error loading function sentence_start_attention from automation_refinement_gpt2\\master_list\\sentence_start_attention.py: 'sentence_start_attention'\n",
            "Loading function: special_token_attention from automation_refinement_gpt2\\master_list\\special_token_attention.py\n",
            "Loading function: token_reinforcement from automation_refinement_gpt2\\master_list\\token_reinforcement.py\n",
            "Error loading function token_reinforcement from automation_refinement_gpt2\\master_list\\token_reinforcement.py: 'token_reinforcement'\n",
            "Total executables loaded: 69\n",
            "['previous_attention', 'previous_attention', 'sentence_position_preference', 'compound_modifier_attention', 'parenthetical_insertion', 'appositive_phrase_attention', 'semantic_grouping', 'previous_attention', 'sentence_beginning_attention_pattern', 'punctuation_attention', 'dependencies', 'initial_token_dominance', 'sentence_opening_salience', 'compound_element_association', 'quotation_association', 'relative_position_attention', 'initial_word_attention', 'adverbial_modulation', 'compound_modifier_attention', 'pos_alignment', 'first_token_domination', 'cls_attention', 'compound_word_attention_pattern', 'high_saliency_relationship_detection', 'punctuation_attention', 'initial_element_reinforcement', 'same_attention', 'compound_word_attention_pattern', 'repeated_attention', 'compound_word_attention_pattern', 'rare_word_dominance', 'initial_contextual_attention', 'eos_attention', 'next_attention', 'pos_alignment', 'last_token_attention', 'sentence_level_attention', 'compound_element_association', 'initial_token_anchoring', 'repeated_attention', 'parenthetical_attention', 'quotation_association', 'uniform_attention', 'appositive_phrase_attention', 'special_token_attention', 'contextual_anchoring', 'dependencies', 'quotation_association', 'compound_modifier_attention', 'relative_position_attention', 'parenthetical_attention', 'special_token_attention', 'conjunction_based_grouping', 'sentence_beginning_salience', 'adverbial_modulation', 'compound_element_association', 'conjunction_based_grouping', 'high_saliency_relationship_detection', 'appositive_phrase_attention', 'pos_alignment', 'parenthetical_insertion', 'initial_token_centralization', 'lexical_diversity_focus', 'initial_phrase_contextualization', 'parenthetical_attention', 'next_attention', 'same_attention', 'parenthetical_insertion', 'same_attention']\n"
          ]
        }
      ],
      "source": [
        "folders = [\"automation_refinement\", \"automation_refinement_bert\", \"automation_refinement_gpt2\"]\n",
        "executables = []\n",
        "for folder in folders:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    # loop through master_list folder, read each python file, use make_executable to define function and add to executables list\n",
        "    master_list_folder = os.path.join(folder, \"master_list\")\n",
        "    for filename in os.listdir(master_list_folder):\n",
        "        if filename.endswith(\".py\"):\n",
        "            file_path = os.path.join(master_list_folder, filename)\n",
        "            with open(file_path, \"r\") as f:\n",
        "                func_name = filename[:-3]  # remove .py extension\n",
        "                if func_name in function_names:\n",
        "                    try:\n",
        "                        print(f\"Loading function: {func_name} from {file_path}\")\n",
        "                        code = f.read()\n",
        "                        exec(code, globals())\n",
        "                        executables.append(globals()[func_name])\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading function {func_name} from {file_path}: {e}\")\n",
        "                        continue\n",
        "\n",
        "executables = list(set(executables))  # remove duplicates\n",
        "print(f\"Total executables loaded: {len(executables)}\")\n",
        "print([exec.__name__ for exec in executables])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "id": "2870d8c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "programs = executables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c820d5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from programs import *\n",
        "\n",
        "# programs = [next_attention, previous_attention, same_attention, punctuation_attention, dependencies,\n",
        "#             last_token_attention, uniform_attention, cls_attention, eos_attention, pos_alignment,\n",
        "#             special_token_attention, repeated_attention, noun_modifier_attention, pronoun_attention,\n",
        "#             single_token_attention, root_cluster_attention]\n",
        "\n",
        "sentence_data = sentences[:25]\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def program_similarity(att_one, att_two):\n",
        "    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "        p = np.clip(p, 1e-12, 1.0)\n",
        "        q = np.clip(q, 1e-12, 1.0)\n",
        "        p /= p.sum()\n",
        "        q /= q.sum()\n",
        "        m = 0.5 * (p + q)\n",
        "        return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
        "\n",
        "    jensonshannon_distances = []\n",
        "    for row_att, row_out in zip(att_one, att_two):\n",
        "        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "    score = np.mean(jensonshannon_distances)\n",
        "    return score\n",
        "\n",
        "x = len(programs)\n",
        "S = np.zeros((x, x))\n",
        "for i in range(x):\n",
        "    print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
        "    for j in range(x):\n",
        "        if i < j:\n",
        "            similarities = []\n",
        "            program_one = programs[i]\n",
        "            program_two = programs[j]\n",
        "\n",
        "            for sentence in sentence_data:\n",
        "                h1, activations_one = program_one(sentence, tokenizer)\n",
        "                h2, activations_two = program_two(sentence, tokenizer)\n",
        "                similarities.append(program_similarity(activations_one, activations_two))\n",
        "            \n",
        "            S[i, j] = np.mean(similarities)\n",
        "            S[j, i] = S[i, j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "id": "dc07b7e4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "144.0\n"
          ]
        }
      ],
      "source": [
        "S = np.load('data/similarity_matrix_auto.npy')\n",
        "print(np.sqrt(np.size(S)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "id": "7b1a3b62",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Group 1: ['previous_attention', 'previous_attention', 'sentence_position_preference', 'compound_modifier_attention', 'parenthetical_insertion', 'appositive_phrase_attention', 'semantic_grouping', 'previous_attention', 'sentence_beginning_attention_pattern', 'punctuation_attention', 'dependencies', 'initial_token_dominance', 'sentence_opening_salience', 'compound_element_association', 'quotation_association', 'relative_position_attention', 'initial_word_attention', 'adverbial_modulation', 'compound_modifier_attention', 'pos_alignment', 'first_token_domination', 'cls_attention', 'compound_word_attention_pattern', 'high_saliency_relationship_detection', 'punctuation_attention', 'initial_element_reinforcement', 'same_attention', 'compound_word_attention_pattern', 'repeated_attention', 'compound_word_attention_pattern', 'rare_word_dominance', 'initial_contextual_attention', 'eos_attention', 'next_attention', 'pos_alignment', 'last_token_attention', 'sentence_level_attention', 'compound_element_association', 'initial_token_anchoring', 'repeated_attention', 'parenthetical_attention', 'quotation_association', 'uniform_attention', 'appositive_phrase_attention', 'special_token_attention', 'contextual_anchoring', 'dependencies', 'quotation_association', 'compound_modifier_attention', 'relative_position_attention', 'parenthetical_attention', 'special_token_attention', 'conjunction_based_grouping', 'sentence_beginning_salience', 'adverbial_modulation', 'compound_element_association', 'conjunction_based_grouping', 'high_saliency_relationship_detection', 'appositive_phrase_attention', 'pos_alignment', 'parenthetical_insertion', 'initial_token_centralization', 'lexical_diversity_focus', 'initial_phrase_contextualization', 'parenthetical_attention', 'next_attention', 'same_attention', 'parenthetical_insertion', 'same_attention']\n"
          ]
        }
      ],
      "source": [
        "S = np.load('data/similarity_matrix_auto.npy')\n",
        "\n",
        "def group_similar_programs(programs, S, threshold=0.6):\n",
        "    groups, used = [], set()\n",
        "    for i in range(len(programs)):\n",
        "        if i in used: continue\n",
        "        group = [i]\n",
        "        used.add(i)\n",
        "        \n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for group_member in group:\n",
        "                for j in range(len(programs)):\n",
        "                    if j not in used and S[group_member, j] < threshold:\n",
        "                        group.append(j)\n",
        "                        used.add(j)\n",
        "                        changed = True\n",
        "        groups.append([programs[idx].__name__ for idx in group])\n",
        "    \n",
        "    return groups\n",
        "\n",
        "groups = group_similar_programs(programs, S, threshold=0.6)\n",
        "for i, group in enumerate(groups):\n",
        "    print(f\"Group {i+1}: {group}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4789011f",
      "metadata": {},
      "outputs": [],
      "source": [
        "folder = \"automation_results_bert\"\n",
        "programs = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        code_path = os.path.join(folder, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "        if os.path.exists(code_path):\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_head{head}\", code_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            try:\n",
        "                spec.loader.exec_module(module)\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        programs.append(attr)\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading program for Layer {layer}, Head {head}: {e}\")\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8f46ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8219481",
      "metadata": {},
      "outputs": [],
      "source": [
        "S = (S + S.T) / 2\n",
        "\n",
        "def group_similar_programs(programs, S, threshold=0.6):\n",
        "    groups, used = [], set()\n",
        "    for i in range(len(programs)):\n",
        "        if i in used: continue\n",
        "        group = [i]\n",
        "        used.add(i)\n",
        "        \n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for group_member in group:\n",
        "                for j in range(len(programs)):\n",
        "                    if j not in used and S[group_member, j] < threshold:\n",
        "                        group.append(j)\n",
        "                        used.add(j)\n",
        "                        changed = True\n",
        "        groups.append([programs[idx].__name__ for idx in group])\n",
        "    \n",
        "    return groups\n",
        "\n",
        "groups = group_similar_programs(programs, S, threshold=0.25)\n",
        "for i, group in enumerate(groups):\n",
        "    if len(group) >= 1:\n",
        "        print(f\"Group {i+1}: {group}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ed1f38",
      "metadata": {},
      "outputs": [],
      "source": [
        "for program in function_names:\n",
        "    print(program.__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "id": "5b464ed4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "name_to_idx = {fn.__name__: i for i, fn in enumerate(programs)}\n",
        "new_order = [name_to_idx[name] for group in groups for name in group]\n",
        "S_grouped = S[np.ix_(new_order, new_order)]\n",
        "# S_grouped = np.load(\"data/similarity_matrix_auto.npy\")\n",
        "colors = \"Blues_r\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
        "im2 = ax.imshow(S_grouped, cmap=colors, aspect='auto')\n",
        "# ax.set_axis_off()\n",
        "# ax.set_xticks(range(len(programs)))\n",
        "# ax.set_yticks(range(len(programs)))\n",
        "# ax.set_xticklabels([p.__name__ for p in programs], rotation=90)\n",
        "# ax.set_yticklabels([p.__name__ for p in programs])\n",
        "# axis off\n",
        "ax.set_axis_off()\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.title(\"Automated Programs | Similarity Matrix\\n\", weight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b0e864",
      "metadata": {},
      "outputs": [],
      "source": [
        "# automated_program_similarity_analysis\n",
        "\n",
        "import os\n",
        "import importlib.util\n",
        "import types\n",
        "# from AutoTokenizer import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "folder = \"automation_results_bert\"\n",
        "programs = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        code_path = os.path.join(folder, \"llm_code\", f\"programs-layer_{layer}\", f\"{head}_output.py\")\n",
        "        if os.path.exists(code_path):\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_head{head}\", code_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            try:\n",
        "                spec.loader.exec_module(module)\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        programs.append(attr)\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading program for Layer {layer}, Head {head}: {e}\")\n",
        "                continue\n",
        "\n",
        "# sentence_data = sentences[:10]\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# def program_similarity(att_one, att_two):\n",
        "#     def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "#         p = np.clip(p, 1e-12, 1.0)\n",
        "#         q = np.clip(q, 1e-12, 1.0)\n",
        "#         p /= p.sum()\n",
        "#         q /= q.sum()\n",
        "#         m = 0.5 * (p + q)\n",
        "#         return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
        "\n",
        "#     jensonshannon_distances = []\n",
        "#     for row_att, row_out in zip(att_one, att_two):\n",
        "#         jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "#     score = np.mean(jensonshannon_distances)\n",
        "#     return score\n",
        "\n",
        "# x = len(programs)\n",
        "# S = np.zeros((x, x))\n",
        "# for i in range(x):\n",
        "#     print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
        "#     for j in range(x):\n",
        "#         if j % 24 == 0: print(f\"  inner loop {j}/{x}\")\n",
        "#         if i != j:\n",
        "#             similarities = []\n",
        "#             program_one = programs[i]\n",
        "#             program_two = programs[j]\n",
        "\n",
        "#             for sentence in sentence_data:\n",
        "#                 h1, activations_one = program_one(sentence, tokenizer)\n",
        "#                 h2, activations_two = program_two(sentence, tokenizer)\n",
        "#                 similarities.append(program_similarity(activations_one, activations_two))\n",
        "            \n",
        "#             S[i, j] = np.mean(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66271c65",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Hypothesis Replacement Experiments\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ce003186",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from programs import *\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "programs = [next_attention, previous_attention, same_attention, punctuation_attention, dependencies,\n",
        "            last_token_attention, uniform_attention, cls_attention, eos_attention, pos_alignment,\n",
        "            special_token_attention, repeated_attention, noun_modifier_attention, pronoun_attention,\n",
        "            single_token_attention, root_cluster_attention]\n",
        "\n",
        "use = \"gpt-2\"\n",
        "\n",
        "if use == \"bert\":\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "    model_replace = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "elif use == \"gpt-2\":\n",
        "    from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    remove = [next_attention, dependencies, last_token_attention, eos_attention, pos_alignment, root_cluster_attention]\n",
        "    programs = [p for p in programs if p not in remove]\n",
        "    model_replace = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "model.eval()\n",
        "model_replace.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b458bef0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper functions for replacing heads in llms\n",
        "\n",
        "def has_hooks(model):\n",
        "    # sanity check to see if any hooks are registered\n",
        "    for module in model.modules():\n",
        "        if module._forward_hooks or module._backward_hooks or module._forward_pre_hooks:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def remove_all_hooks(model):\n",
        "    for module in model.modules():\n",
        "        module._forward_hooks = {}\n",
        "        module._backward_hooks = {}\n",
        "        module._forward_pre_hooks = {}\n",
        "    worked = has_hooks(model)\n",
        "    print(f\"All hooks removed: {not worked}\")\n",
        "    return model\n",
        "\n",
        "def get_hypothesis_head(layer, head, sentence, tokenizer):\n",
        "    hypothesis = best_fit_data.get((layer, head), (None, None))[0]\n",
        "    program_to_use = programs[[i for i, p in enumerate(executables) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "    # print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "    out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3d20fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_hypothesis_dictionary(model, tokenizer, sentences):\n",
        "    if use == \"gpt-2\": best_fit_data = pd.read_csv('data/best_fit_refinement_gpt2_new.csv') # i, j, pattern, score\n",
        "    else: best_fit_data = pd.read_csv('data/best_fit_refinement_bert.csv') # i, j, pattern, score\n",
        "    hypothesis_dict = {} # (layer, head) -> (hypothesis, score)\n",
        "    for row in best_fit_data.itertuples():\n",
        "        layer = row.i\n",
        "        head = row.j\n",
        "        hypothesis = row.Pattern\n",
        "        # hypothesis_dict[(layer, head)] = hypothesis\n",
        "        # attach hypothesis to dict if it's the lowest score for that head\n",
        "        if (layer, head) not in hypothesis_dict:\n",
        "            hypothesis_dict[(layer, head)] = (hypothesis, row.Score)\n",
        "        else:\n",
        "            if row.Score < hypothesis_dict[(layer, head)][1]:\n",
        "                hypothesis_dict[(layer, head)] = (hypothesis, row.Score)\n",
        "    \n",
        "    # collect hypothesis names set and define program_executables list\n",
        "    program_executables = []\n",
        "    hypothesis_names = set()\n",
        "    for key in hypothesis_dict: hypothesis_names.add(hypothesis_dict[key][0])\n",
        "\n",
        "    for name in hypothesis_names:\n",
        "        flag = False\n",
        "        for program in programs:\n",
        "            if program.__name__ == name:\n",
        "                program_executables.append(program)\n",
        "                flag = True\n",
        "                break\n",
        "        if not flag:\n",
        "            print(f\"Program {name} not found among available programs.\")\n",
        "\n",
        "    print(f\"\\nNumber of Heads with Hypotheses: {len(hypothesis_dict)}\")\n",
        "    print(f\"Percentage of Heads with Hypotheses: {len(hypothesis_dict) / 144 * 100:.2f}%\")\n",
        "    print(f\"Number of Program Executables: {len(program_executables)}\\n\")\n",
        "    return hypothesis_dict, program_executables\n",
        "\n",
        "best_fits, programs_ex = generate_hypothesis_dictionary(model, tokenizer, sentences)\n",
        "print([program.__name__ for program in programs_ex])\n",
        "best_fits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c70eca0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "top_percentage = 1.0\n",
        "best_fits_sorted = sorted(best_fits.items(), key=lambda x: x[1][1])\n",
        "best_fits_top = best_fits_sorted[:int(len(best_fits_sorted) * top_percentage)]\n",
        "names_programs = set([item[1][0] for item in best_fits_top])\n",
        "print(f\"Set of {len(names_programs)} programs: {names_programs}\")\n",
        "\n",
        "best_fits_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "3c148dc4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(best_fits_top)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "id": "30646c5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: One day, a little girl named Lily found a needle in her room.\n",
            "Masked:   one day , a little girl named lily found a needle in her room [MASK]\n",
            "\n",
            "Original: She knew it was difficult to play with it because it was sharp.\n",
            "Masked:   she knew it was difficult to play with it because [MASK] was sharp .\n",
            "\n",
            "Original: Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
            "Masked:   lily wanted to share the [MASK] with her mom , so she could sew a button on her shirt .\n",
            "\n",
            "Original: Lily went to her mom and said, \"Mom, I found this needle.\n",
            "Masked:   lily went to her mom and said , \" mom , [MASK] found this needle .\n",
            "\n",
            "Original: Can you share it with me and sew my shirt?\"\n",
            "Masked:   can you [MASK] it with me and sew my shirt ? \"\n",
            "\n",
            "Original: Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
            "Masked:   her mom smiled and said , \" yes , [MASK] , we can share the needle and fix your shirt . \"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if use == \"bert\":\n",
        "    def mask_sentence(sent, tokenizer):\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        if not tokens:\n",
        "            return sent\n",
        "        idx = np.random.randint(len(tokens))\n",
        "        true_token = tokens[idx]\n",
        "        tokens[idx] = tokenizer.mask_token\n",
        "        return tokenizer.convert_tokens_to_string(tokens), true_token\n",
        "\n",
        "    masked_sentences = []\n",
        "    true_tokens = []\n",
        "    for i, sent in enumerate(sentences):\n",
        "        masked_sent, true_token = mask_sentence(sent, tokenizer)\n",
        "        if i < 6: print(f\"Original: {sent}\\nMasked:   {masked_sent}\\n\")\n",
        "        masked_sentences.append(masked_sent)\n",
        "        true_tokens.append(true_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "95e0bd31",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'one two three four five'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cut_sentence_randomly(sent):\n",
        "    len_sent = len(sent.split())\n",
        "    word_idx = np.random.randint(3, len_sent - 1)\n",
        "    # get the first part of the sentence up to word_idx\n",
        "    first_part = \" \".join(sent.split()[:word_idx])\n",
        "    return first_part\n",
        "\n",
        "example_sentence = \"one two three four five six seven eight nine ten\"\n",
        "print(len(example_sentence.split()))\n",
        "print(np.random.randint(3, len(example_sentence.split()) - 1))\n",
        "cut_sentence_randomly(example_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5f99121f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 0 Perplexity: 28.04\n",
            "Sentence 1 Perplexity: 30.17\n",
            "Sentence 2 Perplexity: 37.61\n",
            "Sentence 3 Perplexity: 24.69\n",
            "Sentence 4 Perplexity: 90.55\n"
          ]
        }
      ],
      "source": [
        "def decoder_model_perplexity(sentence):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    return torch.exp(loss).item()\n",
        "\n",
        "for i, sent in enumerate(sentences[:5]):\n",
        "    ppl = decoder_model_perplexity(sent)\n",
        "    print(f\"Sentence {i} Perplexity: {ppl:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3f3567a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Analysis ---\n",
            "Input: One day, a little girl named Lily found a needle in her\n",
            "Token 0: Actual: 'day' | Model Predicted: 'of'\n",
            "Token 1: Actual: ',' | Model Predicted: ','\n",
            "Token 2: Actual: 'a' | Model Predicted: 'I'\n",
            "Token 3: Actual: 'little' | Model Predicted: 'group'\n",
            "Token 4: Actual: 'girl' | Model Predicted: 'girl'\n",
            "Token 5: Actual: 'named' | Model Predicted: 'named'\n",
            "Token 6: Actual: 'Lily' | Model Predicted: 'K'\n",
            "Token 7: Actual: 'found' | Model Predicted: 'was'\n",
            "Token 8: Actual: 'a' | Model Predicted: 'herself'\n",
            "Token 9: Actual: 'needle' | Model Predicted: 'book'\n",
            "Token 10: Actual: 'in' | Model Predicted: 'in'\n",
            "Token 11: Actual: 'her' | Model Predicted: 'her'\n",
            "Prediction for NEXT word: 'hair'\n",
            "Perplexity: 25.77\n",
            "\n",
            "--- Analysis ---\n",
            "Input: She knew it was difficult to play with it because it was\n",
            "Token 0: Actual: 'knew' | Model Predicted: ','\n",
            "Token 1: Actual: 'it' | Model Predicted: 'that'\n",
            "Token 2: Actual: 'was' | Model Predicted: 'was'\n",
            "Token 3: Actual: 'difficult' | Model Predicted: 'a'\n",
            "Token 4: Actual: 'to' | Model Predicted: 'to'\n",
            "Token 5: Actual: 'play' | Model Predicted: 'get'\n",
            "Token 6: Actual: 'with' | Model Predicted: 'the'\n",
            "Token 7: Actual: 'it' | Model Predicted: 'her'\n",
            "Token 8: Actual: 'because' | Model Predicted: ','\n",
            "Token 9: Actual: 'it' | Model Predicted: 'it'\n",
            "Token 10: Actual: 'was' | Model Predicted: 'was'\n",
            "Prediction for NEXT word: 'so'\n",
            "Perplexity: 22.78\n",
            "\n",
            "--- Analysis ---\n",
            "Input: Lily wanted to share the needle with her mom, so she could sew a button on her\n",
            "Token 0: Actual: 'ily' | Model Predicted: '.'\n",
            "Token 1: Actual: 'wanted' | Model Predicted: ','\n",
            "Token 2: Actual: 'to' | Model Predicted: 'to'\n",
            "Token 3: Actual: 'share' | Model Predicted: 'know'\n",
            "Token 4: Actual: 'the' | Model Predicted: 'her'\n",
            "Token 5: Actual: 'needle' | Model Predicted: 'story'\n",
            "Token 6: Actual: 'with' | Model Predicted: 'with'\n",
            "Token 7: Actual: 'her' | Model Predicted: 'her'\n",
            "Token 8: Actual: 'mom' | Model Predicted: 'boyfriend'\n",
            "Token 9: Actual: ',' | Model Predicted: ','\n",
            "Token 10: Actual: 'so' | Model Predicted: 'but'\n",
            "Token 11: Actual: 'she' | Model Predicted: 'she'\n",
            "Token 12: Actual: 'could' | Model Predicted: 'took'\n",
            "Token 13: Actual: 'sew' | Model Predicted: 'get'\n",
            "Token 14: Actual: 'a' | Model Predicted: 'it'\n",
            "Token 15: Actual: 'button' | Model Predicted: 'needle'\n",
            "Token 16: Actual: 'on' | Model Predicted: 'on'\n",
            "Token 17: Actual: 'her' | Model Predicted: 'it'\n",
            "Prediction for NEXT word: 'skirt'\n",
            "Perplexity: 43.76\n",
            "\n",
            "--- Analysis ---\n",
            "Input: Lily went to her mom and said, \"Mom, I found this\n",
            "Token 0: Actual: 'ily' | Model Predicted: '.'\n",
            "Token 1: Actual: 'went' | Model Predicted: ','\n",
            "Token 2: Actual: 'to' | Model Predicted: 'to'\n",
            "Token 3: Actual: 'her' | Model Predicted: 'the'\n",
            "Token 4: Actual: 'mom' | Model Predicted: 'room'\n",
            "Token 5: Actual: 'and' | Model Predicted: 'and'\n",
            "Token 6: Actual: 'said' | Model Predicted: 'told'\n",
            "Token 7: Actual: ',' | Model Predicted: ','\n",
            "Token 8: Actual: '\"' | Model Predicted: '\"'\n",
            "Token 9: Actual: 'Mom' | Model Predicted: 'Mom'\n",
            "Token 10: Actual: ',' | Model Predicted: ','\n",
            "Token 11: Actual: 'I' | Model Predicted: 'I'\n",
            "Token 12: Actual: 'found' | Model Predicted: ''m'\n",
            "Token 13: Actual: 'this' | Model Predicted: 'out'\n",
            "Prediction for NEXT word: 'book'\n",
            "Perplexity: 19.67\n",
            "\n",
            "--- Analysis ---\n",
            "Input: Can you share it with me and sew my\n",
            "Token 0: Actual: 'you' | Model Predicted: ''\n",
            "Token 1: Actual: 'share' | Model Predicted: 'imagine'\n",
            "Token 2: Actual: 'it' | Model Predicted: 'your'\n",
            "Token 3: Actual: 'with' | Model Predicted: 'with'\n",
            "Token 4: Actual: 'me' | Model Predicted: 'us'\n",
            "Token 5: Actual: 'and' | Model Predicted: '?'\n",
            "Token 6: Actual: 'sew' | Model Predicted: 'others'\n",
            "Token 7: Actual: 'my' | Model Predicted: 'it'\n",
            "Prediction for NEXT word: 'own'\n",
            "Perplexity: 107.38\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def prepare_sentence(sent, mode=\"random\"):\n",
        "    words = sent.split()\n",
        "    if len(words) <= 3:\n",
        "        return sent\n",
        "        \n",
        "    if mode == \"random\":\n",
        "        # Cut at a random spot\n",
        "        idx = np.random.randint(3, len(words))\n",
        "        return \" \".join(words[:idx])\n",
        "    elif mode == \"last_word\":\n",
        "        # Return the whole sentence except the very last word\n",
        "        return \" \".join(words[:-1])\n",
        "    else:\n",
        "        return sent\n",
        "\n",
        "def analyze_predictions(sentence_to_use):\n",
        "    inputs = tokenizer(sentence_to_use, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"][0]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        " \n",
        "    logits = outputs.logits[0] \n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    print(f\"--- Analysis ---\")\n",
        "    print(f\"Input: {sentence_to_use}\")\n",
        "   \n",
        "    for i in range(len(input_ids) - 1):\n",
        "        actual_next_token = tokenizer.decode(input_ids[i+1])\n",
        "        predicted_next_token = tokenizer.decode(predicted_ids[i])\n",
        "        \n",
        "        actual = actual_next_token.strip()\n",
        "        pred = predicted_next_token.strip()\n",
        "        \n",
        "        print(f\"Token {i}: Actual: '{actual}' | Model Predicted: '{pred}'\")\n",
        "\n",
        "    final_prediction = tokenizer.decode(predicted_ids[-1])\n",
        "    print(f\"Prediction for NEXT word: '{final_prediction.strip()}'\")\n",
        "    # print(f\"Perplexity: {decoder_model_perplexity(sentence_to_use):.2f}\")\n",
        "    print(f\"Perplexity: {torch.exp(outputs.loss).item():.2f}\\n\")\n",
        "\n",
        "for sent in sentences[:5]:\n",
        "    cut_sent = prepare_sentence(sent, mode=\"last_word\") # or \"random\"\n",
        "    analyze_predictions(cut_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c0b7eaea",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Can you share it with me and sew my shirt?\"'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "id": "4634e65d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 0:\tPerplexity(s): 1.010\tPerplexity(f): 19.637\n",
            "Sentence 50:\tPerplexity(s): 1.009\tPerplexity(f): 69.698\n"
          ]
        }
      ],
      "source": [
        "def single_perplexity(sentence, true_token):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    return torch.exp(-log_prob).item()\n",
        "\n",
        "def full_perplexity(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = torch.tensor([ids])\n",
        "    log_probs = []\n",
        "    for i in range(1, len(tokens)-1):  # consider all tokens except [CLS], [SEP]\n",
        "        masked = input_ids.clone()\n",
        "        masked[0, i] = tokenizer.mask_token_id\n",
        "        with torch.no_grad():\n",
        "            logits = model(masked).logits\n",
        "        prob = torch.log_softmax(logits[0, i], dim=-1)[input_ids[0, i]]\n",
        "        log_probs.append(prob)\n",
        "    return torch.exp(-torch.stack(log_probs).mean())\n",
        "\n",
        "average_ppl_single = []\n",
        "average_ppl_full = []\n",
        "for i, (sent, true_tok) in enumerate(zip(masked_sentences[:100], true_tokens[:100])):\n",
        "    ppl_s = single_perplexity(sent, true_tok)\n",
        "    ppl_f = full_perplexity(sent)\n",
        "    average_ppl_single.append(ppl_s)\n",
        "    average_ppl_full.append(ppl_f)\n",
        "    if i % 50 == 0: print(f\"Sentence {i}:\\tPerplexity(s): {ppl_s:.3f}\\tPerplexity(f): {ppl_f:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "id": "056ccb6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Means: Single Token Perplexity: 18322.788, Full Sentence Perplexity: 779.835\n",
            "Medians: Single Token Perplexity: 1.111, Full Sentence Perplexity: 109.221\n",
            "Stds: Single Token Perplexity: 138849.665, Full Sentence Perplexity: 3437.880\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nMeans: Single Token Perplexity: {np.mean(average_ppl_single):.3f}, Full Sentence Perplexity: {np.mean(average_ppl_full):.3f}\")\n",
        "print(f\"Medians: Single Token Perplexity: {np.median(average_ppl_single):.3f}, Full Sentence Perplexity: {np.median(average_ppl_full):.3f}\")\n",
        "print(f\"Stds: Single Token Perplexity: {np.std(average_ppl_single):.3f}, Full Sentence Perplexity: {np.std(average_ppl_full):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "d5120b87",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993ba5f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERT HOOKS\n",
        "\n",
        "def register_hypothesis_hooks(model, best_fits, tokenizer):\n",
        "    hooks = []\n",
        "    def replace_activation_with_hypothesis(layer, head, sentence, tokenizer):\n",
        "        hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "        program_to_use = programs_ex[[i for i, p in enumerate(programs_ex) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "        # print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "        out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "        return out\n",
        "\n",
        "    for (layer_idx, head_idx) in best_fits:\n",
        "        target_layer = model.bert.encoder.layer[layer_idx].attention.self\n",
        "\n",
        "        def make_hook(layer_i, head_i):\n",
        "            def hook_fn(module, input, output):\n",
        "                context_layer, attn_probs = output\n",
        "                print(context_layer.shape)\n",
        "                # B, S, H, D = context_layer.shape  # (batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "                # sentence = tokenizer.decode(input[0][0], skip_special_tokens=True)\n",
        "                token_ids = inputs[\"input_ids\"][0] \n",
        "                sentence = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
        "                out = replace_activation_with_hypothesis(layer_i, head_i, sentence, tokenizer)  # shape (S, S)\n",
        "\n",
        "                # project (S, S) → (B, S, D)\n",
        "                D = context_layer.size(-1)\n",
        "                rand_head = out.unsqueeze(0).repeat(B, 1, 1)[:, :, :D]\n",
        "                context_layer[:, :, head_i, :] = rand_head\n",
        "                return (context_layer, attn_probs)\n",
        "            return hook_fn\n",
        "\n",
        "        hook = target_layer.register_forward_hook(make_hook(layer_idx, head_idx))\n",
        "        hooks.append(hook)\n",
        "    return hooks\n",
        "\n",
        "hooks = register_hypothesis_hooks(\n",
        "    model_replace,\n",
        "    best_fits=best_fits,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "4ca3d9ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "function_initialization_attention\n",
            "identify_function_declarations\n",
            "function_boundary_detection\n",
            "function_call_contextualization\n",
            "function_structure_attention\n",
            "func_def_start_attention\n",
            "function_header_pattern\n",
            "function_declaration_attention\n",
            "header_token_emphasis\n",
            "local_context_attention\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "5c67f095",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Heads: [(0, 4), (2, 4), (3, 2), (3, 7), (3, 8), (3, 9), (4, 3), (4, 7)]\n"
          ]
        }
      ],
      "source": [
        "# check that all heads present in best_fits_data\n",
        "missing_heads = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        if (layer, head) not in best_fits:\n",
        "            missing_heads.append((layer, head))\n",
        "\n",
        "print(f\"Missing Heads: {missing_heads}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d324349",
      "metadata": {},
      "outputs": [],
      "source": [
        "best_fits_top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "1634bc02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[((1, 5), ('sentence_position_preference', 0.4012687165249795))]\n",
            "Program to use for Layer 1, Head 5: <function sentence_position_preference at 0x000001BC9B0E6340>\n"
          ]
        }
      ],
      "source": [
        "head = 5\n",
        "layer = 1\n",
        "\n",
        "# best_fits_top = [((layer, head), (hypothesis, score)), ...]\n",
        "\n",
        "# relevant_rows = best_fits_top[[best_fits_top[i][0][0] == layer and best_fits_top[i][0][1] == head for i in range(len(best_fits_top))]]\n",
        "relevant_row = [item for item in best_fits_top if item[0] == (layer, head)]\n",
        "program_name = relevant_row[0][1][0]\n",
        "index_program = [i for i, p in enumerate(executables) if p.__name__ == program_name][0]\n",
        "program_to_use = executables[index_program]\n",
        "print(relevant_row)\n",
        "print(f\"Program to use for Layer {layer}, Head {head}: {program_to_use}\"    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "4346f2a4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "any([p.__name__ == \"sentence_position_preference\" for p in executables])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "348e9940",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(4,11) in [item[0] for item in best_fits_top]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55d661d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#GPT-2 working\n",
        "\n",
        "for num_heads_to_replace in range(1, 11):\n",
        "    # remove hooks from model_replace\n",
        "    from collections import OrderedDict\n",
        "    def remove_all_hooks(model):\n",
        "        for module in model.modules():\n",
        "            # Use OrderedDict to keep PyTorch happy\n",
        "            module._forward_hooks = OrderedDict()\n",
        "            module._backward_hooks = OrderedDict()\n",
        "            module._forward_pre_hooks = OrderedDict()\n",
        "        return model\n",
        "    model_replace = remove_all_hooks(model_replace)\n",
        "\n",
        "    # best_fit_data = pd.read_csv('data/best_fit_refinement_gpt2_new.csv') # i, j, pattern, score\n",
        "    gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    def get_hypothesis_head(layer, head, sentence, tokenizer):\n",
        "        if (layer, head) not in [item[0] for item in best_fits_top]:\n",
        "            print(f\"No hypothesis for Layer {layer}, Head {head}\")\n",
        "            return None, None\n",
        "        \n",
        "        # relevant_rows = best_fit_data[(best_fit_data['i'] == layer) & (best_fit_data['j'] == head)]\n",
        "        # max_row = relevant_rows.loc[relevant_rows['Score'].idxmax()]\n",
        "        # hypothesis = max_row['Pattern']\n",
        "        # index_program = [p.__name__ == hypothesis for p in executables].index(True)\n",
        "        # program_to_use = executables[index_program]\n",
        "\n",
        "        relevant_row = [item for item in best_fits_top if item[0] == (layer, head)]\n",
        "        program_name = relevant_row[0][1][0]\n",
        "        index_program = [i for i, p in enumerate(executables) if p.__name__ == program_name][0]\n",
        "        program_to_use = executables[index_program]\n",
        "\n",
        "        name, out = program_to_use(sentence, gpt_tokenizer)  \n",
        "        return name, out \n",
        "\n",
        "    def register_hypothesis_hooks(model, best_fits, tokenizer):\n",
        "        hooks = []\n",
        "        \n",
        "        def replace_activation_with_hypothesis(layer, head, sentence, tokenizer):\n",
        "            hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "            # Assuming programs_ex is defined in your global scope\n",
        "            program_to_use = next((p for p in programs_ex if p.__name__ == hypothesis), None)\n",
        "            return program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "\n",
        "        for (layer_idx, head_idx) in best_fits:\n",
        "            # 1. Dynamically find the layer\n",
        "            if hasattr(model, \"transformer\"): # GPT-2 style\n",
        "                target_layer = model.transformer.h[layer_idx].attn\n",
        "                is_gpt = True\n",
        "            elif hasattr(model, \"bert\"): # BERT style\n",
        "                target_layer = model.bert.encoder.layer[layer_idx].attention.self\n",
        "                is_gpt = False\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model architecture\")\n",
        "\n",
        "            def make_hook(layer_i, head_i, is_gpt_model):\n",
        "                def hook_fn(module, input, output):\n",
        "                    # GPT-2 output is often (hidden_states, presents, attentions)\n",
        "                    # BERT output is (hidden_states, attentions)\n",
        "                    context_layer = output[0] \n",
        "                    \n",
        "                    # Input tokens are usually the first element of the input tuple\n",
        "                    # Handle potential padding/batching\n",
        "                    input_ids = input[0][0]\n",
        "                    sentence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "                    \n",
        "                    new_attn_map = replace_activation_with_hypothesis(layer_i, head_i, sentence, tokenizer)\n",
        "\n",
        "                    if new_attn_map is not None:\n",
        "                        # Logic to project your hypothesis back into the context_layer\n",
        "                        # Note: context_layer shape for GPT2 is (Batch, Seq, Hidden) \n",
        "                        # but internal heads are often reshaped. \n",
        "                        B, S, D = context_layer.shape\n",
        "                        num_heads = model.config.n_head if is_gpt_model else model.config.num_attention_heads\n",
        "                        head_dim = D // num_heads\n",
        "                        \n",
        "                        # Reshape to manipulate specific head: (B, S, N, H)\n",
        "                        context_layer = context_layer.view(B, S, num_heads, head_dim)\n",
        "                        \n",
        "                        # Your projection logic (simplified example)\n",
        "                        # Ensure new_attn_map matches dimensions or use it to weight Values\n",
        "                        # For now, following your pattern:\n",
        "                        rand_head = new_attn_map.unsqueeze(0).repeat(B, 1, 1)[:, :, :head_dim]\n",
        "                        context_layer[:, :, head_i, :] = rand_head\n",
        "                        \n",
        "                        # Reshape back to original\n",
        "                        context_layer = context_layer.view(B, S, D)\n",
        "\n",
        "                    # Return the modified tuple (must match original length)\n",
        "                    new_output = list(output)\n",
        "                    new_output[0] = context_layer\n",
        "                    return tuple(new_output)\n",
        "                    \n",
        "                return hook_fn\n",
        "\n",
        "            hook = target_layer.register_forward_hook(make_hook(layer_idx, head_idx, is_gpt))\n",
        "            hooks.append(hook)\n",
        "            \n",
        "        return hooks\n",
        "\n",
        "    def replace_attention_head_gpt2(module, input, output):\n",
        "        # GPT-2 hook 'output' is a tuple: (hidden_states, presents, optional_attentions)\n",
        "        context_layer = output[0]\n",
        "        \n",
        "        # We need to know how many heads to reshape. \n",
        "        # Since GPT2Attention doesn't always store n_head as a top-level attr, \n",
        "        # we can use the config or infer it.\n",
        "        batch, seq_len, hidden_dim = context_layer.shape\n",
        "        \n",
        "        # Standard GPT-2 Base has 12 heads. Use config if available:\n",
        "        # num_heads = model_replace.config.n_head \n",
        "        num_heads = 12 \n",
        "        head_dim = hidden_dim // num_heads\n",
        "        \n",
        "        # Reshape: (B, S, D) -> (B, S, N, H)\n",
        "        context_layer = context_layer.view(batch, seq_len, num_heads, head_dim)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3)\n",
        "\n",
        "        # Randomly zero out 2 heads\n",
        "        # random_heads = np.random.choice(range(num_heads), size=1, replace=False)\n",
        "        # random_heads = np.arange(num_heads_to_replace)\n",
        "        # eye_matrix = torch.zeros((seq_len, seq_len))\n",
        "        modified_context = context_layer.clone()\n",
        "\n",
        "        current_layer = module.layer_id\n",
        "        # current_head = None\n",
        "        # current_sentence = None\n",
        "        # hypothesis_matrix = get_hypothesis_head(current_layer, current_head, current_sentence, tokenizer)\n",
        "        current_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "        # print(f\"Replacing Heads {random_heads} in Layer {module.layer_id} for Sentence: {current_sentence}\")\n",
        "        for current_head in range(12):\n",
        "            specific_head = (current_layer, current_head)\n",
        "            replacing = [item for item in top_fitting_heads_to_replace if item[0] == specific_head]\n",
        "\n",
        "            if replacing:\n",
        "                hypothesis_name, hypothesis_matrix = get_hypothesis_head(current_layer, current_head, current_sentence, tokenizer)\n",
        "                # hypothesis_matrix = torch.tensor(hypothesis_matrix, dtype=torch.float32)\n",
        "\n",
        "                if hypothesis_matrix is None: continue\n",
        "\n",
        "                # seq_len from current_sentence\n",
        "                # seq_len = len(tokenizer.tokenize(current_sentence))\n",
        "                # hypothesis_matrix = torch.diag(torch.ones(seq_len))\n",
        "                hypothesis_matrix = torch.zeros((seq_len, seq_len))\n",
        "                # print(\"--------------------------------\")\n",
        "                # print(replacing)\n",
        "                # print(hypothesis_name)\n",
        "                # print(\"--------------------------------\")\n",
        "\n",
        "                context_layer[:, current_head, :, :] = torch.matmul(hypothesis_matrix, context_layer[:, current_head, :, :])\n",
        "\n",
        "        # Create a copy or modify in place (be careful with in-place in hooks)\n",
        "        modified_context = context_layer.clone()\n",
        "        # for i in random_heads:\n",
        "            # modified_context[:, :, i, :] = 0\n",
        "            # context_layer[:, i, :, :] = torch.matmul(hypothesis_matrix, context_layer[:, i, :, :])\n",
        "        \n",
        "        # Reshape back: (B, S, N, H) -> (B, S, D)\n",
        "        modified_context = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        modified_context = modified_context.view(batch, seq_len, hidden_dim)\n",
        "        \n",
        "        # Reconstruct the output tuple\n",
        "        new_output = list(output)\n",
        "        new_output[0] = modified_context\n",
        "        return tuple(new_output)\n",
        "    \n",
        "    top_fitting_heads_to_replace = best_fits_top[:(num_heads_to_replace*12)]\n",
        "\n",
        "    # Registration Loop for GPT-2\n",
        "    for layer_number, layer in enumerate(model_replace.transformer.h):\n",
        "        # Access the correct attribute: .attn instead of .attention.self\n",
        "        target_module = layer.attn \n",
        "        # Add metadata if you need it inside the hook\n",
        "        target_module.layer_id = layer_number\n",
        "        # Register the hook\n",
        "        target_module.register_forward_hook(replace_attention_head_gpt2)\n",
        "\n",
        "    for sent in tqdm(sentences[:100]):\n",
        "        regular_perplexities = []\n",
        "        replaced_perplexities = []\n",
        "\n",
        "        cut_sent = prepare_sentence(sent, mode=\"last_word\") # or \"random\"\n",
        "        inputs = tokenizer(cut_sent, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            if num_heads_to_replace == 1: outputs_regular = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            outputs_replaced = model_replace(**inputs, labels=inputs[\"input_ids\"])\n",
        "        ppl_regular = torch.exp(outputs_regular.loss).item()\n",
        "        ppl_replaced = torch.exp(outputs_replaced.loss).item()\n",
        "\n",
        "        display = False\n",
        "        if display:\n",
        "            print(f\"Sentence: {cut_sent}\")\n",
        "            print(f\"\\tRegular Model Perplexity: {ppl_regular:.2f}\")\n",
        "            print(f\"\\tReplaced Model Perplexity: {ppl_replaced:.2f}\\n\")\n",
        "        \n",
        "        regular_perplexities.append(ppl_regular)\n",
        "        replaced_perplexities.append(ppl_replaced)\n",
        "\n",
        "    increase_percentage = [(replaced - normal)/normal * 100 for replaced, normal in zip(replaced_perplexities, regular_perplexities)]\n",
        "    mean_increase = np.mean(increase_percentage)\n",
        "    print(f\"Mean Perplexity Increase after Head Replacement ({num_heads_to_replace}): {mean_increase:.0f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ab2465",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_heads_to_replace = 1\n",
        "top_fitting_heads_to_replace = best_fits_top[:(num_heads_to_replace*12)]\n",
        "\n",
        "def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    # p = np.clip(p, 1e-12, 1.0)\n",
        "    # q = np.clip(q, 1e-12, 1.0)\n",
        "    p /= p.sum()\n",
        "    q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
        "\n",
        "# model is gpt-2\n",
        "model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "for item in top_fitting_heads_to_replace:\n",
        "    # calculate difference between hypothesis similarity matrix and real similarity np_saved_matrix\n",
        "    difference = []\n",
        "\n",
        "    layer, head = item[0]\n",
        "    hypothesis_name = item[1][0]\n",
        "\n",
        "    for sentence in sentences[100]:\n",
        "        # manually get activation matrix at head\n",
        "        tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        # torch_model = model.eval()\n",
        "        att = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "        name, pred_att = get_hypothesis_head(layer, head, sentence, tokenizer)\n",
        "\n",
        "        jensonshannon_distances = []\n",
        "        for row_att, row_out in zip(att, pred_att):\n",
        "            jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "        score = np.mean(jensonshannon_distances)\n",
        "        difference.append(score)\n",
        "    \n",
        "    print(f\"Layer {layer}, Head {head}, Hypothesis: {hypothesis_name}, Avg JS Divergence: {np.mean(difference):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "80d403c4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.]], dtype=float32)"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "7a9f751e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(None, None, None, None, None, None, None, None, None, None, None, None)"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs.attentions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcc5f42d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1.]])"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 100%|██████████| 100/100 [00:12<00:00,  8.14it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (1): 2%\n",
        "# 100%|██████████| 100/100 [00:07<00:00, 12.55it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (2): 26%\n",
        "# 100%|██████████| 100/100 [00:06<00:00, 14.37it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (3): 38%\n",
        "# 100%|██████████| 100/100 [00:07<00:00, 13.12it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (4): 63%\n",
        "# 100%|██████████| 100/100 [00:07<00:00, 13.34it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (5): 88%\n",
        "# 100%|██████████| 100/100 [00:08<00:00, 11.88it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (6): 100%\n",
        "# 100%|██████████| 100/100 [00:07<00:00, 13.45it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (7): 123%\n",
        "# 100%|██████████| 100/100 [00:07<00:00, 12.80it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (8): 319%\n",
        "# 100%|██████████| 100/100 [00:08<00:00, 12.44it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (9): 389%\n",
        "# 100%|██████████| 100/100 [00:08<00:00, 12.14it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (10): 811%\n",
        "\n",
        "# 3%\n",
        "# Mean Perplexity Increase after Head Replacement (2): 16%\n",
        "# 100%|██████████| 1400/1400 [01:36<00:00, 14.51it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (3): 21%\n",
        "# 100%|██████████| 1400/1400 [01:42<00:00, 13.63it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (4): 37%\n",
        "#  30%|██▉       | 418/1400 [00:31<01:14, 13.17it/s]\n",
        "\n",
        "# 100%|██████████| 100/100 [00:12<00:00,  7.87it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (1): 6%\n",
        "# 100%|██████████| 100/100 [00:07<00:00, 13.34it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (2): 3216%\n",
        "# 100%|██████████| 100/100 [00:08<00:00, 12.28it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (3): 3462%\n",
        "# 100%|██████████| 100/100 [00:08<00:00, 11.31it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (4): 3703%\n",
        "# 100%|██████████| 100/100 [00:09<00:00, 11.09it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (5): 3832%\n",
        "# 100%|██████████| 100/100 [00:08<00:00, 11.35it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (6): 3952%\n",
        "# 100%|██████████| 100/100 [00:09<00:00, 10.14it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (7): 5380%\n",
        "# 100%|██████████| 100/100 [00:09<00:00, 10.75it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (8): 5545%\n",
        "# 100%|██████████| 100/100 [00:09<00:00, 10.06it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (9): 5758%\n",
        "# 100%|██████████| 100/100 [00:10<00:00,  9.77it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (10): 6046%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "dd7749f0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cls_attention'"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_fits_top[:(num_heads_to_replace*12)][0][1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "928ee41b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[((5, 1), ('cls_attention', 0.0307966296468495))]\n"
          ]
        }
      ],
      "source": [
        "num_heads_to_replace = 1\n",
        "top_fitting_heads_to_replace = best_fits_top[:(num_heads_to_replace*12)]\n",
        "# print(top_fitting_heads_to_replace)\n",
        "\n",
        "# (5, 1) in top_fitting_heads_to_replace\n",
        "# search for (5, 1) in nested top_fitting_heads_to_replace\n",
        "# e.g. [((5, 1), ('cls_attention', 0.0307966296468495)), ((4, 11), ('previous_attention', 0.0437074035846249)), ((7, 2), ('cls_attention', 0.0522305542941697)), ((6, 9), ('cls_attention', 0.067090146229909)), ((9, 9), ('sentence_beginning_salience', 0.0714835655208253)), ((7, 10), ('sentence_beginning_salience', 0.0830397222865371)), ((9, 6), ('sentence_beginning_salience', 0.0894682433938373)), ((10, 10), ('sentence_beginning_salience', 0.1012886946707291)), ((10, 8), ('sentence_beginning_salience', 0.1075558946310822)), ((9, 11), ('sentence_beginning_salience', 0.1077038776010236)), ((8, 1), ('sentence_beginning_salience', 0.1125680346921121)), ((10, 6), ('sentence_beginning_salience', 0.1145507290046831))]\n",
        "\n",
        "# found = False\n",
        "# for item in top_fitting_heads_to_replace:\n",
        "#     if item[0] == (5, 1):\n",
        "#         found = True\n",
        "#         print(item)\n",
        "#         break\n",
        "\n",
        "# list comprehension\n",
        "specific_head = (5, 1)\n",
        "found = [item for item in top_fitting_heads_to_replace if item[0] == specific_head]\n",
        "print(found)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70fa88db",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 100%|██████████| 10/10 [00:01<00:00,  6.85it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (1): 6%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 14.48it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (2): 16%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 15.13it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (3): 28%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 14.42it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (4): 35%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 14.82it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (5): 51%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 13.64it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (6): 68%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 12.02it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (7): 142%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 13.71it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (8): 247%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 18.09it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (9): 301%\n",
        "# 100%|██████████| 10/10 [00:00<00:00, 15.46it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (10): 341%\n",
        "\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  8.60it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (2): 9%\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  8.78it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (3): 5%\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  9.01it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (4): -4%\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  9.60it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (5): -5%\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  9.87it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (6): -10%\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  9.68it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (7): 62%\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  9.31it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (8): 147%\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  9.88it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (9): 192%\n",
        "# 100%|██████████| 50/50 [00:05<00:00,  9.90it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (10): 310%\n",
        "\n",
        "# 100%|██████████| 300/300 [00:28<00:00, 10.51it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (2): 18%\n",
        "# 100%|██████████| 300/300 [00:29<00:00, 10.29it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (3): 19%\n",
        "# 100%|██████████| 300/300 [00:28<00:00, 10.48it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (4): 34%\n",
        "# 100%|██████████| 300/300 [00:28<00:00, 10.49it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (5): 44%\n",
        "# 100%|██████████| 300/300 [00:28<00:00, 10.35it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (6): 50%\n",
        "# 100%|██████████| 300/300 [00:49<00:00,  6.09it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (7): 113%\n",
        "# 100%|██████████| 300/300 [00:49<00:00,  6.02it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (8): 298%\n",
        "# 100%|██████████| 300/300 [00:37<00:00,  7.93it/s]\n",
        "# Mean Perplexity Increase after Head Replacement (9): 537%\n",
        "# 100%|██████████| 300/300 [00:40<00:00,  7.49it/s]Mean Perplexity Increase after Head Replacement (10): 647%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "id": "e4fd68cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Perplexity Increase after Head Replacement: 0%\n",
            "Mean Perplexity Increase after Head Replacement: 0%\n",
            "Mean Perplexity Increase after Head Replacement: 0%\n",
            "Mean Perplexity Increase after Head Replacement: 0%\n",
            "Mean Perplexity Increase after Head Replacement: 0%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[499]\u001b[39m\u001b[32m, line 141\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    140\u001b[39m     outputs_regular = model(**inputs, labels=inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     outputs_replaced = \u001b[43mmodel_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m ppl_regular = torch.exp(outputs_regular.loss).item()\n\u001b[32m    143\u001b[39m ppl_replaced = torch.exp(outputs_replaced.loss).item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1070\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1050\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1066\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1067\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1068\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:927\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    925\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    940\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:414\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    412\u001b[39m residual = hidden_states\n\u001b[32m    413\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m attn_output, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[32m    425\u001b[39m hidden_states = attn_output + residual\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1879\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1876\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1878\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1881\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1882\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1883\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1827\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1824\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1825\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1827\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1828\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1829\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1830\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1831\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1832\u001b[39m     ):\n\u001b[32m   1833\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:314\u001b[39m, in \u001b[36mGPT2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m         value_states = value_states.view(shape_kv).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     query_states, key_states, value_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mc_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m.split(\u001b[38;5;28mself\u001b[39m.split_size, dim=\u001b[32m2\u001b[39m)\n\u001b[32m    315\u001b[39m     shape_kv = (*key_states.shape[:-\u001b[32m1\u001b[39m], -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    316\u001b[39m     key_states = key_states.view(shape_kv).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:122\u001b[39m, in \u001b[36mConv1D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    121\u001b[39m     size_out = x.size()[:-\u001b[32m1\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.nf,)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     x = x.view(size_out)\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# GPT-2\n",
        "\n",
        "saved_perplexities = [0]\n",
        "\n",
        "for num_heads_to_replace in range(2, 13):\n",
        "    # remove hooks from model_replace\n",
        "    from collections import OrderedDict\n",
        "    def remove_all_hooks(model):\n",
        "        for module in model.modules():\n",
        "            # Use OrderedDict to keep PyTorch happy\n",
        "            module._forward_hooks = OrderedDict()\n",
        "            module._backward_hooks = OrderedDict()\n",
        "            module._forward_pre_hooks = OrderedDict()\n",
        "        return model\n",
        "    model_replace = remove_all_hooks(model_replace)\n",
        "    reset_hooks(model_replace)\n",
        "\n",
        "    def register_hypothesis_hooks(model, best_fits, tokenizer):\n",
        "        hooks = []\n",
        "        \n",
        "        def replace_activation_with_hypothesis(layer, head, sentence, tokenizer):\n",
        "            hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "            # Assuming programs_ex is defined in your global scope\n",
        "            program_to_use = next((p for p in programs_ex if p.__name__ == hypothesis), None)\n",
        "            return program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "\n",
        "        for (layer_idx, head_idx) in best_fits:\n",
        "            # 1. Dynamically find the layer\n",
        "            if hasattr(model, \"transformer\"): # GPT-2 style\n",
        "                target_layer = model.transformer.h[layer_idx].attn\n",
        "                is_gpt = True\n",
        "            elif hasattr(model, \"bert\"): # BERT style\n",
        "                target_layer = model.bert.encoder.layer[layer_idx].attention.self\n",
        "                is_gpt = False\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported model architecture\")\n",
        "\n",
        "            def make_hook(layer_i, head_i, is_gpt_model):\n",
        "                def hook_fn(module, input, output):\n",
        "                    # GPT-2 output is often (hidden_states, presents, attentions)\n",
        "                    # BERT output is (hidden_states, attentions)\n",
        "                    context_layer = output[0] \n",
        "                    \n",
        "                    # Input tokens are usually the first element of the input tuple\n",
        "                    # Handle potential padding/batching\n",
        "                    input_ids = input[0][0]\n",
        "                    sentence = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "                    \n",
        "                    new_attn_map = replace_activation_with_hypothesis(layer_i, head_i, sentence, tokenizer)\n",
        "\n",
        "                    if new_attn_map is not None:\n",
        "                        # Logic to project your hypothesis back into the context_layer\n",
        "                        # Note: context_layer shape for GPT2 is (Batch, Seq, Hidden) \n",
        "                        # but internal heads are often reshaped. \n",
        "                        B, S, D = context_layer.shape\n",
        "                        num_heads = model.config.n_head if is_gpt_model else model.config.num_attention_heads\n",
        "                        head_dim = D // num_heads\n",
        "                        \n",
        "                        # Reshape to manipulate specific head: (B, S, N, H)\n",
        "                        context_layer = context_layer.view(B, S, num_heads, head_dim)\n",
        "                        \n",
        "                        # Your projection logic (simplified example)\n",
        "                        # Ensure new_attn_map matches dimensions or use it to weight Values\n",
        "                        # For now, following your pattern:\n",
        "                        rand_head = new_attn_map.unsqueeze(0).repeat(B, 1, 1)[:, :, :head_dim]\n",
        "                        context_layer[:, :, head_i, :] = rand_head\n",
        "                        \n",
        "                        # Reshape back to original\n",
        "                        context_layer = context_layer.view(B, S, D)\n",
        "\n",
        "                    # Return the modified tuple (must match original length)\n",
        "                    new_output = list(output)\n",
        "                    new_output[0] = context_layer\n",
        "                    return tuple(new_output)\n",
        "                    \n",
        "                return hook_fn\n",
        "\n",
        "            hook = target_layer.register_forward_hook(make_hook(layer_idx, head_idx, is_gpt))\n",
        "            hooks.append(hook)\n",
        "            \n",
        "        return hooks\n",
        "\n",
        "    def replace_attention_head_gpt2(module, input, output):\n",
        "        # GPT-2 hook 'output' is a tuple: (hidden_states, presents, optional_attentions)\n",
        "        context_layer = output[0]\n",
        "        \n",
        "        # We need to know how many heads to reshape. \n",
        "        # Since GPT2Attention doesn't always store n_head as a top-level attr, \n",
        "        # we can use the config or infer it.\n",
        "        batch, seq_len, hidden_dim = context_layer.shape\n",
        "        \n",
        "        # Standard GPT-2 Base has 12 heads. Use config if available:\n",
        "        # num_heads = model_replace.config.n_head \n",
        "        num_heads = 12 \n",
        "        head_dim = hidden_dim // num_heads\n",
        "        \n",
        "        # Reshape: (B, S, D) -> (B, S, N, H)\n",
        "        context_layer = context_layer.view(batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "        # Randomly zero out 2 heads\n",
        "        # random_heads = np.random.choice(range(num_heads), size=num_heads_to_replace, replace=False)\n",
        "        random_heads = np.arange(num_heads_to_replace)\n",
        "        \n",
        "        # Create a copy or modify in place (be careful with in-place in hooks)\n",
        "        modified_context = context_layer.clone()\n",
        "        for head in random_heads:\n",
        "            activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "            # activation_hypothesis[0, 1:] = 1.0\n",
        "            np.fill_diagonal(activation_hypothesis, 1.0)\n",
        "            broadcasted_hypothesis = torch.matmul(\n",
        "                torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "                context_layer[:, :, head, :]\n",
        "            )\n",
        "            modified_context[:, :, head, :] = broadcasted_hypothesis\n",
        "        \n",
        "        # Reshape back: (B, S, N, H) -> (B, S, D)\n",
        "        modified_context = modified_context.view(batch, seq_len, hidden_dim)\n",
        "        \n",
        "        # Reconstruct the output tuple\n",
        "        new_output = list(output)\n",
        "        new_output[0] = modified_context\n",
        "        return tuple(new_output)\n",
        "\n",
        "    # Registration Loop for GPT-2\n",
        "    for layer_number, layer in enumerate(model_replace.transformer.h):\n",
        "        # Access the correct attribute: .attn instead of .attention.self\n",
        "        target_module = layer.attn \n",
        "        # Add metadata if you need it inside the hook\n",
        "        target_module.layer_id = layer_number\n",
        "        # Register the hook\n",
        "        target_module.register_forward_hook(replace_attention_head_gpt2)\n",
        "\n",
        "    for sent in sentences[:20]:\n",
        "        regular_perplexities = []\n",
        "        replaced_perplexities = []\n",
        "\n",
        "        cut_sent = prepare_sentence(sent, mode=\"last_word\") # or \"random\"\n",
        "        inputs = tokenizer(cut_sent, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs_regular = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            outputs_replaced = model_replace(**inputs, labels=inputs[\"input_ids\"])\n",
        "        ppl_regular = torch.exp(outputs_regular.loss).item()\n",
        "        ppl_replaced = torch.exp(outputs_replaced.loss).item()\n",
        "\n",
        "        # print(f\"Sentence: {cut_sent}\")\n",
        "        # print(f\"\\tRegular Model Perplexity: {ppl_regular:.2f}\")\n",
        "        # print(f\"\\tReplaced Model Perplexity: {ppl_replaced:.2f}\\n\")\n",
        "        regular_perplexities.append(ppl_regular)\n",
        "        replaced_perplexities.append(ppl_replaced)\n",
        "\n",
        "    increase_percentage = [(replaced - normal)/normal * 100 for replaced, normal in zip(replaced_perplexities, regular_perplexities)]\n",
        "    mean_increase = np.mean(increase_percentage)\n",
        "    print(f\"Mean Perplexity Increase after Head Replacement: {mean_increase:.0f}%\")\n",
        "\n",
        "    saved_perplexities.append(mean_increase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "id": "f6e9c173",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " np.float64(13.468331430476793),\n",
              " np.float64(12121.705154301768),\n",
              " np.float64(956.3888354127499),\n",
              " np.float64(6819.311270452256),\n",
              " np.float64(5045.876015476382),\n",
              " np.float64(1623.6336675569014),\n",
              " np.float64(2139.4133751034897),\n",
              " np.float64(1219.3935297040264),\n",
              " np.float64(1419.6021359480858),\n",
              " np.float64(1182.8767641160343),\n",
              " np.float64(1789.8429820919291)]"
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "saved_perplexities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "2fda9061",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "seq_len = 10\n",
        "activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "np.fill_diagonal(activation_hypothesis, 1.0)\n",
        "print(activation_hypothesis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "9d88ac76",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[17.0, 23.0, 35.0, 28.0, 24.0, 23.0, 142.0, 105.0, 257.0, 2043.0, 2030.0]"
            ]
          },
          "execution_count": 191,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"\"\"Mean Perplexity Increase after Head Replacement: 17%\n",
        "Mean Perplexity Increase after Head Replacement: 23%\n",
        "Mean Perplexity Increase after Head Replacement: 35%\n",
        "Mean Perplexity Increase after Head Replacement: 28%\n",
        "Mean Perplexity Increase after Head Replacement: 24%\n",
        "Mean Perplexity Increase after Head Replacement: 23%\n",
        "Mean Perplexity Increase after Head Replacement: 142%\n",
        "Mean Perplexity Increase after Head Replacement: 105%\n",
        "Mean Perplexity Increase after Head Replacement: 257%\n",
        "Mean Perplexity Increase after Head Replacement: 2043%\n",
        "Mean Perplexity Increase after Head Replacement: 2030%\"\"\"\n",
        "\n",
        "list_ = [float(line.split(\": \")[-1].strip(\"%\")) for line in text.strip().split(\"\\n\")]\n",
        "list_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "86c33f43",
      "metadata": {},
      "outputs": [],
      "source": [
        "del list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 470,
      "id": "67bb1e07",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 17, 768])\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[470]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo [MASK] token found in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     logits = \u001b[43mmodel_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m.logits  \u001b[38;5;66;03m# shape: [1, seq_len, vocab_size]\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# calculate full psuedo-perplexity\u001b[39;00m\n\u001b[32m    179\u001b[39m mask_pos = mask_token_index[\u001b[32m1\u001b[39m].item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1279\u001b[39m, in \u001b[36mBertForMaskedLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1270\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1272\u001b[39m \u001b[33;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[32m   1273\u001b[39m \u001b[33;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[32m   1274\u001b[39m \u001b[33;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1275\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1277\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1293\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1294\u001b[39m prediction_scores = \u001b[38;5;28mself\u001b[39m.cls(sequence_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:999\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    996\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    997\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1012\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1013\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:649\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    645\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    647\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:557\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    547\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    555\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    556\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    566\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:487\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    478\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    485\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    486\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    497\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1879\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1876\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1878\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1881\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1882\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1883\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1840\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1838\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1839\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1843\u001b[39m     result = hook_result\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[461]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mregister_hypothesis_hooks.<locals>.make_hook.<locals>.hook_fn\u001b[39m\u001b[34m(module, input, output)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(context_layer.shape)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# B, S, H, D = context_layer.shape  # (batch, seq_len, num_heads, head_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m sentence = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m out = replace_activation_with_hypothesis(layer_i, head_i, sentence, tokenizer)  \u001b[38;5;66;03m# shape (S, S)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# project (S, S) → (B, S, D)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3897\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3894\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3895\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3897\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:1092\u001b[39m, in \u001b[36mPreTrainedTokenizer._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decode\u001b[39m(\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1084\u001b[39m     token_ids: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m     **kwargs,\n\u001b[32m   1089\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28mself\u001b[39m._decode_use_source_tokenizer = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33muse_source_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1092\u001b[39m     filtered_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;66;03m# If given is a single id, prevents splitting the string in upcoming loop\u001b[39;00m\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filtered_tokens, \u001b[38;5;28mstr\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:1067\u001b[39m, in \u001b[36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[39m\u001b[34m(self, ids, skip_special_tokens)\u001b[39m\n\u001b[32m   1065\u001b[39m tokens = []\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m     index = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.all_special_ids:\n\u001b[32m   1069\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
          ]
        }
      ],
      "source": [
        "#BERT\n",
        "def replace_attention_head(module, input, output):\n",
        "    \"\"\" module: BertSelfAttention | output: tuple (context_layer, attention_probs)\"\"\"\n",
        "    context_layer, attn_probs = output\n",
        "    batch, seq_len, hidden_dim = context_layer.shape\n",
        "    num_heads = module.num_attention_heads\n",
        "    head_dim = hidden_dim // num_heads\n",
        "    context_layer = context_layer.view(batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "    # print current sentence\n",
        "    # print(f\"Current Sentence: {inputs['input_ids']}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # decoded_sentences = tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # for decoded_sentence in decoded_sentences:\n",
        "        # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "\n",
        "    # layer_id = getattr(module, \"layer_id\", None)\n",
        "    # heads = []\n",
        "    # for (layer, head), (fn_name, score) in best_fits_top:\n",
        "        # if layer == layer_id:\n",
        "            # print(layer, head, fn_name)\n",
        "            # heads.append(head)\n",
        "\n",
        "    # zero_out = 10 # percent of all heads\n",
        "    # random_heads = np.random.choice(range(num_heads), size=int(num_heads * zero_out / 100), replace=False)\n",
        "    # random number from 1 to 100 (2 random numbers in a list)\n",
        "    random_heads = np.random.choice(range(num_heads), size=1, replace=False)\n",
        "    print(len(random_heads))\n",
        "    for i in random_heads:\n",
        "        context_layer[:, :, i, :] = 0\n",
        "\n",
        "    # for head in heads:\n",
        "        # activation_hypothesis = np.eye(seq_len)\n",
        "        # _, activation_hypothesis = get_hypothesis_head(layer_id, head, decoded_sentence, tokenizer)\n",
        "        # assert activation_hypothesis.shape == (seq_len, seq_len), f\"Hypothesis shape {activation_hypothesis.shape} does not match expected {(seq_len, seq_len)} for '{decoded_sentence}'\"\n",
        "\n",
        "        # broadcasted_hypothesis = torch.matmul(\n",
        "        #     torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "        #     context_layer[:, :, head, :]\n",
        "        # )\n",
        "        # context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 0:\n",
        "    #     head = 6\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 4\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 0\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len)) \n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 1:\n",
        "    #     head = 6\n",
        "    #     # cls attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, 0] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 9\n",
        "    #     # relative position attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     for i in range(seq_len):\n",
        "    #         for j in range(seq_len):\n",
        "    #             if j > i:\n",
        "    #                 activation_hypothesis[i, j] = 1.0\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 2:\n",
        "    #     # next attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[0, 1:] = 1.0\n",
        "    #     # activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     head = 0\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 9\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "        \n",
        "    # if layer_id == 7:\n",
        "    #     head = 6\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 8:\n",
        "    #     head = 2\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # [((2, 0), ('next_attention', 0.0134178736883759)),\n",
        "    # ((2, 9), ('next_attention', 0.0195240472425213)),\n",
        "    # ((0, 0), ('uniform_attention', 0.1473364017258112)),\n",
        "    # ((1, 6), ('cls_attention', 0.1820107218156672)),\n",
        "    # ((8, 2), ('eos_attention', 0.198991498540351)),\n",
        "    # ((0, 4), ('uniform_attention', 0.2081918466789174)),\n",
        "    # ((1, 9), ('relative_position_attention', 0.2308144694156767)),\n",
        "    # ((7, 6), ('eos_attention', 0.2383215559745129)),\n",
        "    # ((0, 6), ('uniform_attention', 0.246593192497212))]\n",
        "\n",
        "    context_layer = context_layer.reshape(batch, seq_len, hidden_dim)\n",
        "    return (context_layer, attn_probs)\n",
        "\n",
        "# target_layer = model_replace.bert.encoder.layer[2].attention.self\n",
        "# hook = target_layer.register_forward_hook(replace_attention_head)\n",
        "\n",
        "for layer_number, layer in enumerate(model_replace.bert.encoder.layer):\n",
        "    layer.attention.self.layer_id = layer_number\n",
        "    if layer_number in [2]:\n",
        "        layer.attention.self.register_forward_hook(replace_attention_head)\n",
        "\n",
        "ppl_zero_50 = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:50], true_tokens[:50]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model_replace(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "\n",
        "    # calculate full psuedo-perplexity\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_zero_50.append(torch.exp(-log_prob).item())\n",
        "\n",
        "ppl_subset_normal = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:50], true_tokens[:50]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_subset_normal.append(torch.exp(-log_prob).item())\n",
        "\n",
        "print('\\n')\n",
        "print('\\nWeird: ', np.median(ppl_subset_replaced), '\\t', np.mean(ppl_subset_replaced))\n",
        "print('Normal: ', np.median(ppl_subset_normal), '\\t', np.mean(ppl_subset_normal), '\\n')\n",
        "\n",
        "print(f\"Average Perplexity weird: pd-describe{(pd.Series(ppl_subset_replaced).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 477,
      "id": "75013d61",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All hooks removed: True\n"
          ]
        }
      ],
      "source": [
        "model_replace = remove_all_hooks(model_replace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 488,
      "id": "ed451be9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 6, 10,  4,  1,  2,  0,  9,  3,  5, 11,  7])"
            ]
          },
          "execution_count": 488,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.random.choice(range(num_heads), size=num_heads_to_replace, replace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 489,
      "id": "674d0a67",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
            ]
          },
          "execution_count": 489,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# list numbers from 1 to 10 in np\n",
        "np.arange(1, 11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 490,
      "id": "849739cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1400/1400 [00:52<00:00, 26.49it/s]\n",
            "Evaluating: 100%|██████████| 1400/1400 [00:54<00:00, 25.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Perplexity Increase after Intervention, replacing 2: 10%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1400/1400 [02:34<00:00,  9.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Perplexity Increase after Intervention, replacing 3: 16%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1400/1400 [03:23<00:00,  6.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Perplexity Increase after Intervention, replacing 4: 37%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1400/1400 [02:02<00:00, 11.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Perplexity Increase after Intervention, replacing 5: 90%\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1400/1400 [03:34<00:00,  6.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Perplexity Increase after Intervention, replacing 6: 259%\n",
            "\n",
            "\n",
            "Saved Mean Increases over runs: [np.float64(10.295132158275358), np.float64(16.000128743908615), np.float64(36.63282992295086), np.float64(89.83470414090506), np.float64(259.1387416161393)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# GEMINI GAVE CODE \n",
        "\n",
        "saved_mean_increases = []\n",
        "output = False\n",
        "\n",
        "for num_heads_to_replace in range(2, 7):\n",
        "    import torch\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    # 1. Reset the model hooks entirely to fix the \"weak reference\" error\n",
        "    def reset_hooks(model):\n",
        "        for name, module in model.named_modules():\n",
        "            # Re-initialize the internal hook storage to a clean state\n",
        "            module._forward_hooks = OrderedDict()\n",
        "            module._forward_pre_hooks = OrderedDict()\n",
        "\n",
        "    reset_hooks(model_replace)\n",
        "\n",
        "    # Global variable for the hook\n",
        "    current_input_ids = None\n",
        "\n",
        "    def replace_attention_head(module, input, output):\n",
        "        \"\"\" module: BertSelfAttention | output: tuple (context_layer, attention_probs) \"\"\"\n",
        "        context_layer, attn_probs = output\n",
        "        batch, seq_len, hidden_dim = context_layer.shape\n",
        "        num_heads = module.num_attention_heads\n",
        "        head_dim = hidden_dim // num_heads\n",
        "        \n",
        "        # Crucial: Clone so we don't modify the original tensor in a way that breaks the graph\n",
        "        context_layer = context_layer.view(batch, seq_len, num_heads, head_dim).clone()\n",
        "\n",
        "        global current_input_ids\n",
        "        if current_input_ids is not None:\n",
        "            # Intervention: Zero out a random head\n",
        "            # random_heads = np.random.choice(range(num_heads), size=num_heads_to_replace, replace=False)\n",
        "            random_heads = np.arange(num_heads_to_replace)  # For testing, replace with first N heads\n",
        "            for i in random_heads:\n",
        "                context_layer[:, :, i, :] = 0.0\n",
        "\n",
        "        context_layer = context_layer.reshape(batch, seq_len, hidden_dim)\n",
        "        return (context_layer, attn_probs)\n",
        "\n",
        "    # 2. Register with a fresh handle\n",
        "    for layer_number, layer in enumerate(model_replace.bert.encoder.layer):\n",
        "        layer.attention.self.layer_id = layer_number\n",
        "        if layer_number in range(0, 12):  # All layers\n",
        "            # We store the handle, though for simple scripts it's optional\n",
        "            handle = layer.attention.self.register_forward_hook(replace_attention_head)\n",
        "\n",
        "    # 3. Evaluation Function\n",
        "    def get_perplexity(model_obj, data_list):\n",
        "        ppl_list = []\n",
        "        global current_input_ids\n",
        "        \n",
        "        model_obj.eval() # Ensure model is in eval mode\n",
        "        for sentence, true_token in tqdm(data_list, desc=\"Evaluating\"):\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\").to(model_obj.device)\n",
        "            current_input_ids = inputs[\"input_ids\"]\n",
        "            \n",
        "            mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "            if len(mask_token_index[0]) == 0: continue\n",
        "                \n",
        "            with torch.no_grad():\n",
        "                outputs = model_obj(**inputs)\n",
        "                logits = outputs.logits\n",
        "                \n",
        "            mask_pos = mask_token_index[1].item()\n",
        "            masked_logits = logits[0, mask_pos, :]\n",
        "            true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "            \n",
        "            log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "            log_prob = log_probs[true_id]\n",
        "            ppl_list.append(torch.exp(-log_prob).item())\n",
        "            \n",
        "        return ppl_list\n",
        "\n",
        "    # Prepare data\n",
        "    data_subset = list(zip(masked_sentences[:1400], true_tokens[:1400]))\n",
        "\n",
        "    if output: print(\"Evaluating Modified Model (Layer 2 head zeroed)...\")\n",
        "    ppl_subset_replaced = get_perplexity(model_replace, data_subset)\n",
        "\n",
        "    if output: print(\"Evaluating Original Model...\")\n",
        "    current_input_ids = None # Disable hook logic for original model\n",
        "    if num_heads_to_replace == 1:\n",
        "        ppl_subset_normal = get_perplexity(model, data_subset)\n",
        "\n",
        "    # --- Reporting ---\n",
        "    df_res = pd.DataFrame({\n",
        "        'Modified (Intervention)': pd.Series(ppl_subset_replaced).describe(),\n",
        "        'Normal (Baseline)': pd.Series(ppl_subset_normal).describe()\n",
        "    })\n",
        "    \n",
        "    if output:\n",
        "        print('\\n' + '='*40)\n",
        "        print('BERT PERPLEXITY COMPARISON')\n",
        "        print('='*40)\n",
        "        print(df_res)\n",
        "\n",
        "    increase_percentage = [(replaced - normal)/normal * 100 for replaced, normal in zip(ppl_subset_replaced, ppl_subset_normal)]\n",
        "    mean_increase = np.mean(increase_percentage)\n",
        "    print(f\"Mean Perplexity Increase after Intervention, replacing {num_heads_to_replace}: {mean_increase:.0f}%\\n\")\n",
        "    \n",
        "    saved_mean_increases.append(mean_increase)\n",
        "\n",
        "print(f\"\\nSaved Mean Increases over runs: {saved_mean_increases}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 487,
      "id": "3554d351",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[np.float64(0.0),\n",
              " np.float64(4.726916478445083),\n",
              " np.float64(13.940370124175839),\n",
              " np.float64(26.71263452309001),\n",
              " np.float64(417.1998578905501),\n",
              " np.float64(86.94786249264108),\n",
              " np.float64(141.44707167306677),\n",
              " np.float64(14231.671209202701),\n",
              " np.float64(3399241.4176415466),\n",
              " np.float64(7475839.041683524),\n",
              " np.float64(68337588.52521826)]"
            ]
          },
          "execution_count": 487,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "saved_mean_increases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "id": "e276add3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 17, 768])\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[466]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo [MASK] token found in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     logits = \u001b[43mmodel_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m.logits  \u001b[38;5;66;03m# shape: [1, seq_len, vocab_size]\u001b[39;00m\n\u001b[32m      9\u001b[39m mask_pos = mask_token_index[\u001b[32m1\u001b[39m].item()\n\u001b[32m     10\u001b[39m masked_logits = logits[\u001b[32m0\u001b[39m, mask_pos, :]  \u001b[38;5;66;03m# shape: [vocab_size]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1279\u001b[39m, in \u001b[36mBertForMaskedLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1270\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1272\u001b[39m \u001b[33;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[32m   1273\u001b[39m \u001b[33;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[32m   1274\u001b[39m \u001b[33;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1275\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1277\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1293\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1294\u001b[39m prediction_scores = \u001b[38;5;28mself\u001b[39m.cls(sequence_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:999\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    996\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    997\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1012\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1013\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:649\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    645\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    647\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:557\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    547\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    555\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    556\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    566\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:487\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    478\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    485\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    486\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    497\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1879\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1876\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1878\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1881\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1882\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1883\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1840\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1838\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1839\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1843\u001b[39m     result = hook_result\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[461]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mregister_hypothesis_hooks.<locals>.make_hook.<locals>.hook_fn\u001b[39m\u001b[34m(module, input, output)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(context_layer.shape)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# B, S, H, D = context_layer.shape  # (batch, seq_len, num_heads, head_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m sentence = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m out = replace_activation_with_hypothesis(layer_i, head_i, sentence, tokenizer)  \u001b[38;5;66;03m# shape (S, S)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# project (S, S) → (B, S, D)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3897\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3894\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3895\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3897\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:1092\u001b[39m, in \u001b[36mPreTrainedTokenizer._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decode\u001b[39m(\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1084\u001b[39m     token_ids: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m     **kwargs,\n\u001b[32m   1089\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28mself\u001b[39m._decode_use_source_tokenizer = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33muse_source_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1092\u001b[39m     filtered_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;66;03m# If given is a single id, prevents splitting the string in upcoming loop\u001b[39;00m\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filtered_tokens, \u001b[38;5;28mstr\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:1067\u001b[39m, in \u001b[36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[39m\u001b[34m(self, ids, skip_special_tokens)\u001b[39m\n\u001b[32m   1065\u001b[39m tokens = []\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m     index = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.all_special_ids:\n\u001b[32m   1069\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
          ]
        }
      ],
      "source": [
        "ppl_subset_replaced = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:100], true_tokens[:100]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model_replace(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_subset_replaced.append(torch.exp(-log_prob).item())\n",
        "\n",
        "ppl_subset_normal = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:100], true_tokens[:100]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_subset_normal.append(torch.exp(-log_prob).item())\n",
        "\n",
        "print(f\"Average Perplexity disabled: pd-describe{(pd.Series(ppl_subset_replaced).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a58192c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hypothesis_head(layer, head, sentence, tokenizer):\n",
        "    hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "    program_to_use = programs_ex[[i for i, p in enumerate(programs_ex) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "    # print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "    out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "    return out\n",
        "\n",
        "print(sentences[3])\n",
        "# tokenizer.decode(tokenizer(sentences[0])['input_ids'], skip_special_tokens=True)\n",
        "tokens = tokenizer.tokenize(sentences[0])\n",
        "input = tokenizer(sentences[0], return_tensors=\"pt\")\n",
        "print(len(input), input['input_ids'].shape[1])\n",
        "\n",
        "decoded = tokenizer.decode(input['input_ids'][0], skip_special_tokens=True)\n",
        "layer_id = 8\n",
        "_, activation_hypothesis = get_hypothesis_head(layer_id, head, decoded, tokenizer)\n",
        "print(activation_hypothesis.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "id": "232bd71d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 17, 768])\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[467]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo [MASK] token found in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     logits = \u001b[43mmodel_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m.logits  \u001b[38;5;66;03m# shape: [1, seq_len, vocab_size]\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# calculate full psuedo-perplexity\u001b[39;00m\n\u001b[32m    178\u001b[39m mask_pos = mask_token_index[\u001b[32m1\u001b[39m].item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1279\u001b[39m, in \u001b[36mBertForMaskedLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1270\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1271\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1272\u001b[39m \u001b[33;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[32m   1273\u001b[39m \u001b[33;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[32m   1274\u001b[39m \u001b[33;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1275\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1277\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1293\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1294\u001b[39m prediction_scores = \u001b[38;5;28mself\u001b[39m.cls(sequence_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:999\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    996\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    997\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1012\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1013\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:649\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    645\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    647\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:557\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    547\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    555\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    556\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    565\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    566\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:487\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    478\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    485\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    486\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    497\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1879\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1876\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1878\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1879\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1880\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1881\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1882\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1883\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1840\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1838\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[32m   1839\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m     hook_result = \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1843\u001b[39m     result = hook_result\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[461]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mregister_hypothesis_hooks.<locals>.make_hook.<locals>.hook_fn\u001b[39m\u001b[34m(module, input, output)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(context_layer.shape)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# B, S, H, D = context_layer.shape  # (batch, seq_len, num_heads, head_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m sentence = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m out = replace_activation_with_hypothesis(layer_i, head_i, sentence, tokenizer)  \u001b[38;5;66;03m# shape (S, S)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# project (S, S) → (B, S, D)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3897\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[39m\n\u001b[32m   3894\u001b[39m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[32m   3895\u001b[39m token_ids = to_py_obj(token_ids)\n\u001b[32m-> \u001b[39m\u001b[32m3897\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:1092\u001b[39m, in \u001b[36mPreTrainedTokenizer._decode\u001b[39m\u001b[34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   1082\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decode\u001b[39m(\n\u001b[32m   1083\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1084\u001b[39m     token_ids: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m]],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1088\u001b[39m     **kwargs,\n\u001b[32m   1089\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1090\u001b[39m     \u001b[38;5;28mself\u001b[39m._decode_use_source_tokenizer = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33muse_source_tokenizer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1092\u001b[39m     filtered_tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m     \u001b[38;5;66;03m# If given is a single id, prevents splitting the string in upcoming loop\u001b[39;00m\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filtered_tokens, \u001b[38;5;28mstr\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amkah\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils.py:1067\u001b[39m, in \u001b[36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[39m\u001b[34m(self, ids, skip_special_tokens)\u001b[39m\n\u001b[32m   1065\u001b[39m tokens = []\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m     index = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.all_special_ids:\n\u001b[32m   1069\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
          ]
        }
      ],
      "source": [
        "def replace_attention_head(module, input, output):\n",
        "    \"\"\" module: BertSelfAttention | output: tuple (context_layer, attention_probs)\"\"\"\n",
        "    context_layer, attn_probs = output\n",
        "    batch, seq_len, hidden_dim = context_layer.shape\n",
        "    num_heads = module.num_attention_heads\n",
        "    head_dim = hidden_dim // num_heads\n",
        "    context_layer = context_layer.view(batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "    # print current sentence\n",
        "    # print(f\"Current Sentence: {inputs['input_ids']}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # decoded_sentences = tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # for decoded_sentence in decoded_sentences:\n",
        "        # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "\n",
        "    # layer_id = getattr(module, \"layer_id\", None)\n",
        "    # heads = []\n",
        "    # for (layer, head), (fn_name, score) in best_fits_top:\n",
        "        # if layer == layer_id:\n",
        "            # print(layer, head, fn_name)\n",
        "            # heads.append(head)\n",
        "\n",
        "    # zero_out = 10 # percent of all heads\n",
        "    # random_heads = np.random.choice(range(num_heads), size=int(num_heads * zero_out / 100), replace=False)\n",
        "    # random number from 1 to 100 (2 random numbers in a list)\n",
        "    random_heads = np.random.choice(range(num_heads), size=6, replace=False)\n",
        "    print(len(random_heads))\n",
        "    for i in random_heads:\n",
        "        context_layer[:, :, i, :] = 0\n",
        "\n",
        "    # for head in heads:\n",
        "        # activation_hypothesis = np.eye(seq_len)\n",
        "        # _, activation_hypothesis = get_hypothesis_head(layer_id, head, decoded_sentence, tokenizer)\n",
        "        # assert activation_hypothesis.shape == (seq_len, seq_len), f\"Hypothesis shape {activation_hypothesis.shape} does not match expected {(seq_len, seq_len)} for '{decoded_sentence}'\"\n",
        "\n",
        "        # broadcasted_hypothesis = torch.matmul(\n",
        "        #     torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "        #     context_layer[:, :, head, :]\n",
        "        # )\n",
        "        # context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 0:\n",
        "    #     head = 6\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 4\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 0\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len)) \n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 1:\n",
        "    #     head = 6\n",
        "    #     # cls attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, 0] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 9\n",
        "    #     # relative position attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     for i in range(seq_len):\n",
        "    #         for j in range(seq_len):\n",
        "    #             if j > i:\n",
        "    #                 activation_hypothesis[i, j] = 1.0\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 2:\n",
        "    #     # next attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[0, 1:] = 1.0\n",
        "    #     # activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     head = 0\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 9\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "        \n",
        "    # if layer_id == 7:\n",
        "    #     head = 6\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 8:\n",
        "    #     head = 2\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # [((2, 0), ('next_attention', 0.0134178736883759)),\n",
        "    # ((2, 9), ('next_attention', 0.0195240472425213)),\n",
        "    # ((0, 0), ('uniform_attention', 0.1473364017258112)),\n",
        "    # ((1, 6), ('cls_attention', 0.1820107218156672)),\n",
        "    # ((8, 2), ('eos_attention', 0.198991498540351)),\n",
        "    # ((0, 4), ('uniform_attention', 0.2081918466789174)),\n",
        "    # ((1, 9), ('relative_position_attention', 0.2308144694156767)),\n",
        "    # ((7, 6), ('eos_attention', 0.2383215559745129)),\n",
        "    # ((0, 6), ('uniform_attention', 0.246593192497212))]\n",
        "\n",
        "    context_layer = context_layer.reshape(batch, seq_len, hidden_dim)\n",
        "    return (context_layer, attn_probs)\n",
        "\n",
        "# target_layer = model_replace.bert.encoder.layer[2].attention.self\n",
        "# hook = target_layer.register_forward_hook(replace_attention_head)\n",
        "\n",
        "for layer_number, layer in enumerate(model_replace.bert.encoder.layer):\n",
        "    layer.attention.self.layer_id = layer_number\n",
        "    if layer_number in [2]:\n",
        "        layer.attention.self.register_forward_hook(replace_attention_head)\n",
        "\n",
        "ppl_zero_50 = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:400], true_tokens[:400]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model_replace(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "\n",
        "    # calculate full psuedo-perplexity\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_zero_50.append(torch.exp(-log_prob).item())\n",
        "\n",
        "ppl_subset_normal = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:400], true_tokens[:400]))):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "    if len(mask_token_index[0]) == 0:\n",
        "        raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    mask_pos = mask_token_index[1].item()\n",
        "    masked_logits = logits[0, mask_pos, :]  # shape: [vocab_size]\n",
        "    true_id = tokenizer.convert_tokens_to_ids(true_token)\n",
        "    log_probs = torch.log_softmax(masked_logits, dim=-1)\n",
        "    log_prob = log_probs[true_id] \n",
        "    # if i % 10 == 0: print(torch.exp(-log_prob).item())\n",
        "    ppl_subset_normal.append(torch.exp(-log_prob).item())\n",
        "\n",
        "# print('\\n')\n",
        "print('\\nWeird: ', np.median(ppl_subset_replaced), '\\t', np.mean(ppl_subset_replaced))\n",
        "print('Normal: ', np.median(ppl_subset_normal), '\\t', np.mean(ppl_subset_normal), '\\n')\n",
        "\n",
        "print(f\"Average Perplexity weird: pd-describe{(pd.Series(ppl_subset_replaced).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219244c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# boxplots:\n",
        "import matplotlib.pyplot as plt\n",
        "data = []\n",
        "for dataset in [ppl_subset_normal, ppl_zero_10, ppl_zero_20, ppl_zero_30]:\n",
        "    threshold = np.percentile(dataset, 92)\n",
        "    filtered_data = [x for x in dataset if x <= threshold]\n",
        "    data.append(filtered_data)\n",
        "normal_perplexity = f'Normal Perplexity\\nMedian={np.median(data[0]):.2f}\\nMean={np.mean(data[0]):.2f}\\nStd={np.std(data[0]):.2f}'\n",
        "strange_perplexity_10 = f'Zero Perplexity 10%\\nMedian={np.median(data[1]):.2f}\\nMean={np.mean(data[1]):.2f}\\nStd={np.std(data[1]):.2f}'\n",
        "strange_perplexity_20 = f'Zero Perplexity 20%\\nMedian={np.median(data[2]):.2f}\\nMean={np.mean(data[2]):.2f}\\nStd={np.std(data[2]):.2f}'\n",
        "strange_perplexity_30 = f'Zero Perplexity 30%\\nMedian={np.median(data[3]):.2f}\\nMean={np.mean(data[3]):.2f}\\nStd={np.std(data[3]):.2f}'\n",
        "\n",
        "labels = [normal_perplexity, strange_perplexity_10, strange_perplexity_20, strange_perplexity_30]\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.title(\"Perplexity Comparison | Zeroed Out Heads vs Normal Heads\\n\", weight='bold')\n",
        "plt.boxplot(data, labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b325320e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# boxplots:\n",
        "import matplotlib.pyplot as plt\n",
        "data = []\n",
        "for dataset in [ppl_subset_normal, ppl_weird]:\n",
        "    threshold = np.percentile(dataset, 95)\n",
        "    filtered_data = [x for x in dataset if x <= threshold]\n",
        "    data.append(filtered_data)\n",
        "normal_perplexity = f'Normal Perplexity\\nMedian={np.median(data[0]):.2f}\\nMean={np.mean(data[0]):.2f}\\nStd={np.std(data[0]):.2f}'\n",
        "strange_perplexity = f'Replacement Perplexity\\nMedian={np.median(data[1]):.2f}\\nMean={np.mean(data[1]):.2f}\\nStd={np.std(data[1]):.2f}'\n",
        "labels = [normal_perplexity, strange_perplexity]\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.title(\"Perplexity Comparison | Hypothesis-Replaced Heads vs Normal Heads\\n\", weight='bold')\n",
        "plt.boxplot(data, labels=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1435cd32",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.median(ppl_weird)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b643d9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "arr = [1,2,3,4,5,6,7,8,9]\n",
        "# use pd describe to show statistics\n",
        "import pandas as pd\n",
        "print(pd.Series(arr).describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0e77eca",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_replace = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model_replace.load_state_dict(model.state_dict(), strict=True)\n",
        "model_replace.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f6e5a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_replace = remove_all_hooks(model_replace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd18a26b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Model has hooks: model_replace\", has_hooks(model_replace))\n",
        "print(\"Model has hooks: model\", has_hooks(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e4f9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_outliers(data):\n",
        "    iqr = False\n",
        "    if iqr:\n",
        "        q1 = np.percentile(data, 25)\n",
        "        q3 = np.percentile(data, 75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "        return [x for x in data if lower_bound <= x <= upper_bound]\n",
        "    else:\n",
        "        return data[:-20]\n",
        "\n",
        "average_ppl_single_out = handle_outliers(average_ppl_single)\n",
        "average_ppl_full_out = handle_outliers(average_ppl_full)\n",
        "\n",
        "single = f'Single Token Perplexity\\nMedian={np.median(average_ppl_single):.2f}\\nMean={np.mean(average_ppl_single):.2f}\\nStd={np.std(average_ppl_single):.2f}'\n",
        "all_tokens = f'All Token Perplexity\\nMedian={np.median(average_ppl_full):.2f}\\nMean={np.mean(average_ppl_full):.2f}\\nStd={np.std(average_ppl_full):.2f}'\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot([average_ppl_single_out, average_ppl_full_out, average_ppl_full_out, average_ppl_full_out], labels=[single, all_tokens, all_tokens, all_tokens])\n",
        "plt.title(f'Perplexity Comparison: BERT-base\\nDataset: Tiny Sentences | len={len(average_ppl_full)}', weight='bold')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5deae86",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c670a7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_subset = sentences[:1_000]\n",
        "\n",
        "def replace_activation_with_hypothesis(layer, head, sentence, model, tokenizer):\n",
        "    hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "    program_to_use = programs_ex[[i for i, p in enumerate(programs_ex) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "\n",
        "    print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "    out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "    return out\n",
        "\n",
        "def make_replacement_hook(layer_idx, head_idx):\n",
        "    \"\"\"\n",
        "    Returns a hook function that replaces a specific head's activations.\n",
        "    \"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # output shape: (batch_size, num_heads, seq_length, head_dim)\n",
        "        batch_size, num_heads, seq_length, head_dim = output.shape\n",
        "        for b in range(batch_size):\n",
        "            sentence = tokenizer.decode(input[0][b])\n",
        "            replacement = replace_activation_with_hypothesis(layer_idx, head_idx, sentence, model, tokenizer)\n",
        "            if replacement is not None:\n",
        "                output[b, head_idx] = torch.tensor(replacement, dtype=output.dtype)\n",
        "        return output\n",
        "    return hook\n",
        "# \n",
        "def mask_sentence(sent, tokenizer):\n",
        "    tokens = tokenizer.tokenize(sent)\n",
        "    if not tokens:\n",
        "        return sent\n",
        "    idx = np.random.randint(len(tokens))\n",
        "    tokens[idx] = tokenizer.mask_token\n",
        "    return tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "hooks = []\n",
        "for (layer, head), (hypothesis, score) in best_fits_top:\n",
        "    hook = make_replacement_hook(layer, head)\n",
        "    hooks.append(hook)\n",
        "\n",
        "# evaluate bertbase model on masking with no hooks and calculate perplexity\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.nn import CrossEntropyLoss\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model.eval()\n",
        "loss_fct = CrossEntropyLoss()\n",
        "perplexities = []\n",
        "for i, sent in enumerate(sentence_subset):\n",
        "    masked = mask_sentence(sent, tokenizer)\n",
        "    inputs = tokenizer(masked, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    logits = logits.view(-1, model.config.vocab_size)\n",
        "    perplexity = torch.exp(loss_fct(logits, inputs[\"input_ids\"].view(-1)))\n",
        "    perplexities.append(perplexity.item())\n",
        "    if i % 20 == 0: print(f\"{i}/{len(sentence_subset)}: Perplexity without Hooks: {perplexity.item():.2f}\")\n",
        "print(f\"Average Perplexity without Hooks: {np.mean(perplexities):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c09770ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.nn import CrossEntropyLoss\n",
        "loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "def compute_bert_cross_entropy(sentence, model, tokenizer, replacement=False):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    n_tokens = len(tokens)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(n_tokens):\n",
        "        masked_input_ids = input_ids.copy()\n",
        "        masked_input_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "        inputs = torch.tensor([masked_input_ids])\n",
        "        labels = torch.tensor([input_ids])\n",
        "        labels[0, :i] = -100\n",
        "        labels[0, i+1:] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if replacement:\n",
        "                \n",
        "            else:\n",
        "                outputs = model(inputs, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                losses.append(loss.item())\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "sentence_subset = sentences[:50]\n",
        "cross_entropies = []\n",
        "\n",
        "for i, sent in enumerate(sentence_subset):\n",
        "    ce_loss = compute_bert_cross_entropy(sent, model, tokenizer)\n",
        "    cross_entropies.append(ce_loss)\n",
        "    if i % 2 == 0:\n",
        "        print(f\"{i}/{len(sentence_subset)}: Cross-Entropy = {ce_loss:.4f}\")\n",
        "\n",
        "print(f\"Average Cross-Entropy: {np.mean(cross_entropies):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9418594",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a550aad3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "# from nnsight import InstrumentedModel\n",
        "from nnsight import LanguageModel\n",
        "import math\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. Setup\n",
        "# ------------------------------------------------------------\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# We'll use the MLM head so logits are token-prediction probabilities\n",
        "hf_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "model = LanguageModel(hf_model)   # wrap for nnsight tracing\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "hf_model.to(device)\n",
        "\n",
        "# Heads you plan to replace\n",
        "replace_heads = [(0, 0), (4, 4), (11, 11)]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Prepare evaluation sentences\n",
        "# ------------------------------------------------------------\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial intelligence is transforming many industries.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"BERT models are powerful for natural language understanding.\",\n",
        "    \"Machine learning can detect patterns in data.\"\n",
        "]\n",
        "\n",
        "# Simple masking utility: mask one random non-special token per sentence\n",
        "def mask_sentence(sent, tokenizer):\n",
        "    tokens = tokenizer.tokenize(sent)\n",
        "    if not tokens:\n",
        "        return sent\n",
        "    idx = random.randrange(len(tokens))\n",
        "    tokens[idx] = tokenizer.mask_token\n",
        "    return tokenizer.convert_tokens_to_string(tokens)\n",
        "\n",
        "masked_sentences = [mask_sentence(s, tokenizer) for s in sentences]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Utility: compute MLM loss/perplexity for a batch\n",
        "# ------------------------------------------------------------\n",
        "def compute_mlm_loss(model, tokenizer, sentences, device):\n",
        "    \"\"\"Return total cross-entropy loss and perplexity for a list of masked sentences.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        inputs = tokenizer(sent, return_tensors=\"pt\").to(device)\n",
        "        # find masked index\n",
        "        mask_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "        if len(mask_index[0]) == 0:\n",
        "            continue\n",
        "        labels = inputs[\"input_ids\"].clone()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += 1\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Evaluate baseline (no head replacement)\n",
        "# ------------------------------------------------------------\n",
        "print(\"Evaluating baseline BERT ...\")\n",
        "baseline_loss, baseline_ppl = compute_mlm_loss(hf_model, tokenizer, masked_sentences, device)\n",
        "print(f\"Baseline cross-entropy: {baseline_loss:.4f}, Perplexity: {baseline_ppl:.4f}\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. Evaluate with head replacements\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\nEvaluating with replaced heads ...\")\n",
        "\n",
        "total_loss_replaced = 0.0\n",
        "total_tokens = 0\n",
        "\n",
        "for sent in tqdm(masked_sentences):\n",
        "    inputs = tokenizer(sent, return_tensors=\"pt\").to(device)\n",
        "    labels = inputs[\"input_ids\"].clone()\n",
        "\n",
        "    # Run a traced forward pass where we inject our hypotheses\n",
        "    with model.trace(inputs) as trace:\n",
        "        # Get seq_len and batch size\n",
        "        seq_len = trace[\"input_ids\"].shape[1]\n",
        "        batch_size = trace[\"input_ids\"].shape[0]\n",
        "\n",
        "        for (layer_i, head_j) in replace_heads:\n",
        "            attn_path = f\"bert.encoder.layer.{layer_i}.attention.self.softmax\"\n",
        "            attn_probs = trace[attn_path]  # shape [batch, num_heads, seq_len, seq_len]\n",
        "\n",
        "            # Obtain your hypothesis n×n matrix\n",
        "            hypothesis = hypothesize_attention(layer_i, head_j).to(device)\n",
        "\n",
        "            # Ensure shapes match (pad/trim if necessary)\n",
        "            if hypothesis.shape != (seq_len, seq_len):\n",
        "                raise ValueError(f\"Hypothesis shape {hypothesis.shape} != ({seq_len}, {seq_len})\")\n",
        "            attn_probs[:, head_j, :, :] = hypothesis\n",
        "\n",
        "        outputs = trace.output  # forward pass continues with replaced attention\n",
        "\n",
        "    # Compute MLM loss for this sentence\n",
        "    logits = outputs.logits\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fn(logits.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
        "    total_loss_replaced += loss.item()\n",
        "    total_tokens += 1\n",
        "\n",
        "avg_loss_replaced = total_loss_replaced / total_tokens\n",
        "ppl_replaced = math.exp(avg_loss_replaced)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. Compare results\n",
        "# ------------------------------------------------------------\n",
        "print(\"\\n=== RESULTS ===\")\n",
        "print(f\"Baseline Cross-Entropy: {baseline_loss:.4f}\")\n",
        "print(f\"Replaced  Cross-Entropy: {avg_loss_replaced:.4f}\")\n",
        "print(f\"Δ Cross-Entropy: {avg_loss_replaced - baseline_loss:+.4f}\")\n",
        "print(f\"Baseline Perplexity: {baseline_ppl:.4f}\")\n",
        "print(f\"Replaced  Perplexity: {ppl_replaced:.4f}\")\n",
        "print(f\"Δ Perplexity: {ppl_replaced - baseline_ppl:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e172eb8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for the bar chart\n",
        "scores = [5.6, 0.0]\n",
        "labels = ['baseline-BERT', 'replaced-BERT']\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.bar(labels, scores, color=['blue', 'salmon'])\n",
        "plt.ylabel('Score')\n",
        "plt.title('BERT Score Comparison (Bar Chart)')\n",
        "plt.ylim(0, 6) # Ensure the y-axis starts at 0\n",
        "plt.savefig('bert_score_comparison_bar_chart.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b639c4a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318aa7c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hypothesis_head(layer, head, sentence, tokenizer):\n",
        "        hypothesis = best_fits.get((layer, head), (None, None))[0]\n",
        "        program_to_use = programs_ex[[i for i, p in enumerate(programs_ex) if p.__name__ == hypothesis][0]] if hypothesis else None\n",
        "        # print(f\"Replacing Layer {layer}, Head {head} with Hypothesis: {hypothesis} | program: {program_to_use}\")\n",
        "        out = program_to_use(sentence, tokenizer) if program_to_use else None\n",
        "        return out\n",
        "\n",
        "def replace_attention_head(module, input, output):\n",
        "    \"\"\" module: BertSelfAttention | output: tuple (context_layer, attention_probs)\"\"\"\n",
        "    context_layer, attn_probs = output\n",
        "    batch, seq_len, hidden_dim = context_layer.shape\n",
        "    num_heads = module.num_attention_heads\n",
        "    head_dim = hidden_dim // num_heads\n",
        "    context_layer = context_layer.view(batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "    # print current sentence\n",
        "    # print(f\"Current Sentence: {inputs['input_ids']}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "    # decoded_sentence = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
        "    # decoded_sentences = tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True)\n",
        "\n",
        "    # for decoded_sentence in decoded_sentences:\n",
        "        # print(f\"Decoded Sentence: {decoded_sentence}\")\n",
        "\n",
        "    layer_id = getattr(module, \"layer_id\", None)\n",
        "    # heads = []\n",
        "    # for (layer, head), (fn_name, score) in best_fits_top:\n",
        "        # if layer == layer_id:\n",
        "            # print(layer, head, fn_name)\n",
        "            # heads.append(head)\n",
        "\n",
        "    # ((5, 1), ('eos_attention', 0.2554249071807607)),\n",
        "    # ((1, 8), ('relative_position_attention', 0.2565078611780894)),\n",
        "    # ((4, 3), ('pronoun_reference', 0.2626135723148083)),\n",
        "    # ((7, 7), ('eos_attention', 0.2630709228109905)),\n",
        "    # ((8, 6), ('eos_attention', 0.2714455279356093)),\n",
        "    # ((3, 1), ('relative_position_attention', 0.2823563840604032)),\n",
        "    # ((3, 7), ('relative_position_attention', 0.2825369197971396)),\n",
        "    # ((7, 3), ('eos_attention', 0.2884066405333515)),\n",
        "    # ((2, 3), ('relative_position_attention', 0.2897239912937058)),\n",
        "    # ((2, 5), ('relative_position_attention', 0.2932481224141057))]\n",
        "\n",
        "    # for layer in range(12):\n",
        "    #     if layer in best_fits_top[0] and layer_id == layer:\n",
        "    \n",
        "    # if layer_id == 0:\n",
        "    #     head = 6\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 4\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len))\n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 0\n",
        "    #     # uniform attention np\n",
        "    #     activation_hypothesis = np.ones((seq_len, seq_len)) \n",
        "    #     activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 1:\n",
        "    #     head = 6\n",
        "    #     # cls attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, 0] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     # head = 9\n",
        "    #     # # relative position attention np\n",
        "    #     # activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     # for i in range(seq_len):\n",
        "    #     #     for j in range(seq_len):\n",
        "    #     #         if j > i:\n",
        "    #     #             activation_hypothesis[i, j] = 1.0\n",
        "    #     # activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "    #     # broadcasted_hypothesis = torch.matmul(\n",
        "    #     #     torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #     #     context_layer[:, :, head, :]\n",
        "    #     # )\n",
        "    #     # context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 2:\n",
        "    #     # next attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[0, 1:] = 1.0\n",
        "    #     # activation_hypothesis = activation_hypothesis / activation_hypothesis.sum(axis=1, keepdims=True)\n",
        "        \n",
        "    #     head = 0\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    #     head = 9\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "        \n",
        "    # if layer_id == 7:\n",
        "    #     head = 6\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # if layer_id == 8:\n",
        "    #     head = 2\n",
        "    #     # eos attention np\n",
        "    #     activation_hypothesis = np.zeros((seq_len, seq_len))\n",
        "    #     activation_hypothesis[:, -1] = 1.0\n",
        "\n",
        "    #     broadcasted_hypothesis = torch.matmul(\n",
        "    #         torch.from_numpy(activation_hypothesis).to(context_layer.device, context_layer.dtype),\n",
        "    #         context_layer[:, :, head, :]\n",
        "    #     )\n",
        "    #     context_layer[:, :, head, :] = broadcasted_hypothesis\n",
        "\n",
        "    # zero_out = 10 # percent of all heads\n",
        "    # random_heads = np.random.choice(range(num_heads), size=int(num_heads * zero_out / 100), replace=False)\n",
        "    # random number from 1 to 100 (2 random numbers in a list)\n",
        "    random_heads = np.random.choice(range(num_heads), size=4, replace=False)\n",
        "    # print(len(random_heads))\n",
        "    for i in random_heads:\n",
        "        context_layer[:, :, i, :] = 0\n",
        "\n",
        "    context_layer = context_layer.reshape(batch, seq_len, hidden_dim)\n",
        "    return (context_layer, attn_probs)\n",
        "\n",
        "# target_layer = model_replace.bert.encoder.layer[2].attention.self\n",
        "# hook = target_layer.register_forward_hook(replace_attention_head)\n",
        "\n",
        "for layer_number, layer in enumerate(model_replace.bert.encoder.layer):\n",
        "    layer.attention.self.layer_id = layer_number\n",
        "    # if layer_number in [2]:\n",
        "    layer.attention.self.register_forward_hook(replace_attention_head)\n",
        "\n",
        "ppl_weird = []\n",
        "save_tokens = []\n",
        "for i, (sentence, true_token) in tqdm(enumerate(zip(sentences[:100], true_tokens[:100]))):\n",
        "    # calculate full psuedo-perplexity\n",
        "    # inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    # mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "    # if len(mask_token_index[0]) == 0:\n",
        "    #     raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "    # with torch.no_grad():\n",
        "    #     logits = model_replace(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    # tokens = tokenizer.tokenize(sentence)\n",
        "    # ids = tokenizer.convert_tokens_to_ids(tokens)   \n",
        "    # input_ids = torch.tensor([ids])\n",
        "    # log_probs = []\n",
        "    # for i in range(1, len(tokens)-1):  # consider all tokens except [CLS], [SEP]\n",
        "    #     masked = input_ids.clone()\n",
        "    #     masked[0, i] = tokenizer.mask_token_id\n",
        "    #     with torch.no_grad():\n",
        "    #         logits = model_replace(masked).logits\n",
        "    #     prob = torch.log_softmax(logits[0, i], dim=-1)[input_ids[0, i]]\n",
        "    #     log_probs.append(prob)\n",
        "    # ppl_weird.append(torch.exp(-torch.stack(log_probs).mean()).item())\n",
        "\n",
        "    # calculate single token perplexity\n",
        "    enc = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    input_ids = enc[\"input_ids\"][0]\n",
        "    attention_mask = enc[\"attention_mask\"]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    # for token in tokens:\n",
        "    #     save_tokens.append(token)\n",
        "    per_token_ppl = []\n",
        "\n",
        "    # Loop over tokens EXCLUDING CLS (0) and SEP (last)\n",
        "    for i in range(1, len(input_ids) - 1):\n",
        "        original_id = input_ids[i].item()\n",
        "        original_token = tokens[i]\n",
        "        masked_ids = input_ids.clone()\n",
        "        masked_ids[i] = tokenizer.mask_token_id\n",
        "        with torch.no_grad():\n",
        "            logits = model_replace(\n",
        "                masked_ids.unsqueeze(0),\n",
        "                attention_mask=attention_mask).logits\n",
        "        log_probs = torch.log_softmax(logits[0, i], dim=-1)\n",
        "        log_prob_original = log_probs[original_id]\n",
        "        ppl = torch.exp(-log_prob_original).item()\n",
        "        \n",
        "        # if original_token != \"[MASK]\": \n",
        "        ppl_weird.append(ppl)\n",
        "        save_tokens.append(original_token)\n",
        "\n",
        "# ppl_subset_normal = []\n",
        "# for i, (sentence, true_token) in tqdm(enumerate(zip(masked_sentences[:100], true_tokens[:100]))):\n",
        "#     inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "#     mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
        "\n",
        "#     if len(mask_token_index[0]) == 0:\n",
        "#         raise ValueError(f\"No [MASK] token found in: {sentence}\")\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(**inputs).logits  # shape: [1, seq_len, vocab_size]\n",
        "    \n",
        "#     # calculate full psuedo-perplexity\n",
        "#     tokens = tokenizer.tokenize(sentence)\n",
        "#     ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "#     input_ids = torch.tensor([ids])\n",
        "#     log_probs = []\n",
        "#     for i in range(1, len(tokens)-1):  # consider all tokens except [CLS], [SEP]\n",
        "#         masked = input_ids.clone()\n",
        "#         masked[0, i] = tokenizer.mask_token_id\n",
        "#         with torch.no_grad():\n",
        "#             logits = model(masked).logits\n",
        "#         prob = torch.log_softmax(logits[0, i], dim=-1)[input_ids[0, i]]\n",
        "#         log_probs.append(prob)\n",
        "#     ppl_subset_normal.append(torch.exp(-torch.stack(log_probs).mean()).item())\n",
        "\n",
        "# print('\\n')\n",
        "# get rid of top 5% outliers\n",
        "# ppl_weird = np.percentile(sorted(ppl_weird), 95)\n",
        "\n",
        "print('\\nWeird: ', np.median(ppl_weird), '\\t', np.mean(ppl_weird), '\\t', np.std(ppl_weird))\n",
        "print('Normal: ', np.median(ppl_subset_normal), '\\t', np.mean(ppl_subset_normal), '\\n')\n",
        "\n",
        "print(f\"Average Perplexity weird: pd-describe{(pd.Series(ppl_weird).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f85b2ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_replace = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "model_replace.load_state_dict(model.state_dict(), strict=True)\n",
        "model_replace.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f1ea626",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normal:  1.806810438632965 \t 7845.270243987441 \t 74461.00454762368\n",
        "# Weird"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c390dd97",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids = enc[\"input_ids\"][0]\n",
        "attention_mask = enc[\"attention_mask\"]\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "per_token_ppl = []\n",
        "\n",
        "# Loop over tokens EXCLUDING CLS (0) and SEP (last)\n",
        "for i in range(1, len(input_ids) - 1):\n",
        "    original_id = input_ids[i].item()\n",
        "    original_token = tokens[i]\n",
        "    masked_ids = input_ids.clone()\n",
        "    masked_ids[i] = tokenizer.mask_token_id\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            masked_ids.unsqueeze(0),\n",
        "            attention_mask=attention_mask).logits\n",
        "    log_probs = torch.log_softmax(logits[0, i], dim=-1)\n",
        "    log_prob_original = log_probs[original_id]\n",
        "    ppl = torch.exp(-log_prob_original).item()\n",
        "    \n",
        "    ppl_weird.append(ppl)\n",
        "    save_tokens.append(original_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ac7d78",
      "metadata": {},
      "outputs": [],
      "source": [
        "zipped_results = list(zip(save_tokens, ppl_weird))\n",
        "np.savetxt('data/twelve_heads_replacement_perplexity_results_500.txt', zipped_results, fmt='%s', delimiter='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ca43e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "for token, ppl in zip(save_tokens, ppl_weird):\n",
        "    print(f\"Token: {token}\\t\\tPerplexity: {ppl}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd0a8951",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Setup the sample data (recreating the DataFrame)\n",
        "# The data provided by the user:\n",
        "\n",
        "sentence = full_story\n",
        "\n",
        "# Encode with special tokens\n",
        "enc = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
        "input_ids = enc[\"input_ids\"][0]\n",
        "attention_mask = enc[\"attention_mask\"]\n",
        "\n",
        "# Convert ids → tokens (these include [CLS] and [SEP])\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "per_token_ppl = []\n",
        "\n",
        "# Loop over tokens EXCLUDING CLS (0) and SEP (last)\n",
        "for i in range(1, len(input_ids) - 1):\n",
        "\n",
        "    original_id = input_ids[i].item()\n",
        "    original_token = tokens[i]\n",
        "\n",
        "    masked_ids = input_ids.clone()\n",
        "    masked_ids[i] = tokenizer.mask_token_id\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            masked_ids.unsqueeze(0),\n",
        "            attention_mask=attention_mask\n",
        "        ).logits\n",
        "\n",
        "    log_probs = torch.log_softmax(logits[0, i], dim=-1)\n",
        "    log_prob_original = log_probs[original_id]\n",
        "\n",
        "    ppl = torch.exp(-log_prob_original).item()\n",
        "\n",
        "    per_token_ppl.append((original_token, ppl))\n",
        "\n",
        "tokens, perplexities = zip(*per_token_ppl)\n",
        "    \n",
        "raw_data = np.array([\n",
        "    [token, f\"{prob:.2f}\"] for token, prob in zip(tokens, perplexities)\n",
        "])\n",
        "\n",
        "# Create a DataFrame for context (assuming columns were 'Token' and 'Perplexity')\n",
        "df = pd.DataFrame(raw_data, columns=['Token', 'Perplexity'])\n",
        "\n",
        "# Convert Perplexity column to numeric for calculations\n",
        "perplexity_values = df['Perplexity'].astype(float)\n",
        "\n",
        "# 2. Setup the plot and transpose the table data\n",
        "fig, ax = plt.subplots(figsize=(16, 1.25)) # Slightly increased height for statistics text\n",
        "ax.axis('off') # Hide the axis lines\n",
        "\n",
        "# --- Core modification: Transposing the data ---\n",
        "cell_text = df.values.T\n",
        "\n",
        "# Define the new row labels based on the original columns\n",
        "row_labels = ['Token', 'Perplexity']\n",
        "\n",
        "# 3. Create the table with transposed data and updated labels\n",
        "# --- Modification 1: Setting colLabels=None to remove the empty header row ---\n",
        "table = ax.table(\n",
        "    cellText=cell_text,\n",
        "    rowLabels=row_labels,  # 'Token' and 'Perplexity' are now row headers\n",
        "    colLabels=None,        # Removed column labels to eliminate the top empty row\n",
        "    cellLoc='center',\n",
        "    loc='center')\n",
        "\n",
        "# Optional: Adjust table properties\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1, 2) # Scale height slightly for better spacing\n",
        "\n",
        "# 4. Calculate and display statistics below the table\n",
        "median_val = np.median(perplexity_values)\n",
        "mean_val = np.mean(perplexity_values)\n",
        "std_val = np.std(perplexity_values)\n",
        "\n",
        "stats_text = (\n",
        "    f\"Perplexity Statistics | MODEL UNTOUCHED:\\n\"\n",
        "    f\"Median: {median_val:.2f} | Mean: {mean_val:.2f} | Std Dev: {std_val:.0f}\"\n",
        ")\n",
        "\n",
        "# --- Modification 2: Removed plt.title() ---\n",
        "plt.title(stats_text)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbe8a9e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "765b2886",
      "metadata": {},
      "outputs": [],
      "source": [
        "perplexity_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb26d1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46454c1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nWeird: ', np.median(ppl_zero_30), '\\t', np.mean(ppl_zero_30))\n",
        "print('Normal: ', np.median(ppl_subset_normal), '\\t', np.mean(ppl_subset_normal), '\\n')\n",
        "\n",
        "print(f\"Average Perplexity weird: pd-describe{(pd.Series(ppl_subset_replaced).describe())}\\n\")\n",
        "print(f\"Average Perplexity normal: pd-describe{(pd.Series(ppl_subset_normal).describe())}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
