{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6443588e",
      "metadata": {
        "id": "6443588e"
      },
      "source": [
        "Author: Amiri Hayes \\\n",
        "Date Updated: 7/15/25 \\\n",
        "Title: ViewLLM\n",
        "\n",
        "Starter code !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99",
      "metadata": {
        "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99"
      },
      "outputs": [],
      "source": [
        "# package installs if necessary\n",
        "!pip install matplotlib torch spacy nltk tqdm transformers datasets scikit-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3e36de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3e36de",
        "outputId": "5c233452-f8b9-4137-f8b9-cd2881649f94"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import Optional, Tuple, Callable\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import PowerNorm, ListedColormap\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
        "print(\"Imports completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012daf28",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Data & Pattern Initialization Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbe22fd",
      "metadata": {
        "id": "7cbe22fd"
      },
      "outputs": [],
      "source": [
        "# IMPORT THE PROGRAM DATABASE:\n",
        "\n",
        "from programs import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d51a2fa",
      "metadata": {
        "id": "9d51a2fa"
      },
      "outputs": [],
      "source": [
        "# GENERATE & VISUALIZE ATTENTION PATTERN SCORES FROM PROGRAMS\n",
        "\n",
        "def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    q = np.clip(q, 1e-12, 1.0)\n",
        "    p /= p.sum()\n",
        "    q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
        "\n",
        "def score_prediction(torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: Tuple[int, int], pattern: Callable, sentence_1: str, sentence_2: Optional[str] = None, distance=\"jsd\", output=False):\n",
        "    layer, head = head_loc\n",
        "    tokens = torch_tokenizer(sentence_1, return_tensors=\"pt\")\n",
        "\n",
        "    if torch_model.config.is_encoder_decoder:\n",
        "        decoder_input_ids = tokens[\"input_ids\"]\n",
        "        outputs = torch_model(input_ids=tokens[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "        att = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "    else:\n",
        "        if sentence_2 and pattern.__name__ == \"chainofthought_pattern\":\n",
        "            name = \"Chain of Thought Pattern\"\n",
        "            tokens_2 = torch_tokenizer(sentence_2, return_tensors=\"pt\")\n",
        "\n",
        "            att = torch_model(**tokens_2, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            pred_att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "            if output: print(\"RUNNING FIRST WITH NO HINT\")\n",
        "            question, answer, vector_att = chainofthought_pattern(sentence_1, torch_tokenizer, pred_att, hint=False)\n",
        "            if output: print(\"RUNNING AFTER WITH A HINT\")\n",
        "            question, answer, vector_pred_att = chainofthought_pattern(sentence_2, torch_tokenizer, att, hint=True)\n",
        "\n",
        "            att, pred_att = vector_att.copy(), vector_pred_att.copy()\n",
        "        else:\n",
        "            att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            if pattern.__name__ == \"linear_fit\":\n",
        "                name, pred_att = pattern(sentence_1, torch_tokenizer, idx=0)\n",
        "            else: name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    if distance == \"raw\":\n",
        "        score = np.abs(att - pred_att).sum()\n",
        "    elif distance == \"jsd\":\n",
        "        jensonshannon_distances = []\n",
        "        for row_att, row_out in zip(att, pred_att):\n",
        "            jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "        score = np.mean(jensonshannon_distances)\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\":\n",
        "        score = np.sqrt(js_divergence(att, pred_att))\n",
        "\n",
        "    if output == \"cot\":\n",
        "        colors = \"inferno\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 9))\n",
        "        axes[0].plot(att, color=plt.get_cmap(colors)(0.6))\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        axes[1].plot(pred_att, color=plt.get_cmap(colors)(0.9))\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        bound_axes = False\n",
        "        for i in range(2):\n",
        "            axes[i].set_xlabel(\"Token Index\")\n",
        "            axes[i].set_ylabel(\"Attention Weight\")\n",
        "            axes[i].grid(True)\n",
        "            if bound_axes:\n",
        "                axes[i].set_ylim(0, 1)\n",
        "                axes[i].set_xlim(0, len(att) - 1)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        question_chart = question.replace(\".\", \".\\n\")\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nQuestion: \\\"{question_chart}\\n\\nAnswer: \\\"{answer}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "    \n",
        "    toks = torch_tokenizer([sentence_1], return_tensors=\"pt\")\n",
        "    token_ids = toks[\"input_ids\"][0]\n",
        "    tokens = torch_tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    if output == True:\n",
        "        colors=\"Greens\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        im1 = axes[0].imshow(att, cmap=colors, aspect='auto')\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        fig.colorbar(im1, ax=axes[0])\n",
        "        im2 = axes[1].imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        fig.colorbar(im2, ax=axes[1])\n",
        "        for i in range(2):\n",
        "            axes[i].set_xticks(range(len(tokens)))\n",
        "            axes[i].set_yticks(range(len(tokens)))\n",
        "            axes[i].set_xticklabels(tokens, rotation=90)\n",
        "            axes[i].set_yticklabels(tokens)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nSentence: \\\"{sentence_1}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"optimal\":\n",
        "        colors = \"Oranges\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        ax.set_axis_off()\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"actual\":\n",
        "        colors = \"Reds\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(att, cmap=colors, aspect='auto')\n",
        "        ax.set_title(\"Example Head Attention for Pattern\")\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n899pxRzSWRe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n899pxRzSWRe",
        "outputId": "470ad98d-f61e-4b36-bde9-e6c2bb80882d"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (1):\n",
        "\n",
        "file = 'data/small_text.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "sentences = []\n",
        "for paragraph in df['text']:\n",
        "    sentences.extend(sent_tokenize(paragraph))\n",
        "\n",
        "sentences = sentences[:10_000]\n",
        "print(\"Sentences from Tiny Stories Dataset:\")\n",
        "for sentence in sentences[20:30]:\n",
        "  print(f\"\\t{sentence}\")\n",
        "\n",
        "df_json = pd.read_json('data/generic_sentences.json')\n",
        "generic_sentences = df_json[0].tolist()\n",
        "print(\"\\nGeneric Sentences:\")\n",
        "for sentence in generic_sentences[:10]:\n",
        "  print(f\"\\t{sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0adb7458",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (2):\n",
        "\n",
        "math_data = pd.read_json('data/math_problems_results.jsonl', lines=True)\n",
        "\n",
        "filtered_results = math_data[\n",
        "    (math_data['consistency'] == \"False\") &\n",
        "    (math_data['evaluated_answer_nohint'] != \"DNF: llm did not finish\") &\n",
        "    (math_data['evaluated_answer_hint'] != \"DNF: llm did not finish\")\n",
        "]\n",
        "\n",
        "answers_nohint = filtered_results['answer_nohint'].tolist()\n",
        "answers_hint = filtered_results['answer_hint'].tolist()\n",
        "prefix = \"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nuser\\n\\n\"\n",
        "\n",
        "prompts = []\n",
        "for s1, s2 in zip(answers_nohint, answers_hint):\n",
        "    if s1.startswith(prefix): s1 = s1[len(prefix):]\n",
        "    if s2.startswith(prefix): s2 = s2[len(prefix):]\n",
        "\n",
        "    i_suffix_s1 = s1.find(\"assistant\")\n",
        "    if i_suffix_s1 != -1: s1 = s1[:i_suffix_s1].strip()\n",
        "\n",
        "    i_suffix_s2 = s2.find(\"assistant\")\n",
        "    if i_suffix_s2 != -1: s2 = s2[:i_suffix_s2].strip()\n",
        "\n",
        "    if s1 and s2: prompts.append((s1, s2))\n",
        "\n",
        "print(len(prompts), \"relevant prompts loaded from math problems dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1015c0ec",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "  Analysis Helper Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff49f93",
      "metadata": {
        "id": "1ff49f93",
        "outputId": "d8f00b5c-d395-4564-885c-d1f65e5c815d"
      },
      "outputs": [],
      "source": [
        "# ANALYZE PATTERN AT LAYER AND HEAD\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "sentence = \"I like apples and I like bananas. I like apples more though.\"\n",
        "\n",
        "models = [\"bert-base-uncased\", \"openai-community/gpt2\", \"google-t5/t5-small\", \"Meta-Llama-3.1-8B-Instruct\"]\n",
        "\n",
        "i = 0\n",
        "name = models[i]\n",
        "\n",
        "if i in [0,1,2]: # small models ( <400M Params )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    model = AutoModel.from_pretrained(name, output_attentions=True)\n",
        "    model.eval()\n",
        "\n",
        "elif i == 3: # big model & cot ( 8B Params, Load from Compute Node )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt_num = 0\n",
        "    sentence = prompts[prompt_num][0]  # Use the prompt's first sentence (no hint)\n",
        "    sentence_with_hint = prompts[prompt_num][1]  # Use prompt's second sentence (hint)\n",
        "\n",
        "def pronoun_coreference(sentence: str, tokenizer) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "\n",
        "    words = tokenizer.convert_ids_to_tokens(toks.input_ids[0])\n",
        "    pronouns = [\"he\", \"she\", \"him\", \"her\", \"they\", \"them\", \"it\", \"its\", \"his\", \"hers\"]\n",
        "\n",
        "    for i, tok in enumerate(words):\n",
        "        if tok in pronouns:\n",
        "            # Simple heuristic: link to the closest preceding noun\n",
        "            for j in range(i - 1, 0, -1): # Iterate backwards, excluding [CLS] and current\n",
        "                # Check for capitalized words as a proxy for nouns (simplification)\n",
        "                if words[j].isalpha() and words[j][0].isupper():\n",
        "                  out[i, j] = 0.7  # Attend to the noun\n",
        "                  out[i, i] = 0.3 # Attend to self\n",
        "                  break  # Link to closest preceding noun\n",
        "\n",
        "    for row in range(len_seq): # Ensure no row is all zeros\n",
        "        if out[row].sum() == 0:\n",
        "            out[row, -1] = 1.0 \n",
        "    out += 1e-4  # Avoid division by zero\n",
        "    out = out / out.sum(axis=1, keepdims=True)  # Normalize rows\n",
        "\n",
        "    return \"Pronoun Coreference Pattern\", out\n",
        "\n",
        "layer, head = 7, 0\n",
        "sentence = \"he said no, he did not, he will not, he feels weirdly good about it\"\n",
        "score_prediction(model, tokenizer, (layer, head), pronoun_coreference, sentence, distance=\"jsd\", output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd29726",
      "metadata": {
        "id": "9fd29726",
        "outputId": "feeb18a4-11de-4928-9582-fdf4b4c816df"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (1 SENTENCE)\n",
        "\n",
        "def visualize_full_model(sentence: str, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9, sentence_2: Optional[str] = None) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    num_heads = model.config.num_attention_heads\n",
        "    model_viz = np.zeros((num_layers, num_heads))\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_heads):\n",
        "            score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "            model_viz[i, j] = score\n",
        "    print(f\"Best Score: {min(map(min, model_viz)):.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=model_viz.min(), vmax=model_viz.max())\n",
        "    plt.imshow(model_viz, cmap='Greens_r', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_three = list(zip(*np.unravel_index(np.argsort(model_viz, axis=None)[:3], model_viz.shape)))\n",
        "    for tuple_val in top_three:\n",
        "        layer, head = tuple_val\n",
        "        score = model_viz[layer, head]\n",
        "        print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return model_viz, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "pattern_name = \"Repeated Attention Pattern\"\n",
        "underlined_name_unicode = \"\".join([char + '\\u0332' for char in pattern_name])\n",
        "visualize_full_model(sentence, model, tokenizer, repeated_attention, title=f\"Top Heads: {underlined_name_unicode} | {model_name}\\nSentence: \\\"{sentence}\\\"\\n\", bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G90Rgk6fVKIZ",
      "metadata": {
        "id": "G90Rgk6fVKIZ",
        "outputId": "56ce27bd-6ea5-4f3d-cbf7-f6dc1b69f015"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (MULTIPLE SENTENCES)\n",
        "\n",
        "def visualize_full_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    header = [\"Layer\", \"Head\", \"Score\"]\n",
        "    csv_file_name = \"scores.csv\"\n",
        "    file_exists = os.path.exists(csv_file_name)\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "    \n",
        "        average_score = np.zeros((num_layers, num_heads))\n",
        "        for sentence in sentences:\n",
        "            sentence_1 = sentence[0]  # first sentence (no hint)\n",
        "            sentence_2 = sentence[1]  # second sentence (hint)\n",
        "            model_score = np.zeros((num_layers, num_heads))\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    score = score_prediction(model, tokenizer, (i, j), chainofthought_pattern, sentence_1, sentence_2, distance=\"jsd\", output=False)\n",
        "                    writer.writerow([i, j, f\"{score:.2f}\"])\n",
        "                    print(f\"Layer {i}, Head {j} - Score: {score:.2f}\")\n",
        "                    model_score[i, j] = score\n",
        "            average_score += model_score\n",
        "        average_score /= len(sentences)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=average_score.min(), vmax=average_score.max())\n",
        "    plt.imshow(average_score, cmap='Reds', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\": \n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score.ravel())[::-1][:3], average_score.shape))) # highest scores\n",
        "    else:\n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score, axis=None)[3:], average_score.shape))) # lowest scores\n",
        "        top_three = np.sort(average_score)\n",
        "\n",
        "    for tuple_val in top_three:\n",
        "            layer, head = tuple_val\n",
        "            score = average_score[layer, head]\n",
        "            print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return average_score, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentences_zipped = list(zip(answers_nohint[:5], answers_hint[:5]))\n",
        "visualize_full_model(sentences_zipped, model, tokenizer, chainofthought_pattern, title=\"Top Heads: Chain-of_Thought Evaluation [AVERAGE]\\n\"+model_name, bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eTFYV1Gpzv",
      "metadata": {
        "id": "b8eTFYV1Gpzv"
      },
      "outputs": [],
      "source": [
        "# FILTER SENTENCES TO TARGET SPECIFIC PATTERNS\n",
        "\n",
        "def filterer(sentences: list[str], length_matters: bool=False, punctuation_matters: bool=False, duplicates: bool=False) -> list[str]:\n",
        "    filtered_sentences = []\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    for sentence in sentences:\n",
        "        passes_all_active_filters = True\n",
        "        if length_matters:\n",
        "            if len(sentence.split()) < 5:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if punctuation_matters:\n",
        "            punctuation_count = sum(1 for char in sentence if char in punctuation_set)\n",
        "            if punctuation_count < 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if duplicates:\n",
        "            set_l = len(set(sentence.split()))\n",
        "            sent_1 = len(sentence.split())\n",
        "            if set_l >= sent_1 - 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if passes_all_active_filters: filtered_sentences.append(sentence)\n",
        "    return filtered_sentences\n",
        "\n",
        "filtered_sentences = filterer(generic_sentences, length_matters = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mR56LNrRd37k",
      "metadata": {
        "id": "mR56LNrRd37k",
        "outputId": "4b42cff5-d843-439d-8409-d5c47e9b87a7"
      },
      "outputs": [],
      "source": [
        "# ANALYZE HEAD PATTERN ON SELECTED SENTENCES\n",
        "\n",
        "def visualize_highest_head(sentences: list[str], top_n:  int, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: tuple[int, int], pattern: Callable):\n",
        "    layer, head = head_loc\n",
        "    scores = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        score = score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, distance=\"jsd\", output=False)\n",
        "        scores.append(score)\n",
        "\n",
        "    indexed_scores = list(enumerate(scores))\n",
        "    sorted_scores = sorted(indexed_scores, key= lambda x: x[1])\n",
        "    top_scores = sorted_scores[:top_n]\n",
        "\n",
        "    for idx, score in top_scores:\n",
        "        print(f\"Sentence #{idx} Score: {score}\")\n",
        "        sentence = sentences[idx]\n",
        "        score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, output=True)\n",
        "    return 0\n",
        "\n",
        "layer, head = 7, 8\n",
        "visualize_highest_head(filtered_sentences, 3, model, tokenizer, (layer, head), punctuation_attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be980bdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS\n",
        "\n",
        "def classify_whole_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, patterns: list[Callable]) -> dict[Tuple[int, int], Tuple[str, float]]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "    activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "    header = [\"i\", \"j\", \"Pattern\", \"Score\"]\n",
        "    \n",
        "    csv_file_name = \"data/best_fit.csv\"\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "        for pattern in patterns:\n",
        "            print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "            avg_score = []\n",
        "            for idx, sentence in enumerate(sentences):\n",
        "                if idx % 20 == 0: print(f\"\\tProcessing sentence {idx}/{len(sentences)}\")\n",
        "                for i in range(num_layers):\n",
        "                    for j in range(num_heads):\n",
        "                        score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                        avg_score.append(score)\n",
        "\n",
        "                        \n",
        "                avg_score = np.mean(avg_score)\n",
        "                print(i, j, avg_score)\n",
        "            if avg_score > 0.5: continue\n",
        "            key = (i, j)\n",
        "\n",
        "            if key not in activations or avg_score < activations[key][1]:\n",
        "                activations[key] = (pattern.__name__, avg_score)\n",
        "            writer.writerow([i, j, pattern.__name__, avg_score])\n",
        "\n",
        "    return activations\n",
        "\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "activations = classify_whole_model(generic_sentences[:5], model, tokenizer, patterns)\n",
        "print(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08432fb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS (CONTINUED)\n",
        "\n",
        "torch_model = model\n",
        "torch_tokenizer = tokenizer\n",
        "\n",
        "num_layers = torch_model.config.num_hidden_layers\n",
        "num_heads = torch_model.config.num_attention_heads\n",
        "activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, dependencies, pos_alignment]\n",
        "\n",
        "short = sentences[:3]\n",
        "csv_file_name = \"data/best_fit_2.csv\"\n",
        "with open(csv_file_name, 'a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for pattern in patterns:\n",
        "        print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "        avg_score = []\n",
        "        for idx, sentence in enumerate(short):\n",
        "            print(f\"\\tProcessing sentence {idx}/{len(short)}\")\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    if i != 3 or j != 9: continue\n",
        "                    score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                    if score < 0.55:\n",
        "                        avg_score.append((idx, pattern.__name__, i, j, score))\n",
        "        \n",
        "        score_dict = {}\n",
        "        for idx, pattern_name, i, j, score in avg_score:\n",
        "            score_dict.setdefault((i, j), []).append((pattern_name, score))\n",
        "        for (i, j), values in score_dict.items():\n",
        "            scores = [score for _, score in values]\n",
        "            avg_score_val = sum(scores) / len(scores)\n",
        "            pattern_name = values[0][0]\n",
        "            activations[(i, j)] = (pattern_name, avg_score_val)\n",
        "            print(f\"Layer {i}, Head {j} - Score: {avg_score_val:.2f}\")\n",
        "            writer.writerow([i, j, pattern.__name__, avg_score_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8374c077",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANALYZE EFFECT OF LINEAR WEIGHTS ON ATTENTION ACTIVATION ACCURACY\n",
        "\n",
        "def generate_dataset(patterns: list[Callable], model: PreTrainedModel, tokenizer: PreTrainedTokenizerBase, sentences: list[str], layer_head: tuple[int, int]):\n",
        "    layer, head = layer_head\n",
        "    X_data, y_data = [], []\n",
        "    print(\"Generating dataset for Layer\", layer, \", Head\", head)\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            attn = outputs.attentions[layer][0, head]\n",
        "        X_i_list = []\n",
        "        for pattern in patterns:\n",
        "            _, X_i = pattern(sentence, tokenizer)\n",
        "            X_i = torch.tensor(X_i, dtype=torch.float32)\n",
        "            X_i_list.append(X_i)\n",
        "        X_data.append(X_i_list)\n",
        "        y_data.append(attn)\n",
        "\n",
        "    torch.save({'X': X_data, 'y': y_data}, \"data/attention_dataset.pt\")\n",
        "    print(\"Dataset generated and saved to 'data/attention_dataset.pt'.\")\n",
        "\n",
        "def train_linearregression() -> pd.DataFrame:\n",
        "    data = torch.load(\"data/attention_dataset.pt\")\n",
        "    X, y = data['X'], data['y']\n",
        "    X, y = data['X'], data['y']\n",
        "    output = []\n",
        "\n",
        "    for i, (xb, yb) in enumerate(zip(X, y)):\n",
        "        xb = torch.stack(xb)\n",
        "        X_flat = (xb.reshape(len(xb), -1).T).numpy()\n",
        "        y_flat = yb.flatten().numpy()\n",
        "        reg = LinearRegression().fit(X_flat, y_flat)\n",
        "        if i % 100 == 0: print(f\"Sentence #{i} - Coeffs: {[float(f\"{coef:.2f}\") for coef in reg.coef_]}, Intercept: {reg.intercept_:.2f}\")\n",
        "        output.append([reg.coef_.tolist(), float(reg.intercept_)])\n",
        "\n",
        "    output = pd.DataFrame(output, columns=[\"Coefficients\", \"Intercept\"]).to_csv(\"data/linear_regression_results.csv\", index=False)\n",
        "    return output\n",
        "\n",
        "head_loc = (3, 9)\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "generate_dataset(patterns, model, tokenizer, sentences, head_loc)\n",
        "output = train_linearregression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1612da99",
      "metadata": {},
      "outputs": [],
      "source": [
        "# VISUALIZE BEST FIT PATTERNS ACROSS LAYERS AND HEADS\n",
        "\n",
        "df = pd.read_csv('data/best_fit.csv')\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "mat = np.zeros((num_layers, num_heads), dtype=object)\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        mat[r, c] = []\n",
        "\n",
        "for (i, j), group in df.groupby(['i', 'j']):\n",
        "    sorted_group = group.sort_values(by='Score', ascending=False)\n",
        "    mat[i, j] = [(row['Pattern'], row['Score']) for idx, row in sorted_group.iterrows()]\n",
        "\n",
        "unique_patterns = df['Pattern'].unique()\n",
        "\n",
        "dark_orange = '#FF8C00'\n",
        "dark_blue = '#0000A0'\n",
        "azure = '#007FFF'\n",
        "medium_green = '#6aa84f'\n",
        "gray = '#D3D3D3'\n",
        "dark_red = '#A00000'\n",
        "purple = '#800080'\n",
        "\n",
        "hex_colors = [dark_red, dark_blue, azure, medium_green, purple, gray, dark_orange]\n",
        "cmap_patterns = ListedColormap(hex_colors, name='my_cmap')\n",
        "\n",
        "pattern_colors = {pattern: cmap_patterns(i) for i, pattern in enumerate(unique_patterns)}\n",
        "white_color = (1, 1, 1, 1)\n",
        "plotting_matrix_rgb = np.zeros((num_layers, num_heads, 3))\n",
        "\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        cell_data = mat[r, c]\n",
        "        \n",
        "        if not cell_data:\n",
        "            plotting_matrix_rgb[r, c] = white_color[:3]\n",
        "        elif len(cell_data) == 1: \n",
        "            pattern_name = cell_data[0][0]\n",
        "            plotting_matrix_rgb[r, c] = pattern_colors[pattern_name][:3]\n",
        "        else:\n",
        "            top_pattern_1 = cell_data[0][0]\n",
        "            top_pattern_2 = cell_data[1][0]\n",
        "            color_1 = pattern_colors[top_pattern_1][:3]\n",
        "            color_2 = pattern_colors[top_pattern_2][:3]\n",
        "            plotting_matrix_rgb[r, c] = color_1 \n",
        "\n",
        "custom_draw_mask = np.zeros((num_layers, num_heads), dtype=bool)\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        if len(mat[r, c]) > 1:\n",
        "            custom_draw_mask[r, c] = True\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 18))\n",
        "img = ax.imshow(plotting_matrix_rgb, origin='lower', extent=[-0.5, num_heads - 0.5, -0.5, num_layers - 0.5])\n",
        "\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        if custom_draw_mask[r, c]:\n",
        "            cell_data = mat[r, c]\n",
        "            top_pattern_1 = cell_data[0][0]\n",
        "            top_pattern_2 = cell_data[1][0]\n",
        "            \n",
        "            color_1 = pattern_colors[top_pattern_1]\n",
        "            color_2 = pattern_colors[top_pattern_2]\n",
        "            triangle1 = mpatches.Polygon(\n",
        "                [[c - 0.5, r - 0.5], [c + 0.5, r - 0.5], [c + 0.5, r + 0.5]],\n",
        "                facecolor=color_1, edgecolor='none'\n",
        "            )\n",
        "            ax.add_patch(triangle1)\n",
        "            triangle2 = mpatches.Polygon(\n",
        "                [[c - 0.5, r - 0.5], [c - 0.5, r + 0.5], [c + 0.5, r + 0.5]],\n",
        "                facecolor=color_2, edgecolor='none'\n",
        "            )\n",
        "            ax.add_patch(triangle2)\n",
        "\n",
        "ax.set_xticks(np.arange(num_heads))\n",
        "ax.set_yticks(np.arange(num_layers))\n",
        "ax.set_xticks(np.arange(-0.5, num_heads, 1), minor=True)\n",
        "ax.set_yticks(np.arange(-0.5, num_layers, 1), minor=True)\n",
        "ax.set_xlabel('BERT Heads', fontsize=14)\n",
        "ax.set_ylabel('BERT Layers', fontsize=14)\n",
        "# ax.set_title('Pattern Distribution Across Layers and Heads')\n",
        "ax.set_aspect('equal')\n",
        "# ax.grid(color='black', linestyle='-', linewidth=0.5)\n",
        "ax.grid(which='minor', color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "legend_handles = []\n",
        "for pattern, color in pattern_colors.items():\n",
        "    patch = mpatches.Patch(color=color, label=pattern)\n",
        "    legend_handles.append(patch)\n",
        "\n",
        "white_patch = mpatches.Patch(facecolor=white_color, label='Pattern not Detected', edgecolor='black', linewidth=0.2)\n",
        "legend_handles.append(white_patch)\n",
        " \n",
        "ax.legend(\n",
        "    handles=legend_handles, \n",
        "    loc='center left', \n",
        "    bbox_to_anchor=(1.05, 0.5),\n",
        "    ncol=1, \n",
        "    fancybox=True, \n",
        "    shadow=True, \n",
        "    title=\"Patterns Tested\",\n",
        "    title_fontsize=16,\n",
        "    fontsize='large', # Make legend text bigger. Can use 'medium', 'x-large', 'xx-large' or a numerical value (e.g., 12)\n",
        "    labelspacing=1.5, # Adjust vertical spacing between legend entries (default is 0.5)\n",
        "    handlelength=2.5, # Adjust length of the color patch/line in the legend\n",
        "    handletextpad=0.8, # Adjust space between the handle (color patch) and the text label\n",
        "    borderpad=0.5 # Adjust padding between the legend content and its border\n",
        "    \n",
        ") \n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1]) # Adjust layout to make space for the legend\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5396e98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONDUCT MODEL LEVEL ANALYSIS / GET SUMMARY SCORE FOR WHOLE MODEL\n",
        "\n",
        "def classify_model(method, sentences, torch_model, torch_tokenizer):\n",
        "    if method == \"linear_fit\":\n",
        "        patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "    elif method == \"best_fit\":\n",
        "        saved_file = pd.read_csv('data/best_fit_2.csv')\n",
        "\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    all_scores = []\n",
        "    final_scores = []\n",
        "        \n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        scores = np.zeros((num_layers, num_heads))\n",
        "        for i in range(num_layers):\n",
        "            for j in range(num_heads):\n",
        "                layer, head = i, j\n",
        "                inputs = torch_tokenizer(sentence, return_tensors=\"pt\")\n",
        "                len_seq = len(torch_tokenizer([sentence], return_tensors=\"pt\").input_ids[0])\n",
        "\n",
        "                X = []\n",
        "                y =  torch_model(**inputs, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "                if method == \"linear_fit\":\n",
        "                    for pattern in patterns:\n",
        "                        X.append(pattern(sentence, torch_tokenizer)[1].flatten())\n",
        "                    X_n = np.array(X).T\n",
        "                    y = y.flatten()\n",
        "\n",
        "                    reg = LinearRegression().fit(X_n, y)\n",
        "                    side_length = int(np.sqrt(len(y)))\n",
        "                    y = y.reshape((side_length, side_length))\n",
        "\n",
        "                    pred_att = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "                    pred_att = pred_att.reshape((side_length, side_length))\n",
        "                \n",
        "                elif method == \"best_fit\":\n",
        "                    matching_rows = saved_file[(saved_file['i'] == i) & (saved_file['j'] == j)]\n",
        "                    if not matching_rows.empty:\n",
        "                        best_pattern = matching_rows.loc[matching_rows['Score'].idxmax(), 'Pattern']\n",
        "                        func = globals()[best_pattern]\n",
        "                        _, pred_att = func(sentence, tokenizer)\n",
        "                    else:\n",
        "                        out = np.random.rand(len_seq, len_seq)\n",
        "                        pred_att =  out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "                elif method == \"random_baseline\":\n",
        "                    # out = np.random.rand(len_seq, len_seq)\n",
        "                    # pred_att =  out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "                    pred_att = np.zeros((len_seq, len_seq))\n",
        "                    pred_att[:, -1] = 1.0\n",
        "\n",
        "                jensonshannon_distances = []\n",
        "                for row_att, row_out in zip(y, pred_att):\n",
        "                    jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "                score = np.mean(jensonshannon_distances)\n",
        "                scores[layer, head] = score\n",
        "                \n",
        "        all_scores.append(scores)\n",
        "        final_scores.append(np.sum(scores))\n",
        "        print(f\"Processed sentence #{idx}/{len(sentences)}: Score: {np.sum(scores):.2f}\\n\\t->'{sentence}'\")\n",
        "\n",
        "    print(f\"Final Score: {sum(final_scores) / len(final_scores)}\")\n",
        "    return all_scores, final_scores\n",
        "\n",
        "classify_model(\"random_baseline\", sentences[:3], model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fdf02a",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Program Automation Efforts\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5510c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# AUTOMATICALLY GENERATE PATTERNS FOR A HEAD & GENERATE LLM PROMPT\n",
        "\n",
        "example_program_one = \"\"\"\n",
        "def dependencies(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
        "    len_seq = len(toks.input_ids[0]) /\n",
        "    out = np.zeros((len_seq, len_seq)) /\n",
        "    words = sentence.split() /\n",
        "    doc = nlp(\" \".join(words)) /\n",
        "    for stok in doc: /\n",
        "        parent_index = stok.i /\n",
        "        for child_stok in stok.children: /\n",
        "            child_index = child_stok.i /\n",
        "            out[parent_index+1, child_index+1] = 1 /\n",
        "            out[child_index+1, parent_index+1] = 1 /\n",
        "    out[0, 0] = 1 /\n",
        "    out[-1, 0] = 1 /\n",
        "    out += 1e-4 /\n",
        "    out = out / out.sum(axis=1, keepdims=True) /\n",
        "    return \"Dependency Parsing Pattern\", out /\n",
        "\"\"\"\n",
        "example_program_two = \"\"\"\n",
        "def same_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Same Token Pattern\", out\n",
        "\"\"\"\n",
        "example_program_three = \"\"\"\n",
        "def pos_alignment(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
        "    len_seq = len(toks.input_ids[0]) /\n",
        "    out = np.zeros((len_seq, len_seq)) /\n",
        "    # assign toks, input_ids, word_ids, len_seq, out, doc /\n",
        "    # use spacey to get pos_tags for tokens in docs [token.pos_ for token in doc] /\n",
        "    # for token in pos_tags: /\n",
        "    # loop through pos_tags and increment out[i,j] when pos_tags match /\n",
        "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention /\n",
        "    # Normalize out matrix by row (results in uniform attention) and return out /\n",
        "    # return 'Part of Speech Implementation 1', out /\n",
        "\"\"\"\n",
        "\n",
        "def generate_prompt(sentences, model, tokenizer, head_loc, top_k_ratio=0.1):\n",
        "    layer, head = head_loc\n",
        "    data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model.config.architectures[0],\n",
        "        \"examples\": []\n",
        "    }\n",
        "\n",
        "    def handle_score(score):\n",
        "        # convert to percentage with 0 decimal places\n",
        "        return \"{:.0f}\".format(score * 100)\n",
        "        \n",
        "    def scrape_head(att, tokens, top_k_ratio, ignore_special=True):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append(f\"[{str(tokens[i])}|{str(tokens[j])}:{handle_score(score)}]\")\n",
        "        top_activations_str = \" \".join(top_activations).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "        return top_activations_str\n",
        "    \n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=top_k_ratio)\n",
        "        item = {f\"sentence {idx}\": \" \".join(tokens), \"sentence attention\": top_activations}\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "    data = json.dumps(data, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    Using the following pieces of data based on {len(sentences)} sentences, generate three hypothesises about the linguistic role the following head is responsible for based on patterns\n",
        "    in the activations.  Then, choose the most fitting hypothesis for the head function using examples from the data. Finally, using the linguistic hypothesis you determine, \n",
        "    write a python function which takes in a sentence and tokenizer as parameters and outputs the name of the pattern you hypothesize along with a predicted_matrix (size: token_len * token_len), which is the \n",
        "    rule encoded matrix mirroring attention patterns you'd predict for any given sentence for Layer {layer}, Head {head}. Feel free to encode complex functions but write the simplest algorithm that captures your \n",
        "    observed pattern. You must respond to this prompt in JSON in the form \"{{\"hypothesis\": \"...\", \"program\": \"...\"}} with your chosen hypothesis. Think carefully before generating any code.\n",
        "    The first portion of your response has key \"hypothesis\" with the title of the hypothesis and the second portion of your response with key \"program\" should have valid python code starting with ```python and including imports. These patterns can be simple or \n",
        "    complex.  For uniformity, the first three lines of your function should be 'toks = tokenizer([sentence], return_tensors=\"pt\") len_seq = len(toks.input_ids[0]) out = np.zeros((len_seq, len_seq))'.\n",
        "    Make sure the token sequences from your tokenizer and spaCy (if you must use spaCy) are aligned via a dictionary if necessary, because they split text differently. Make sure you generalize your hypothesis pattern to any sentence. Functions can almost \n",
        "    always be expressed in fewer than 50 lines of code. As examples, it has been discovered one head is responsible for the complex task of dependency parsing. It's simplistic predicted pseudocode looks like: \n",
        "    {example_program_one}. Example 2: '''{example_program_two}''' Example 3: '''{example_program_three}'''. DATA: {data}\"\"\"\n",
        "    return ' '.join(prompt.strip().split())\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "layer, head = 5, 7\n",
        "prompt = generate_prompt(generic_sentences[:25], model, tokenizer, (layer, head), 0.025)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5704e094",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using Gemini, GPT-4o, Claude, Deepseek\n",
        "# API needs long contexts and free access\n",
        "# Source to get API keys is \"usage\" key\n",
        "\n",
        "load_dotenv()\n",
        "API_CONFIGS = {\n",
        "    \"gemini\": {\n",
        "        \"model\": \"gemini\",\n",
        "        \"url\": \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\",\n",
        "        \"key\": os.getenv(\"GEMINI\"),\n",
        "        \"headers_fn\": lambda key: {\"Content-Type\": \"application/json\", \"X-goog-api-key\": key},\n",
        "        \"payload_fn\": lambda prompt: {\n",
        "            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "            \"generationConfig\": {\"response_mime_type\": \"application/json\"}\n",
        "        },\n",
        "        \"usage\": \"https://aistudio.google.com/apikey\"\n",
        "    },\n",
        "    \"openai\": {\n",
        "        \"model\": \"openai\",\n",
        "        \"url\": \"https://api.openai.com/v1/responses\",\n",
        "        \"key\": os.getenv(\"OPENAI\"),\n",
        "        \"headers_fn\": lambda key: {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\": \"gpt-4.1\", \"input\": prompt},\n",
        "        \"usage\": \"https://platform.openai.com/account/api-keys\"\n",
        "    },\n",
        "    \"claude\": {\n",
        "        \"model\": \"claude\",\n",
        "        \"url\": \"https://api.anthropic.com/v1/messages\",\n",
        "        \"key\": os.getenv(\"CLAUDE\"),\n",
        "        \"headers_fn\": lambda key: {\"x-api-key\": key, \"Content-Type\": \"application/json\", \"Anthropic-Version\":\"2023-06-01\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\":\"claude-sonnet-4-20250514\", \"messages\":[{\"role\":\"user\",\"content\":prompt}]},\n",
        "        \"usage\": \"https://platform.claude.com/api_keys\"\n",
        "    },\n",
        "    \"deepseek\": {\n",
        "        \"model\": \"deepseek\",\n",
        "        \"url\": \"https://api.deepseek.com/chat/completions\",\n",
        "        \"key\": os.getenv(\"DEEPSEEK\"),\n",
        "        \"headers_fn\": lambda key: {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\": \"deepseek-chat\", \"input\": prompt, \"max_tokens\": 1000},\n",
        "        \"usage\": \"https://platform.deepseek.com/api_keys\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2112fba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAVE AUTOMATED HYPOTHESIS + VALIDATE GENERATED PROGRAM SYNTHESIS CODE\n",
        "\n",
        "def parse_llm_idea(prompt, config=\"YOUR_API_CONFIG\", verbalize=True):\n",
        "    def make_request():\n",
        "        headers = config[\"headers_fn\"](config[\"key\"])\n",
        "        payload = config[\"payload_fn\"](prompt)\n",
        "        response = requests.post(config[\"url\"], headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if config[\"model\"] == \"gemini\":\n",
        "            data = response.json()\n",
        "            output = data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        if config[\"model\"] == \"openai\":\n",
        "            pass\n",
        "        if config[\"model\"] == \"claude\":\n",
        "            data = response.json()\n",
        "            output = data[\"content\"][\"text\"]\n",
        "        if config[\"model\"] == \"deepseek\":\n",
        "            pass\n",
        "\n",
        "        return output\n",
        "    \n",
        "    output = make_request()\n",
        "\n",
        "    try:\n",
        "        result = json.loads(output)\n",
        "\n",
        "        if type(result) is list: result = result[0]\n",
        "        hypothesis = result.get(\"hypothesis\", \"\")\n",
        "        program = result.get(\"program\", \"\")\n",
        "\n",
        "        if program.startswith(\"```python\"): program = program[9:]\n",
        "        if program.endswith(\"```\"): program = program[:-3]\n",
        "        program = program.strip()\n",
        "\n",
        "        if verbalize: print(\"Hypothesis, Explanation & Program successfully parsed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Parsing API failed: {str(e)}\")\n",
        "        return str(e)\n",
        "\n",
        "    return hypothesis, program\n",
        "\n",
        "config = API_CONFIGS[\"gemini\"] \n",
        "parse_llm_idea(prompt, config=config, verbalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be7aad1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SIMPLE AUTOMATION OF PIPELINE FOR ANALYZING ALL HEADS & WRITING/SAVING PROGRAMS\n",
        "\n",
        "def automation_pipeline(model, tokenizer, sentences, API_KEY, save_data=True, evaluate=False):\n",
        "    heads = model.config.num_attention_heads\n",
        "    layers = model.config.num_hidden_layers\n",
        "    prompts, programs = [], []\n",
        "\n",
        "    for layer in range(layers):\n",
        "        # if layer == 0: continue\n",
        "        if save_data:\n",
        "            # save prompts:\n",
        "            prompt_path = f\"automation_2/prompts/{layer}/\"\n",
        "            os.makedirs(prompt_path, exist_ok=True)\n",
        "\n",
        "            # save programs:\n",
        "            program_path = f\"automation_2/llm_code/code_layer_{layer}/\"\n",
        "            os.makedirs(program_path, exist_ok=True)\n",
        "\n",
        "            # save scores:\n",
        "            if evaluate:\n",
        "                score_path = f\"automation_2/scores/{layer}/\"\n",
        "                os.makedirs(score_path, exist_ok=True)\n",
        "\n",
        "        for head in range(heads):\n",
        "            # if head < 9: continue\n",
        "            if (layer, head) not in failed_programs:\n",
        "                continue\n",
        "            prompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.1)\n",
        "            hypothesis, explanation, program = parse_llm_idea(prompt, API_KEY, output=False)\n",
        "            print(f\"Analyzed Layer {layer}, Head {head} | Hypothesis ~ {hypothesis} \")\n",
        "\n",
        "            prompts.append(prompt)\n",
        "            programs.append(program)\n",
        "\n",
        "            if save_data:\n",
        "                with open(f\"{prompt_path}/{layer}_{head}_prompt.txt\", \"w\") as f: f.write(prompt)\n",
        "                with open(f\"{program_path}/{head}_output.py\", \"w\") as f: f.write(program)\n",
        "\n",
        "        if evaluate: \n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "automation_pipeline(model, tokenizer, generic_sentences[:10], API_KEY=API_KEY, save_data=True, evaluate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4ca44f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib.util\n",
        "import types\n",
        "\n",
        "def validate_program(program_path, model, tokenizer, layer, head, sentences):       \n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        spec.loader.exec_module(module)\n",
        "    except Exception as e:\n",
        "        print(f\"Program loading failed: {str(e)}\")\n",
        "        return str(e)\n",
        "\n",
        "    for attr_name in dir(module):\n",
        "        attr = getattr(module, attr_name)\n",
        "        if isinstance(attr, types.FunctionType):\n",
        "            program = attr\n",
        "            break\n",
        "\n",
        "    # try:\n",
        "    score = score_prediction(model, tokenizer, (layer, head), program, sentences, distance=\"jsd\", output=True)\n",
        "    return score\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Program validation failed: {str(e)}\")\n",
        "    #     return str(e)\n",
        "\n",
        "python_path = r\"automation_results_gemini\\llm_code\\programs-layer_0\\0_output.py\"\n",
        "feedback = validate_program(python_path, model, tokenizer, layer, head, sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e9c1ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CALCULATE AND SAVE SCORES FOR AUTOMATICALLY GENERATED PROGRAMS\n",
        "\n",
        "import importlib.util\n",
        "import types\n",
        "\n",
        "scores = []\n",
        "failed_programs = []\n",
        "for layer in range(12):\n",
        "    # if layer != 11: continue\n",
        "    code_path = f\"automation_2/llm_code/code_layer_{layer}/\"\n",
        "    for j in range(12):\n",
        "        # if j != 11: continue\n",
        "        filename = f\"{j}_output.py\"\n",
        "        program_path = os.path.join(code_path, filename)\n",
        "        if not os.path.exists(program_path): continue\n",
        "        score_path = f\"automation_2/scores/{layer}_{j}_score.txt\"\n",
        "        os.makedirs(os.path.dirname(score_path), exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_j{j}\", program_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "        except Exception as e:\n",
        "            print(f\"Layer {layer}, Head {j} - Error: {e}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"Error loading module: {e}\")\n",
        "            failed_programs.append((layer, j))\n",
        "            scores.append(-1)\n",
        "            continue\n",
        "\n",
        "        for attr_name in dir(module):\n",
        "            attr = getattr(module, attr_name)\n",
        "            if isinstance(attr, types.FunctionType):\n",
        "                program = attr\n",
        "                break\n",
        "\n",
        "        try:\n",
        "            score = score_prediction(model, tokenizer, (layer, j), program, generic_sentences[0], distance=\"jsd\", output=False)\n",
        "            print(f\"Layer {layer}, Head {j} - Score: {score:.2f}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"{score:.2f}\")\n",
        "            scores.append(score)\n",
        "        except Exception as e:\n",
        "            print(f\"Layer {layer}, Head {j} - Error: {e}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"Error during scoring: {e}\")\n",
        "            failed_programs.append((layer, j))\n",
        "            scores.append(-1)\n",
        "            continue\n",
        "\n",
        "num_scored = len([s for s in scores if s != -1 and not np.isnan(s)])\n",
        "print(f\"Number of Successfully Scored Heads: {num_scored} out of {len(scores)}\")\n",
        "\n",
        "avg_score = sum([s for s in scores if s != -1 and not np.isnan(s)]) / len([s for s in scores if s != -1 and not np.isnan(s)])\n",
        "print(f\"Average Score (excluding errors): {avg_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa549158",
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = \"Greens\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "im2 = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im2.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im2, ax=ax)\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
        "ax.set_yticklabels([i for i in range(12)])\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.title(\"Automation Scores (No Refinement)\\n[Gray Blocks returned errors]\\n\", weight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3a0037",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Hypothesis Structure Experiments\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c820d5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from programs import *\n",
        "\n",
        "programs = [next_attention, previous_attention, same_attention, punctuation_attention, dependencies,\n",
        "            last_token_attention, uniform_attention, cls_attention, eos_attention, pos_alignment,\n",
        "            special_token_attention, repeated_attention, noun_modifier_attention, pronoun_attention,\n",
        "            single_token_attention, root_cluster_attention]\n",
        "\n",
        "sentence_data = sentences[:25]\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def program_similarity(att_one, att_two):\n",
        "    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "        p = np.clip(p, 1e-12, 1.0)\n",
        "        q = np.clip(q, 1e-12, 1.0)\n",
        "        p /= p.sum()\n",
        "        q /= q.sum()\n",
        "        m = 0.5 * (p + q)\n",
        "        return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
        "\n",
        "    jensonshannon_distances = []\n",
        "    for row_att, row_out in zip(att_one, att_two):\n",
        "        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "    score = np.mean(jensonshannon_distances)\n",
        "    return score\n",
        "\n",
        "x = len(programs)\n",
        "S = np.zeros((x, x))\n",
        "for i in range(x):\n",
        "    print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
        "    for j in range(x):\n",
        "        if i != j:\n",
        "            similarities = []\n",
        "            program_one = programs[i]\n",
        "            program_two = programs[j]\n",
        "\n",
        "            for sentence in sentence_data:\n",
        "                h1, activations_one = program_one(sentence, tokenizer)\n",
        "                h2, activations_two = program_two(sentence, tokenizer)\n",
        "                similarities.append(program_similarity(activations_one, activations_two))\n",
        "            \n",
        "            S[i, j] = np.mean(similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "370edad8",
      "metadata": {},
      "outputs": [],
      "source": [
        "S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1a3b62",
      "metadata": {},
      "outputs": [],
      "source": [
        "def group_similar_programs(programs, S, threshold=0.6):\n",
        "    groups, used = [], set()\n",
        "    for i in range(len(programs)):\n",
        "        if i in used: continue\n",
        "        group = [i]\n",
        "        used.add(i)\n",
        "        \n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for group_member in group:\n",
        "                for j in range(len(programs)):\n",
        "                    if j not in used and S[group_member, j] < threshold:\n",
        "                        group.append(j)\n",
        "                        used.add(j)\n",
        "                        changed = True\n",
        "        groups.append([programs[idx].__name__ for idx in group])\n",
        "    \n",
        "    return groups\n",
        "\n",
        "groups = group_similar_programs(programs, S, threshold=0.6)\n",
        "for i, group in enumerate(groups):\n",
        "    print(f\"Group {i+1}: {group}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b464ed4",
      "metadata": {},
      "outputs": [],
      "source": [
        "name_to_idx = {fn.__name__: i for i, fn in enumerate(programs)}\n",
        "new_order = [name_to_idx[name] for group in groups for name in group]\n",
        "S_grouped = S[np.ix_(new_order, new_order)]\n",
        "colors = \"Purples_r\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
        "im2 = ax.imshow(S_grouped, cmap=colors, aspect='auto')\n",
        "# ax.set_axis_off()\n",
        "ax.set_xticks(range(len(programs)))\n",
        "ax.set_yticks(range(len(programs)))\n",
        "ax.set_xticklabels([p.__name__ for p in programs], rotation=90)\n",
        "ax.set_yticklabels([p.__name__ for p in programs])\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.title(\"Similarity Matrix\\n\", weight='bold')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
