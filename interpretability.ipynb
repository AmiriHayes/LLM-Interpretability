{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6443588e",
      "metadata": {
        "id": "6443588e"
      },
      "source": [
        "Author: Amiri Hayes \\\n",
        "Date Updated: 7/15/25 \\\n",
        "Title: ViewLLM\n",
        "\n",
        "Starter code !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99",
      "metadata": {
        "id": "c44807ec-dc14-4508-adaf-3cbb7e4f7e99"
      },
      "outputs": [],
      "source": [
        "# package installs if necessary\n",
        "!pip install torch spacy nltk tqdm transformers datasets scikit-learn\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3e36de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb3e36de",
        "outputId": "5c233452-f8b9-4137-f8b9-cd2881649f94"
      },
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import requests\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import Optional, Tuple, Callable\n",
        "# import matplotlib.patches as mpatches\n",
        "# from matplotlib.colors import PowerNorm, ListedColormap\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
        "from openai import OpenAI\n",
        "load_dotenv(find_dotenv())\n",
        "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"),)\n",
        "print(\"Imports completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012daf28",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Data & Pattern Initialization Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbe22fd",
      "metadata": {
        "id": "7cbe22fd"
      },
      "outputs": [],
      "source": [
        "# IMPORT THE PROGRAM DATABASE:\n",
        "\n",
        "from programs import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d51a2fa",
      "metadata": {
        "id": "9d51a2fa"
      },
      "outputs": [],
      "source": [
        "# GENERATE & VISUALIZE ATTENTION PATTERN SCORES FROM PROGRAMS\n",
        "\n",
        "def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    q = np.clip(q, 1e-12, 1.0)\n",
        "    p /= p.sum()\n",
        "    q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
        "\n",
        "def score_prediction(torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: Tuple[int, int], pattern: Callable, sentence_1: str, sentence_2: Optional[str] = None, distance=\"jsd\", output=False):\n",
        "    layer, head = head_loc\n",
        "    tokens = torch_tokenizer(sentence_1, return_tensors=\"pt\")\n",
        "\n",
        "    if torch_model.config.is_encoder_decoder:\n",
        "        decoder_input_ids = tokens[\"input_ids\"]\n",
        "        outputs = torch_model(input_ids=tokens[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "        att = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "        name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    else:\n",
        "        if sentence_2 and pattern.__name__ == \"chainofthought_pattern\":\n",
        "            name = \"Chain of Thought Pattern\"\n",
        "            tokens_2 = torch_tokenizer(sentence_2, return_tensors=\"pt\")\n",
        "\n",
        "            att = torch_model(**tokens_2, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            pred_att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "            if output: print(\"RUNNING FIRST WITH NO HINT\")\n",
        "            question, answer, vector_att = chainofthought_pattern(sentence_1, torch_tokenizer, pred_att, hint=False)\n",
        "            if output: print(\"RUNNING AFTER WITH A HINT\")\n",
        "            question, answer, vector_pred_att = chainofthought_pattern(sentence_2, torch_tokenizer, att, hint=True)\n",
        "\n",
        "            att, pred_att = vector_att.copy(), vector_pred_att.copy()\n",
        "        else:\n",
        "            att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "            if pattern.__name__ == \"linear_fit\":\n",
        "                name, pred_att = pattern(sentence_1, torch_tokenizer, idx=0)\n",
        "            else: name, pred_att = pattern(sentence_1, torch_tokenizer)\n",
        "\n",
        "    if distance == \"raw\":\n",
        "        score = np.abs(att - pred_att).sum()\n",
        "    elif distance == \"jsd\":\n",
        "        jensonshannon_distances = []\n",
        "        for row_att, row_out in zip(att, pred_att):\n",
        "            jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "        score = np.mean(jensonshannon_distances)\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\":\n",
        "        score = np.sqrt(js_divergence(att, pred_att))\n",
        "\n",
        "    if output == \"cot\":\n",
        "        colors = \"inferno\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 9))\n",
        "        axes[0].plot(att, color=plt.get_cmap(colors)(0.6))\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        axes[1].plot(pred_att, color=plt.get_cmap(colors)(0.9))\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        bound_axes = False\n",
        "        for i in range(2):\n",
        "            axes[i].set_xlabel(\"Token Index\")\n",
        "            axes[i].set_ylabel(\"Attention Weight\")\n",
        "            axes[i].grid(True)\n",
        "            if bound_axes:\n",
        "                axes[i].set_ylim(0, 1)\n",
        "                axes[i].set_xlim(0, len(att) - 1)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        question_chart = question.replace(\".\", \".\\n\")\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nQuestion: \\\"{question_chart}\\n\\nAnswer: \\\"{answer}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "    \n",
        "    toks = torch_tokenizer([sentence_1], return_tensors=\"pt\")\n",
        "    token_ids = toks[\"input_ids\"][0]\n",
        "    tokens = torch_tokenizer.convert_ids_to_tokens(token_ids)\n",
        "\n",
        "    if output == True:\n",
        "        colors=\"Greens\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        im1 = axes[0].imshow(att, cmap=colors, aspect='auto')\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        fig.colorbar(im1, ax=axes[0])\n",
        "        im2 = axes[1].imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        fig.colorbar(im2, ax=axes[1])\n",
        "        for i in range(2):\n",
        "            axes[i].set_xticks(range(len(tokens)))\n",
        "            axes[i].set_yticks(range(len(tokens)))\n",
        "            axes[i].set_xticklabels(tokens, rotation=90)\n",
        "            axes[i].set_yticklabels(tokens)\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nSentence: \\\"{sentence_1}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"optimal\":\n",
        "        colors = \"Oranges\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        ax.set_axis_off()\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    elif output == \"actual\":\n",
        "        colors = \"Reds\"\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
        "        im2 = ax.imshow(att, cmap=colors, aspect='auto')\n",
        "        ax.set_title(\"Example Head Attention for Pattern\")\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n899pxRzSWRe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n899pxRzSWRe",
        "outputId": "470ad98d-f61e-4b36-bde9-e6c2bb80882d"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (1):\n",
        "\n",
        "file = 'data/small_text.csv'\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "sentences = []\n",
        "for paragraph in df['text']:\n",
        "    sentences.extend(sent_tokenize(paragraph))\n",
        "\n",
        "sentences = sentences[:10_000]\n",
        "print(\"Sentences from Tiny Stories Dataset:\")\n",
        "for sentence in sentences[20:30]:\n",
        "  print(f\"\\t{sentence}\")\n",
        "\n",
        "df_json = pd.read_json('data/generic_sentences.json')\n",
        "generic_sentences = df_json[0].tolist()\n",
        "print(\"\\nGeneric Sentences:\")\n",
        "for sentence in generic_sentences[:10]:\n",
        "  print(f\"\\t{sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0adb7458",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA (2):\n",
        "\n",
        "math_data = pd.read_json('data/math_problems_results.jsonl', lines=True)\n",
        "\n",
        "filtered_results = math_data[\n",
        "    (math_data['consistency'] == \"False\") &\n",
        "    (math_data['evaluated_answer_nohint'] != \"DNF: llm did not finish\") &\n",
        "    (math_data['evaluated_answer_hint'] != \"DNF: llm did not finish\")\n",
        "]\n",
        "\n",
        "answers_nohint = filtered_results['answer_nohint'].tolist()\n",
        "answers_hint = filtered_results['answer_hint'].tolist()\n",
        "prefix = \"system\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nuser\\n\\n\"\n",
        "\n",
        "prompts = []\n",
        "for s1, s2 in zip(answers_nohint, answers_hint):\n",
        "    if s1.startswith(prefix): s1 = s1[len(prefix):]\n",
        "    if s2.startswith(prefix): s2 = s2[len(prefix):]\n",
        "\n",
        "    i_suffix_s1 = s1.find(\"assistant\")\n",
        "    if i_suffix_s1 != -1: s1 = s1[:i_suffix_s1].strip()\n",
        "\n",
        "    i_suffix_s2 = s2.find(\"assistant\")\n",
        "    if i_suffix_s2 != -1: s2 = s2[:i_suffix_s2].strip()\n",
        "\n",
        "    if s1 and s2: prompts.append((s1, s2))\n",
        "\n",
        "print(len(prompts), \"relevant prompts loaded from math problems dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1015c0ec",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "  Analysis Helper Functions\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff49f93",
      "metadata": {
        "id": "1ff49f93",
        "outputId": "d8f00b5c-d395-4564-885c-d1f65e5c815d"
      },
      "outputs": [],
      "source": [
        "# ANALYZE PATTERN AT LAYER AND HEAD\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "sentence = \"I like apples and I like bananas. I like apples more though.\"\n",
        "\n",
        "models = [\"bert-base-uncased\", \"openai-community/gpt2\", \"google-t5/t5-small\", \"Meta-Llama-3.1-8B-Instruct\"]\n",
        "\n",
        "i = 2\n",
        "name = models[i]\n",
        "\n",
        "if i in [0,1,2]: # small models ( <400M Params )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "    model = AutoModel.from_pretrained(name, output_attentions=True)\n",
        "    model.eval()\n",
        "\n",
        "elif i == 3: # big model & cot ( 8B Params, Load from Compute Node )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"/raid/lingo/models/Meta-Llama-3.1-8B-Instruct/\")\n",
        "    model.eval()\n",
        "\n",
        "    prompt_num = 0\n",
        "    sentence = prompts[prompt_num][0]  # Use the prompt's first sentence (no hint)\n",
        "    sentence_with_hint = prompts[prompt_num][1]  # Use prompt's second sentence (hint)\n",
        "\n",
        "import numpy as np\n",
        "from transformers import PreTrainedTokenizerBase\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "def pronoun_antecedent_reference(sentence: str, tokenizer: PreTrainedTokenizerBase) -> str:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "    token_pos_to_idx = {token.idx: i for i, token in enumerate(doc)}\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"PRON\":\n",
        "            antecedents = [anc for anc in token.ancestors if anc.dep_ in {\"nsubj\", \"dobj\", \"pobj\"}]\n",
        "            for antecedent in antecedents:\n",
        "                token_idx = token_pos_to_idx.get(token.idx, -1) + 1\n",
        "                antecedent_idx = token_pos_to_idx.get(antecedent.idx, -1) + 1\n",
        "                if 0 < token_idx < len_seq and 0 < antecedent_idx < len_seq:\n",
        "                    out[token_idx, antecedent_idx] = 1.0\n",
        "                    out[antecedent_idx, token_idx] = 1.0\n",
        "\n",
        "    for row in range(len_seq):\n",
        "        if out[row].sum() == 0:\n",
        "            out[row, -1] = 1.0\n",
        "\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Pronoun-Antecedent Reference\", out\n",
        "\n",
        "\n",
        "layer, head = 3,2\n",
        "# sentence = \"he said no, he did not, he will not, he feels weirdly good about it\"\n",
        "score_prediction(model, tokenizer, (layer, head), pronoun_antecedent_reference, sentence, distance=\"jsd\", output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd29726",
      "metadata": {
        "id": "9fd29726",
        "outputId": "feeb18a4-11de-4928-9582-fdf4b4c816df"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (1 SENTENCE)\n",
        "\n",
        "def visualize_full_model(sentence: str, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9, sentence_2: Optional[str] = None) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    num_heads = model.config.num_attention_heads\n",
        "    model_viz = np.zeros((num_layers, num_heads))\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_heads):\n",
        "            score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "            model_viz[i, j] = score\n",
        "    print(f\"Best Score: {min(map(min, model_viz)):.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=model_viz.min(), vmax=model_viz.max())\n",
        "    plt.imshow(model_viz, cmap='Greens_r', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_three = list(zip(*np.unravel_index(np.argsort(model_viz, axis=None)[:3], model_viz.shape)))\n",
        "    for tuple_val in top_three:\n",
        "        layer, head = tuple_val\n",
        "        score = model_viz[layer, head]\n",
        "        print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return model_viz, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "pattern_name = \"Repeated Attention Pattern\"\n",
        "underlined_name_unicode = \"\".join([char + '\\u0332' for char in pattern_name])\n",
        "visualize_full_model(sentence, model, tokenizer, repeated_attention, title=f\"Top Heads: {underlined_name_unicode} | {model_name}\\nSentence: \\\"{sentence}\\\"\\n\", bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G90Rgk6fVKIZ",
      "metadata": {
        "id": "G90Rgk6fVKIZ",
        "outputId": "56ce27bd-6ea5-4f3d-cbf7-f6dc1b69f015"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (MULTIPLE SENTENCES)\n",
        "\n",
        "def visualize_full_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, pattern: Callable, title: str, bias_towards_best: float = 0.9) -> Tuple[np.ndarray, list]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    header = [\"Layer\", \"Head\", \"Score\"]\n",
        "    csv_file_name = \"scores.csv\"\n",
        "    file_exists = os.path.exists(csv_file_name)\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "    \n",
        "        average_score = np.zeros((num_layers, num_heads))\n",
        "        for sentence in sentences:\n",
        "            sentence_1 = sentence[0]  # first sentence (no hint)\n",
        "            sentence_2 = sentence[1]  # second sentence (hint)\n",
        "            model_score = np.zeros((num_layers, num_heads))\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    score = score_prediction(model, tokenizer, (i, j), chainofthought_pattern, sentence_1, sentence_2, distance=\"jsd\", output=False)\n",
        "                    writer.writerow([i, j, f\"{score:.2f}\"])\n",
        "                    print(f\"Layer {i}, Head {j} - Score: {score:.2f}\")\n",
        "                    model_score[i, j] = score\n",
        "            average_score += model_score\n",
        "        average_score /= len(sentences)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=average_score.min(), vmax=average_score.max())\n",
        "    plt.imshow(average_score, cmap='Reds', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if pattern.__name__ == \"chainofthought_pattern\": \n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score.ravel())[::-1][:3], average_score.shape))) # highest scores\n",
        "    else:\n",
        "        top_three = list(zip(*np.unravel_index(np.argsort(average_score, axis=None)[3:], average_score.shape))) # lowest scores\n",
        "        top_three = np.sort(average_score)\n",
        "\n",
        "    for tuple_val in top_three:\n",
        "            layer, head = tuple_val\n",
        "            score = average_score[layer, head]\n",
        "            print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return average_score, top_three\n",
        "\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentences_zipped = list(zip(answers_nohint[:5], answers_hint[:5]))\n",
        "visualize_full_model(sentences_zipped, model, tokenizer, chainofthought_pattern, title=\"Top Heads: Chain-of_Thought Evaluation [AVERAGE]\\n\"+model_name, bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eTFYV1Gpzv",
      "metadata": {
        "id": "b8eTFYV1Gpzv"
      },
      "outputs": [],
      "source": [
        "# FILTER SENTENCES TO TARGET SPECIFIC PATTERNS\n",
        "\n",
        "def filterer(sentences: list[str], length_matters: bool=False, punctuation_matters: bool=False, duplicates: bool=False) -> list[str]:\n",
        "    filtered_sentences = []\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    for sentence in sentences:\n",
        "        passes_all_active_filters = True\n",
        "        if length_matters:\n",
        "            if len(sentence.split()) < 5:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if punctuation_matters:\n",
        "            punctuation_count = sum(1 for char in sentence if char in punctuation_set)\n",
        "            if punctuation_count < 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if duplicates:\n",
        "            set_l = len(set(sentence.split()))\n",
        "            sent_1 = len(sentence.split())\n",
        "            if set_l >= sent_1 - 3:\n",
        "                passes_all_active_filters = False\n",
        "                continue\n",
        "        if passes_all_active_filters: filtered_sentences.append(sentence)\n",
        "    return filtered_sentences\n",
        "\n",
        "filtered_sentences = filterer(generic_sentences, length_matters = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mR56LNrRd37k",
      "metadata": {
        "id": "mR56LNrRd37k",
        "outputId": "4b42cff5-d843-439d-8409-d5c47e9b87a7"
      },
      "outputs": [],
      "source": [
        "# ANALYZE HEAD PATTERN ON SELECTED SENTENCES\n",
        "\n",
        "def visualize_highest_head(sentences: list[str], top_n:  int, torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, head_loc: tuple[int, int], pattern: Callable):\n",
        "    layer, head = head_loc\n",
        "    scores = []\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        score = score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, distance=\"jsd\", output=False)\n",
        "        scores.append(score)\n",
        "\n",
        "    indexed_scores = list(enumerate(scores))\n",
        "    sorted_scores = sorted(indexed_scores, key= lambda x: x[1])\n",
        "    top_scores = sorted_scores[:top_n]\n",
        "\n",
        "    for idx, score in top_scores:\n",
        "        print(f\"Sentence #{idx} Score: {score}\")\n",
        "        sentence = sentences[idx]\n",
        "        score_prediction(torch_model, torch_tokenizer, (layer, head), pattern, sentence, output=True)\n",
        "    return 0\n",
        "\n",
        "layer, head = 7, 8\n",
        "visualize_highest_head(filtered_sentences, 3, model, tokenizer, (layer, head), punctuation_attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be980bdc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS\n",
        "\n",
        "def classify_whole_model(sentences: list[str], torch_model: PreTrainedModel, torch_tokenizer: PreTrainedTokenizerBase, patterns: list[Callable]) -> dict[Tuple[int, int], Tuple[str, float]]:\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "    activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "    header = [\"i\", \"j\", \"Pattern\", \"Score\"]\n",
        "    \n",
        "    csv_file_name = \"data/best_fit_t5.csv\"\n",
        "    with open(csv_file_name, 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        file_exists = os.path.exists(csv_file_name)\n",
        "        if not file_exists:\n",
        "            writer.writerow(header)\n",
        "\n",
        "        for pattern in patterns:\n",
        "            print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "            all_scores = []\n",
        "            for idx, sentence in enumerate(sentences):\n",
        "                if idx % 20 == 0: print(f\"\\tProcessing sentence {idx}/{len(sentences)}\")\n",
        "                for i in range(num_layers):\n",
        "                    for j in range(num_heads):\n",
        "                        score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                        if score < 0.55: print(f\"sentence {idx}\", i, j, score)\n",
        "                        all_scores.append(score)\n",
        "\n",
        "            average_scores = np.array(all_scores).reshape(len(sentences), num_layers * num_heads).mean(axis=0)\n",
        "            head_performance = average_scores.reshape(num_layers, num_heads)\n",
        "            print(head_performance)\n",
        "\n",
        "            ix, jx = np.where(head_performance < 0.55)\n",
        "            pairs = list(zip(ix, jx))\n",
        "\n",
        "            for (ix, jx) in pairs:\n",
        "                print(ix, jx, head_performance[ix, jx])\n",
        "                # if key not in activations or pattern_score < activations[key][1]:\n",
        "                #     activations[key] = (pattern.__name__, pattern_score)\n",
        "                writer.writerow([ix, jx, pattern.__name__, head_performance[ix, jx]])\n",
        "\n",
        "\n",
        "            # print(len(avg_scores))\n",
        "            # pattern_score = np.mean(avg_scores)\n",
        "            # print(i, j, pattern_score)\n",
        "\n",
        "            # if pattern_score > 0.5: continue\n",
        "            # key = (i, j)\n",
        "\n",
        "            # if key not in activations or pattern_score < activations[key][1]:\n",
        "            #     activations[key] = (pattern.__name__, pattern_score)\n",
        "            # writer.writerow([i, j, pattern.__name__, pattern_score])\n",
        "\n",
        "    return activations\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"openai-community/gpt2\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "\n",
        "# patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "activations = classify_whole_model(generic_sentences[:5], model, tokenizer, patterns)\n",
        "print(activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08432fb2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DETERMINE BEST FIT PATTERNS FOR ALL HEADS (CONTINUED)\n",
        "\n",
        "model = AutoModel.from_pretrained(\"google-t5/t5-small\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "\n",
        "torch_model = model\n",
        "torch_tokenizer = tokenizer\n",
        "\n",
        "num_layers = torch_model.config.num_hidden_layers\n",
        "num_heads = torch_model.config.num_attention_heads\n",
        "activations = {}  # key: (i, j), value: (pattern_name, score)\n",
        "\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, eos_attention, special_token_attention, pos_alignment, dependencies]\n",
        "\n",
        "short = sentences[:8]\n",
        "csv_file_name = \"data/best_fit_t5.csv\"\n",
        "with open(csv_file_name, 'a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for pattern in patterns:\n",
        "        print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
        "        avg_score = []\n",
        "        for idx, sentence in enumerate(short):\n",
        "            print(f\"\\tProcessing sentence {idx}/{len(short)}\")\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    if i != 3 or j != 9: continue\n",
        "                    score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
        "                    if score < 0.55:\n",
        "                        avg_score.append((idx, pattern.__name__, i, j, score))\n",
        "        \n",
        "        score_dict = {}\n",
        "        for idx, pattern_name, i, j, score in avg_score:\n",
        "            score_dict.setdefault((i, j), []).append((pattern_name, score))\n",
        "        for (i, j), values in score_dict.items():\n",
        "            scores = [score for _, score in values]\n",
        "            avg_score_val = sum(scores) / len(scores)\n",
        "            pattern_name = values[0][0]\n",
        "            activations[(i, j)] = (pattern_name, avg_score_val)\n",
        "            print(f\"Layer {i}, Head {j} - Score: {avg_score_val:.2f}\")\n",
        "            writer.writerow([i, j, pattern.__name__, avg_score_val])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8374c077",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANALYZE EFFECT OF LINEAR WEIGHTS ON ATTENTION ACTIVATION ACCURACY\n",
        "\n",
        "def generate_dataset(patterns: list[Callable], model: PreTrainedModel, tokenizer: PreTrainedTokenizerBase, sentences: list[str], layer_head: tuple[int, int]):\n",
        "    layer, head = layer_head\n",
        "    X_data, y_data = [], []\n",
        "    print(\"Generating dataset for Layer\", layer, \", Head\", head)\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            attn = outputs.attentions[layer][0, head]\n",
        "        X_i_list = []\n",
        "        for pattern in patterns:\n",
        "            _, X_i = pattern(sentence, tokenizer)\n",
        "            X_i = torch.tensor(X_i, dtype=torch.float32)\n",
        "            X_i_list.append(X_i)\n",
        "        X_data.append(X_i_list)\n",
        "        y_data.append(attn)\n",
        "\n",
        "    torch.save({'X': X_data, 'y': y_data}, \"data/attention_dataset.pt\")\n",
        "    print(\"Dataset generated and saved to 'data/attention_dataset.pt'.\")\n",
        "\n",
        "def train_linearregression() -> pd.DataFrame:\n",
        "    data = torch.load(\"data/attention_dataset.pt\")\n",
        "    X, y = data['X'], data['y']\n",
        "    X, y = data['X'], data['y']\n",
        "    output = []\n",
        "\n",
        "    for i, (xb, yb) in enumerate(zip(X, y)):\n",
        "        xb = torch.stack(xb)\n",
        "        X_flat = (xb.reshape(len(xb), -1).T).numpy()\n",
        "        y_flat = yb.flatten().numpy()\n",
        "        reg = LinearRegression().fit(X_flat, y_flat)\n",
        "        if i % 100 == 0: print(f\"Sentence #{i} - Coeffs: {[float(f\"{coef:.2f}\") for coef in reg.coef_]}, Intercept: {reg.intercept_:.2f}\")\n",
        "        output.append([reg.coef_.tolist(), float(reg.intercept_)])\n",
        "\n",
        "    output = pd.DataFrame(output, columns=[\"Coefficients\", \"Intercept\"]).to_csv(\"data/linear_regression_results.csv\", index=False)\n",
        "    return output\n",
        "\n",
        "head_loc = (3, 9)\n",
        "patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "generate_dataset(patterns, model, tokenizer, sentences, head_loc)\n",
        "output = train_linearregression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1612da99",
      "metadata": {},
      "outputs": [],
      "source": [
        "# VISUALIZE BEST FIT PATTERNS ACROSS LAYERS AND HEADS\n",
        "\n",
        "# model = AutoModel.from_pretrained(\"roberta-base\", output_attentions=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "df = pd.read_csv('data/best_fit_t5.csv')\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "mat = np.zeros((num_layers, num_heads), dtype=object)\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        mat[r, c] = []\n",
        "\n",
        "for (i, j), group in df.groupby(['i', 'j']):\n",
        "    sorted_group = group.sort_values(by='Score', ascending=False)\n",
        "    mat[i, j] = [(row['Pattern'], row['Score']) for idx, row in sorted_group.iterrows()]\n",
        "\n",
        "unique_patterns = df['Pattern'].unique()\n",
        "\n",
        "dark_orange = '#FF8C00'\n",
        "dark_blue = '#0000A0'\n",
        "azure = '#007FFF'\n",
        "medium_green = '#6aa84f'\n",
        "gray = '#D3D3D3'\n",
        "dark_red = '#A00000'\n",
        "purple = '#800080'\n",
        "\n",
        "hex_colors = ['brown', dark_blue, azure, 'pink', purple, gray, dark_orange, dark_red, medium_green]\n",
        "cmap_patterns = ListedColormap(hex_colors, name='my_cmap')\n",
        "\n",
        "pattern_colors = {pattern: cmap_patterns(i) for i, pattern in enumerate(unique_patterns)}\n",
        "white_color = (1, 1, 1, 1)\n",
        "plotting_matrix_rgb = np.zeros((num_layers, num_heads, 3))\n",
        "\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        cell_data = mat[r, c]\n",
        "        \n",
        "        if not cell_data:\n",
        "            plotting_matrix_rgb[r, c] = white_color[:3]\n",
        "        elif len(cell_data) == 1: \n",
        "            pattern_name = cell_data[0][0]\n",
        "            plotting_matrix_rgb[r, c] = pattern_colors[pattern_name][:3]\n",
        "        else:\n",
        "            top_pattern_1 = cell_data[0][0]\n",
        "            top_pattern_2 = cell_data[1][0]\n",
        "            color_1 = pattern_colors[top_pattern_1][:3]\n",
        "            color_2 = pattern_colors[top_pattern_2][:3]\n",
        "            plotting_matrix_rgb[r, c] = color_1 \n",
        "\n",
        "custom_draw_mask = np.zeros((num_layers, num_heads), dtype=bool)\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        if len(mat[r, c]) > 1:\n",
        "            custom_draw_mask[r, c] = True\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9, 18))\n",
        "img = ax.imshow(plotting_matrix_rgb, origin='lower', extent=[-0.5, num_heads - 0.5, -0.5, num_layers - 0.5])\n",
        "\n",
        "for r in range(num_layers):\n",
        "    for c in range(num_heads):\n",
        "        if custom_draw_mask[r, c]:\n",
        "            cell_data = mat[r, c]\n",
        "            top_pattern_1 = cell_data[0][0]\n",
        "            top_pattern_2 = cell_data[1][0]\n",
        "            \n",
        "            color_1 = pattern_colors[top_pattern_1]\n",
        "            color_2 = pattern_colors[top_pattern_2]\n",
        "            triangle1 = mpatches.Polygon(\n",
        "                [[c - 0.5, r - 0.5], [c + 0.5, r - 0.5], [c + 0.5, r + 0.5]],\n",
        "                facecolor=color_1, edgecolor='none'\n",
        "            )\n",
        "            ax.add_patch(triangle1)\n",
        "            triangle2 = mpatches.Polygon(\n",
        "                [[c - 0.5, r - 0.5], [c - 0.5, r + 0.5], [c + 0.5, r + 0.5]],\n",
        "                facecolor=color_2, edgecolor='none'\n",
        "            )\n",
        "            ax.add_patch(triangle2)\n",
        "\n",
        "ax.set_xticks(np.arange(num_heads))\n",
        "ax.set_yticks(np.arange(num_layers))\n",
        "ax.set_xticks(np.arange(-0.5, num_heads, 1), minor=True)\n",
        "ax.set_yticks(np.arange(-0.5, num_layers, 1), minor=True)\n",
        "ax.set_xlabel(f'{model.config.architectures[0]} - Heads', fontsize=14)\n",
        "ax.set_ylabel(f'{model.config.architectures[0]} - Layers', fontsize=14)\n",
        "ax.set_title('Pattern Distribution Across Layers and Heads')\n",
        "ax.set_aspect('equal')\n",
        "# ax.grid(color='black', linestyle='-', linewidth=0.5)\n",
        "ax.grid(which='minor', color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "legend_handles = []\n",
        "for pattern, color in pattern_colors.items():\n",
        "    patch = mpatches.Patch(color=color, label=pattern)\n",
        "    legend_handles.append(patch)\n",
        "\n",
        "white_patch = mpatches.Patch(facecolor=white_color, label='Pattern not Detected', edgecolor='black', linewidth=0.2)\n",
        "legend_handles.append(white_patch)\n",
        " \n",
        "ax.legend(\n",
        "    handles=legend_handles, \n",
        "    loc='center left', \n",
        "    bbox_to_anchor=(1.05, 0.5),\n",
        "    ncol=1, \n",
        "    fancybox=True, \n",
        "    shadow=True, \n",
        "    title=\"Manual Patterns Detected\",\n",
        "    title_fontsize=16,\n",
        "    fontsize='large', # Make legend text bigger. Can use 'medium', 'x-large', 'xx-large' or a numerical value (e.g., 12)\n",
        "    labelspacing=1.5, # Adjust vertical spacing between legend entries (default is 0.5)\n",
        "    handlelength=2.5, # Adjust length of the color patch/line in the legend\n",
        "    handletextpad=0.8, # Adjust space between the handle (color patch) and the text label\n",
        "    borderpad=0.5 # Adjust padding between the legend content and its border\n",
        "    \n",
        ") \n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.1, 1, 1]) # Adjust layout to make space for the legend\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42abc485",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    instructions=\"You are a coding assistant that talks like a pirate.\",\n",
        "    input=\"How do I check if a Python object is an instance of a class?\",\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45650d28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# prompt pieces (sequential)\n",
        "# 1 look at attention matrix and try to predict pattern\n",
        "# 2. come up with five candidate hypotheses and pick the top one\n",
        "# 3. explain why you picked that hypothesis\n",
        "# 4. write code that generates that pattern according to the hypothesis\n",
        "\n",
        "from automation_helper import example_one, example_two, example_three\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "layer = 3\n",
        "head = 9\n",
        "sentences = sentences[:10]\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model.config.architectures[0],\n",
        "        \"examples\": []}\n",
        "\n",
        "def handle_score(score):\n",
        "        # convert to percentage with 0 decimal places\n",
        "        return \"{:.0f}\".format(score * 100)\n",
        "\n",
        "def scrape_head(att, tokens, top_k_ratio, ignore_special=True):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append(f\"[{str(tokens[i])}|{str(tokens[j])}:{handle_score(score)}]\")\n",
        "        #make top activations str and delete brackets\n",
        "        top_activations_str = \" \".join(top_activations).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "        # print(top_activations_str)\n",
        "        return top_activations_str\n",
        "\n",
        "for idx, sentence in enumerate(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=0.05)\n",
        "        item = {f\"sentence {idx}\": \" \".join(tokens), \"sentence attention\": top_activations}\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "prompt_1 = f\"\"\"\n",
        "        You are given the following attention matrices sourced from {model.config.architectures[0]} based on {len(sentences)} sentences\n",
        "        from Layer {layer}, Head {head}. Look at the attenion matrix subset below and try to predict the most fitting three hypotheses\n",
        "        for the head function. Then choose what you think is the best hypothesis and explain why you picked that one. DATA: {data}\n",
        "        \"\"\"\n",
        "\n",
        "prompt_2 = f\"\"\"\n",
        "        Now use your explanation to write a Python function that generates an attention matrix for any given sentence according to your hypothesis  in less than 50 lines. The input to your function is def fn(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: and \n",
        "        the output is a tuple of a string (the name of the pattern) and a numpy array (the attention matrix). Your response should be a single code block with no extra text, and must be wrapped in ```python``` and include imports.\n",
        "        Think carefully before generating any code. These patterns can be simple or complex.  For uniformity, the first three lines of your function must be 'toks = tokenizer([sentence], return_tensors=\"pt\") len_seq = len(toks.input_ids[0]) out = np.zeros((len_seq, len_seq))'.\n",
        "        Make sure the token sequences from your tokenizer and spaCy (if you must use spaCy) are aligned via a dictionary if necessary, because they split text differently. Make sure you generalize your hypothesis pattern to any sentence. Functions can almost \n",
        "        always be expressed in fewer than 50 lines of code. As examples, it has been discovered one head is responsible for the complex task of dependency parsing. It's simplistic predicted pseudocode looks like: \n",
        "        {example_one}. Example 2: '''{example_two}''' Example 3: '''{example_three}'''. Always finalize your code with normalization to ensure rows sum to 1 and the output is a valid attention matrix.\n",
        "        \"\"\"\n",
        "\n",
        "# ----- AGENT GENERATES HYPOTHESIS ----- #\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\",\n",
        "    \"content\": \"You are a coding assistant well-versed in linguistics.\"}\n",
        "]\n",
        "conversation_history.append({\"role\": \"user\", \"content\": prompt_1})\n",
        "response_1 = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", messages=conversation_history, temperature = 0.5\n",
        ")\n",
        "assistant_response_1 = response_1.choices[0].message.content\n",
        "conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "\n",
        "# ----- AGENT WRITES PYTHON PROGRAM ----- #\n",
        "prompt_2 = \"Now, give me a Python function that uses a linguistic concept from your previous explanation.\"\n",
        "conversation_history.append({\"role\": \"user\", \"content\": prompt_2})\n",
        "response_2 = client.chat.completions.create(\n",
        "    model=\"gpt-5\",\n",
        "    messages=conversation_history\n",
        ")\n",
        "assistant_response_2 = response_2.choices[0].message.content\n",
        "print(f\"\\n--- Response 2 ---\\n{assistant_response_2}\")\n",
        "\n",
        "# ----- SAVE AGENT RESULTS ----- #\n",
        "folder = \"automation_results_gpt4o\"\n",
        "if not os.path.exists(folder): os.makedirs(folder)\n",
        "subfolders = [\"prompts\", \"llm_code\"]\n",
        "for subfolder in subfolders:\n",
        "    if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "        os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "with open(os.path.join(folder, \"prompts\", f\"layer{layer}_head{head}_prompts.txt\"), \"w\") as f:\n",
        "    f.write(f\"--- Response 1 ---\\n{assistant_response_1}\\n\\n--- Response 2 ---\\n{assistant_response_2}\")\n",
        "\n",
        "python_snippet = assistant_response_2.split(\"```python\")[1].split(\"```\")[0].strip()\n",
        "with open(os.path.join(folder, \"llm_code\", f\"layer{layer}_head{head}_code.py\"), \"w\") as f:\n",
        "    f.write(python_snippet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c1a30c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#helpers\n",
        "\n",
        "import json\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "def parse_model_output(output_text: str):\n",
        "    \"\"\"\n",
        "    Robustly parse model output with 'hypothesis' and 'program' keys.\n",
        "    Handles triple quotes, code fences, and inconsistent indentation.\n",
        "    Ignores extra text after the JSON object.\n",
        "    \"\"\"\n",
        "    # 1 Remove outer markdown fences\n",
        "    cleaned = re.sub(r\"^```(?:json)?|```$\", \"\", output_text.strip(), flags=re.MULTILINE).strip()\n",
        "\n",
        "    # 2 Handle malformed JSON with triple quotes\n",
        "    if '\"\"\"' in cleaned:\n",
        "        hyp_match = re.search(r'\"hypothesis\"\\s*:\\s*\"([^\"]+)\"', cleaned)\n",
        "        prog_match = re.search(r'\"program\"\\s*:\\s*(\"\"\"|```python|```)(.*?)(\\1|```)', cleaned, re.DOTALL)\n",
        "        if not hyp_match or not prog_match:\n",
        "            raise ValueError(\"Could not locate hypothesis or program block.\")\n",
        "        hypothesis = hyp_match.group(1).strip()\n",
        "        program_raw = prog_match.group(2)\n",
        "        program = textwrap.dedent(program_raw).strip()\n",
        "        program = program.replace('\\r\\n', '\\n')\n",
        "        return {\"hypothesis\": hypothesis, \"program\": program}\n",
        "\n",
        "    # 3 Parse valid JSON normally, but extract only the first JSON object\n",
        "    # This ignores any extra text after the first closing brace\n",
        "    json_match = re.search(r\"\\{.*?\\}\\s*(?=\\n|$)\", cleaned, flags=re.DOTALL)\n",
        "    if not json_match:\n",
        "        raise ValueError(\"No JSON object found in model output.\")\n",
        "\n",
        "    json_str = json_match.group(0)\n",
        "\n",
        "    try:\n",
        "        data = json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Invalid JSON: {e}\\nExtracted text:\\n{json_str}\")\n",
        "\n",
        "    if not all(k in data for k in (\"hypothesis\", \"program\")):\n",
        "        raise KeyError(f\"Missing keys. Found: {list(data.keys())}\")\n",
        "\n",
        "    # Clean code block & indentation\n",
        "    program_raw = re.sub(r\"^```(?:python)?|```$\", \"\", data[\"program\"].strip(), flags=re.MULTILINE)\n",
        "    program = textwrap.dedent(program_raw).strip().replace('\\r\\n', '\\n')\n",
        "\n",
        "    return {\n",
        "        \"hypothesis\": data[\"hypothesis\"].strip(),\n",
        "        \"program\": program,\n",
        "    }\n",
        "\n",
        "\n",
        "# def parse_model_output(output_text: str):\n",
        "#     \"\"\"\n",
        "#     Robustly parse model output with 'hypothesis' and 'program' keys.\n",
        "#     Handles triple quotes, code fences, and inconsistent indentation.\n",
        "#     \"\"\"\n",
        "#     # 1 Remove outer markdown fences\n",
        "#     cleaned = re.sub(r\"^```(?:json)?|```$\", \"\", output_text.strip(), flags=re.MULTILINE).strip()\n",
        "#     # 2 Handle malformed JSON with triple quotes\n",
        "#     if '\"\"\"' in cleaned:\n",
        "#         hyp_match = re.search(r'\"hypothesis\"\\s*:\\s*\"([^\"]+)\"', cleaned)\n",
        "#         prog_match = re.search(r'\"program\"\\s*:\\s*(\"\"\"|```python|```)(.*?)(\\1|```)', cleaned, re.DOTALL)\n",
        "#         if not hyp_match or not prog_match:\n",
        "#             raise ValueError(\"Could not locate hypothesis or program block.\")\n",
        "#         hypothesis = hyp_match.group(1).strip()\n",
        "#         program_raw = prog_match.group(2)\n",
        "#         #  Clean indentation and leading/trailing blank lines\n",
        "#         program = textwrap.dedent(program_raw).strip()\n",
        "#         #  Normalize line endings (just in case)\n",
        "#         program = program.replace('\\r\\n', '\\n')\n",
        "#         return {\"hypothesis\": hypothesis, \"program\": program}\n",
        "#     # 3 Parse valid JSON normally\n",
        "#     json_match = re.search(r\"\\{.*\\}\", cleaned, flags=re.DOTALL)\n",
        "#     if not json_match:\n",
        "#         raise ValueError(\"No JSON object found in model output.\")\n",
        "#     json_str = json_match.group(0)\n",
        "#     try:\n",
        "#         data = json.loads(json_str)\n",
        "#     except json.JSONDecodeError as e:\n",
        "#         raise ValueError(f\"Invalid JSON: {e}\\nExtracted text:\\n{json_str}\")\n",
        "#     if not all(k in data for k in (\"hypothesis\", \"program\")):\n",
        "#         raise KeyError(f\"Missing keys. Found: {list(data.keys())}\")\n",
        "#     #  Clean code block & indentation even for valid JSON\n",
        "#     program_raw = re.sub(r\"^```(?:python)?|```$\", \"\", data[\"program\"].strip(), flags=re.MULTILINE)\n",
        "#     program = textwrap.dedent(program_raw).strip().replace('\\r\\n', '\\n')\n",
        "#     return {\n",
        "#         \"hypothesis\": data[\"hypothesis\"].strip(),\n",
        "#         \"program\": program,\n",
        "#     }\n",
        "\n",
        "import traceback\n",
        "import importlib.util\n",
        "import types\n",
        "def validate_program(program_path, model, tokenizer, layer, head, sentences):       \n",
        "    try:\n",
        "        spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        spec.loader.exec_module(module)\n",
        "    except Exception as e:\n",
        "        print(f\"Program loading failed: {str(e)}\")\n",
        "        return str(e)\n",
        "\n",
        "    for attr_name in dir(module):\n",
        "        attr = getattr(module, attr_name)\n",
        "        if isinstance(attr, types.FunctionType):\n",
        "            program_to_test = attr\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        # print(\"Scoring program...\")\n",
        "        score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        error = traceback.format_exc()\n",
        "        full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "        return full_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aea170",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "#print name, num heads, num layers, \n",
        "print(model.config.architectures[0])\n",
        "print(model.config.num_attention_heads)\n",
        "print(model.config.num_hidden_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6ba30d",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "torch_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "torch_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "        if (layer, head) in [(0, 1), (0, 2), (0, 3), (0, 4), (0,5)]: continue\n",
        "\n",
        "        if sq_score[layer, head] < 0.7: continue\n",
        "        \n",
        "        # if (layer, head) not in fails: continue\n",
        "\n",
        "        success = False\n",
        "\n",
        "        print(f\"Layer {layer}, Head {head}\")\n",
        "        # max_attempts = 3  # Initial try + two more attempt \n",
        "        head_attempts = 0\n",
        "\n",
        "        # while head_attempts < max_attempts:\n",
        "        if success: break\n",
        "\n",
        "        try:\n",
        "            # from automation_helper import generate_prompt, validate_program\n",
        "            # print(\"got here\")\n",
        "            fullprompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "\n",
        "            conversation_history = [\n",
        "                {\"role\": \"system\",\n",
        "                \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "            ]\n",
        "            conversation_history.append({\"role\": \"user\", \"content\": fullprompt})\n",
        "            response_1 = client.chat.completions.create(\n",
        "                model=\"gpt-4o\", messages=conversation_history\n",
        "            )\n",
        "            assistant_response_1 = response_1.choices[0].message.content\n",
        "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "            print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "            try:\n",
        "                parsed = parse_model_output(assistant_response_1)\n",
        "            except:\n",
        "                continue\n",
        "            feedback = \"invalid_output\"\n",
        "\n",
        "            folder = \"automation_results_bert_2\"\n",
        "            if not os.path.exists(folder): os.makedirs(folder)\n",
        "            subfolders = [\"prompts\", \"llm_code\", \"scores\"]\n",
        "            for subfolder in subfolders:\n",
        "                if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "                    os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "            hypothesis_path = f\"{folder}/prompts/layer{layer}_head{head}_prompts.txt\"\n",
        "            with open(hypothesis_path, \"w\") as f: f.write(parsed[\"hypothesis\"])\n",
        "\n",
        "            python_path = f\"{folder}/llm_code/layer{layer}_head{head}_code.py\"\n",
        "            with open(python_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "\n",
        "            import traceback\n",
        "            import importlib.util\n",
        "            import types\n",
        "            def validate_program(program_path, model, tokenizer, layer, head, sentences):       \n",
        "                try:\n",
        "                    spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "                    module = importlib.util.module_from_spec(spec)\n",
        "                    module.__dict__['np'] = np\n",
        "                    spec.loader.exec_module(module)\n",
        "                except Exception as e:\n",
        "                    print(f\"Program loading failed: {str(e)}\")\n",
        "                    return str(e)\n",
        "\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        program_to_test = attr\n",
        "                        break\n",
        "\n",
        "                try:\n",
        "                    # print(\"Scoring program...\")\n",
        "                    head_scores = []\n",
        "                    for sentence in sentences[:10]:\n",
        "                            score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "                            head_scores.append(score)\n",
        "                    score = np.mean(head_scores)\n",
        "                    return score\n",
        "                except Exception as e:\n",
        "                    error = traceback.format_exc()\n",
        "                    full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "                    return full_error\n",
        "\n",
        "            max_refinements = 2 # no refinements\n",
        "            while type(feedback) is str or feedback > 0.7:\n",
        "                if max_refinements >= 3: break\n",
        "                \n",
        "                feedback = validate_program(python_path, model, tokenizer, layer, head, sentences)\n",
        "                print(feedback)\n",
        "                if isinstance(feedback, np.float64) and feedback <= 0.7:\n",
        "                    with open(python_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "                    success = True\n",
        "                    break\n",
        "\n",
        "                if type(feedback) is str:\n",
        "                    conversation_history.append({\"role\": \"user\", \"content\": f\"The following error was encountered when running your code: {feedback}. Please fix your code and return json with two keys: 'hypothesis' and 'program', where hypothesis is the explanation of what you fixed and program is the updated program. Here is your previous code: {parsed['program']}\"})\n",
        "                else:\n",
        "                    conversation_history.append({\"role\": \"user\", \"content\": f\"The score of your program was {feedback:.2f}, which is not good enough. Please refine your code and return json with two keys: 'hypothesis' and 'program', where hypothesis is the explanation of what you fixed and program is the updated program. Here is your previous code: {parsed['program']}\"})\n",
        "                response_2 = client.chat.completions.create(\n",
        "                    model=\"gpt-4o\",\n",
        "                    messages=conversation_history\n",
        "                )\n",
        "\n",
        "                try:\n",
        "                    parsed = parse_model_output(response_2.choices[0].message.content)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing response: {e}\")\n",
        "                    continue\n",
        "\n",
        "                with open(os.path.join(folder, \"prompts\", f\"layer{layer}_head{head}_prompts_try{max_refinements}.txt\"), \"w\") as f:\n",
        "                    f.write(parsed[\"hypothesis\"])\n",
        "                \n",
        "                max_refinements += 1\n",
        "\n",
        "            score = feedback\n",
        "            score_path = f\"{folder}/scores/layer{layer}_head{head}_score.txt\"\n",
        "            with open(score_path, \"w\") as f: f.write(str(score))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing Layer {layer}, Head {head}: {e}\")\n",
        "            head_attempts += 1\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3e6325",
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate through scores folder and count how many are floats and how many are strings\n",
        "# edit the following code to count nan as a failure\n",
        "\n",
        "folder = r\"automation_results_gpt2\\scores\"\n",
        "float_count = 0\n",
        "string_count = 0\n",
        "nan_count = 0\n",
        "no_code_count = 0\n",
        "\n",
        "fails = []\n",
        "\n",
        "for i in range(12):\n",
        "    for j in range(12):\n",
        "        layer, head = i, j\n",
        "        if not os.path.exists(os.path.join(folder, f\"layer{layer}_head{head}_score.txt\")):\n",
        "            fails.append((layer, head))\n",
        "            no_code_count += 1\n",
        "\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\"_score.txt\"):\n",
        "        with open(os.path.join(folder, filename), \"r\") as f:\n",
        "            content = f.read().strip()\n",
        "            try:\n",
        "                value = float(content)\n",
        "                if value != value:  # Check for NaN\n",
        "                    nan_count += 1\n",
        "                    layer = int(filename.split(\"_\")[0][5:])\n",
        "                    head = int(filename.split(\"_\")[1][4:])\n",
        "                    fails.append((layer, head))\n",
        "                else:\n",
        "                    float_count += 1\n",
        "            except ValueError:\n",
        "                layer = int(filename.split(\"_\")[0][5:])\n",
        "                head = int(filename.split(\"_\")[1][4:])\n",
        "                fails.append((layer, head))\n",
        "                string_count += 1\n",
        "\n",
        "print(f\"Float scores: {float_count}\")\n",
        "print(f\"String scores: {string_count}\")\n",
        "print(f\"NaN scores: {nan_count}\")\n",
        "print(f\"No code scores: {no_code_count}\")\n",
        "print(f\"Total scores: {float_count + string_count + nan_count + no_code_count}\\n\\n\")\n",
        "print(fails)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac782014",
      "metadata": {},
      "outputs": [],
      "source": [
        "# retest the bert scores using existing functions with 100 sentences for each head\n",
        "\n",
        "retest = [(11, 10), (4,11)]\n",
        "\n",
        "scores = []\n",
        "for layer in range(model.config.num_hidden_layers):\n",
        "    for head in range(model.config.num_attention_heads):\n",
        "\n",
        "        if (layer, head) not in retest: continue\n",
        "\n",
        "        print(f\"Layer {layer}, Head {head}\")\n",
        "        # find file in llm_code e.g. layer0_head0_code\n",
        "        program_path = f\"automation_results_bert/llm_code/layer{layer}_head{head}_code.py\"\n",
        "\n",
        "        try:\n",
        "            spec = importlib.util.spec_from_file_location(\"loaded_program\", program_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "        except Exception as e:\n",
        "            print(f\"Program loading failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "        for attr_name in dir(module):\n",
        "            attr = getattr(module, attr_name)\n",
        "            if isinstance(attr, types.FunctionType):\n",
        "                program_to_test = attr\n",
        "                break\n",
        "\n",
        "        head_scores = []\n",
        "        for sentence in sentences[:100]:\n",
        "            try:\n",
        "                score = score_prediction(model, tokenizer, (layer, head), program_to_test, sentence, distance=\"jsd\", output=False)\n",
        "                head_scores.append(score)\n",
        "            except Exception as e:\n",
        "                error = traceback.format_exc()\n",
        "                full_error = f\"Program validation failed: {str(e)} +\\n{error}\"\n",
        "                continue\n",
        "        \n",
        "        scores.append(np.mean(head_scores))\n",
        "        print(f\"Average Score for Layer {layer}, Head {head}: {np.mean(head_scores)}\")\n",
        "        # overwrite score file\n",
        "        score_path = f\"automation_results_bert/scores/layer{layer}_head{head}_score.txt\"\n",
        "        with open(score_path, \"w\") as f: \n",
        "            f.write(str(np.mean(head_scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84555c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "folder = r\"automation_results_bert\\scores\"\n",
        "scores = []\n",
        "\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\"_score.txt\"):\n",
        "        with open(os.path.join(folder, filename), \"r\") as f:\n",
        "            content = f.read().strip()\n",
        "            try:\n",
        "                value = float(content)\n",
        "                scores.append(round(value, 3))\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "scores = np.array(scores, dtype=float)\n",
        "scores[np.isnan(scores)] = np.mean(scores[~np.isnan(scores)])\n",
        "\n",
        "def find_outliers(data):\n",
        "    data = np.array(data)\n",
        "    Q1 = np.percentile(data, 25)\n",
        "    Q3 = np.percentile(data, 75)\n",
        "\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - (1.5 * IQR)\n",
        "    upper_bound = Q3 + (1.5 * IQR)\n",
        "\n",
        "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound, Q1, Q3, IQR\n",
        "\n",
        "outliers, lower_bound, upper_bound, Q1, Q3, IQR = find_outliers(scores)\n",
        "\n",
        "def plot_scores_boxplot(scores):\n",
        "    plt.figure(figsize=(3.2,8))\n",
        "    plt.boxplot(\n",
        "        scores,\n",
        "        positions=[0.75], \n",
        "        vert=True,\n",
        "        patch_artist=True,\n",
        "        medianprops={'color': 'black', 'linewidth': 3},\n",
        "        boxprops={'facecolor': 'gray', 'edgecolor': 'black'},\n",
        "        flierprops={'marker': 'D', 'markerfacecolor': 'black', 'markersize': 3, 'linestyle': 'none'}\n",
        "    )\n",
        "\n",
        "    plt.title('Automation | BERT', fontsize=14, weight='bold')\n",
        "    plt.ylabel('Automation Scores', fontsize=12)\n",
        "    plt.xticks([])\n",
        "    plt.ylim(0, 1.05)\n",
        "\n",
        "    #insert the text 'WIP' in center of plot\n",
        "    # plt.text(0.75, 0.5, 'WIP', fontsize=12, ha='center', va='center')\n",
        "\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "    x = np.ones_like(scores)\n",
        "    plt.scatter(\n",
        "        x,\n",
        "        scores,\n",
        "        color='gray',\n",
        "        edgecolor='black',\n",
        "        s=30,\n",
        "        alpha=0.9,\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "plot_scores_boxplot(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1c5fae",
      "metadata": {},
      "outputs": [],
      "source": [
        "def relative_position_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> tuple:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "\n",
        "    # Assign relative position importance to special tokens [CLS] and [SEP]\n",
        "    out[0, :] = 1   # [CLS] self-attention\n",
        "    out[:, 0] = 1   # [CLS] attends to all\n",
        "\n",
        "    out[-1, :] = 1  # [SEP] self-attention\n",
        "    out[:, -1] = 1  # [SEP] attends to all\n",
        "\n",
        "    # Calculate relative distance decay, favoring 'central' tokens\n",
        "    center = len_seq // 2\n",
        "    for i in range(1, len_seq-1):\n",
        "        dist_from_center = abs(center - i)\n",
        "        decayed_importance = 1 / (1 + dist_from_center)\n",
        "        out[i, :] += decayed_importance\n",
        "\n",
        "    # Normalize out matrix by row to simulate attention distribution\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Relative Position Attention\", out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e40e6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Started with 10 hand-written programs, classified all heads w/ linear interpolation, iterated over worst 20% scores one head at a time to generate multiple candidate hypotheses and choose the one with the best interpolation improvement for adding to db\n",
        "\n",
        "import inspect\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "model = AutoModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "num_layers = model.config.num_hidden_layers\n",
        "num_heads = model.config.num_attention_heads\n",
        "\n",
        "interpolation_matrix = np.zeros((num_layers, num_heads))\n",
        "# patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment, dependencies]\n",
        "\n",
        "\n",
        "patterns = [previous_attention, same_attention, punctuation_attention, last_token_attention, repeated_attention, uniform_attention, cls_attention, special_token_attention, pos_alignment, relative_position_attention]\n",
        "\n",
        "folder = r\"automation_refinement_gpt2\"\n",
        "subfolders = [\"master_list\", \"candidate\"]\n",
        "if not os.path.exists(folder): os.makedirs(folder)\n",
        "for subfolder in subfolders:\n",
        "    if not os.path.exists(os.path.join(folder, subfolder)):\n",
        "        os.makedirs(os.path.join(folder, subfolder))\n",
        "\n",
        "for pattern in patterns:\n",
        "    pattern_name = pattern.__name__\n",
        "    with open(os.path.join(folder, \"master_list\", f\"{pattern_name}.py\"), \"w\") as f:\n",
        "        f.write(inspect.getsource(pattern))\n",
        "\n",
        "new_patterns = 0\n",
        "while new_patterns < 30:\n",
        "    print(f\"Searching for new patterns, iteration {new_patterns}\")\n",
        "\n",
        "    # get interpolation scores for all heads\n",
        "    print(f\"\\tCalculating interpolation scores for all heads...\")\n",
        "    for layer in range(num_layers):\n",
        "        if layer % 3 == 0: print(f\"\\t\\tLayer {layer}/{num_layers} complete\")\n",
        "        for head in range(num_heads):\n",
        "\n",
        "            X = []\n",
        "            for pattern in patterns:\n",
        "                X.append(pattern(sentence, tokenizer)[1].flatten())\n",
        "            X_n = np.array(X).T\n",
        "\n",
        "            y = model(**tokenizer(sentence, return_tensors=\"pt\"), output_attentions=True).attentions[layer][0, head].detach().numpy().flatten()\n",
        "            reg = LinearRegression().fit(X_n, y.flatten())\n",
        "            out = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "            len_seq = len(tokenizer([sentence], return_tensors=\"pt\").input_ids[0])\n",
        "            out = out.reshape((len_seq, len_seq))\n",
        "            pred_att = out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "            tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "            att = model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "            jensonshannon_distances = []\n",
        "            for row_att, row_out in zip(att, pred_att):\n",
        "                jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "            interpolation_score = np.mean(jensonshannon_distances)\n",
        "\n",
        "            interpolation_matrix[layer, head] = interpolation_score\n",
        "            # print(f\"Layer {layer}, Head {head} - Interpolation Score: {interpolation_score:.4f}\")\n",
        "\n",
        "    # select worst 20% of heads based on interpolation score\n",
        "    sorted_indices = np.dstack(np.unravel_index(np.argsort(interpolation_matrix.ravel()), interpolation_matrix.shape))[0]\n",
        "    num_to_select = int(0.09 * num_layers * num_heads)\n",
        "    worst_indices = sorted_indices[-num_to_select:]\n",
        "\n",
        "    # generate candidate programs for each of the worst heads\n",
        "    candidate_scores = []\n",
        "    candidate_paths = []\n",
        "    for layer, head in worst_indices[::-1]: # start with worst head\n",
        "        layer, head = int(layer), int(head)\n",
        "        print(f\"\\t TRYING | Layer {layer}, Head {head} - Previous Interpolation Score: {interpolation_matrix[layer, head]:.4f}\")\n",
        "\n",
        "        try:\n",
        "            fullprompt = generate_prompt(generic_sentences[:25], model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "            refinement_part = \"It's already been found that this head doesn't do well on any of the following patterns: \"\n",
        "            refinement_part += \", \".join([pattern.__name__ for pattern in patterns])\n",
        "            refinement_part += \". So don't suggest any of those patterns again.\"\n",
        "            fullprompt += \" \".join(refinement_part)\n",
        "\n",
        "            conversation_history = [\n",
        "                {\"role\": \"system\",\n",
        "                \"content\": \"You are a coding assistant with linguistic expertise.\"}\n",
        "            ]\n",
        "            conversation_history.append({\"role\": \"user\", \"content\": fullprompt})\n",
        "            response_1 = client.chat.completions.create(\n",
        "                model=\"gpt-4o\", messages=conversation_history\n",
        "            )\n",
        "            assistant_response_1 = response_1.choices[0].message.content\n",
        "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response_1})\n",
        "            # print(f\"--- Response 1 ---\\n{assistant_response_1}\")\n",
        "            try:\n",
        "                parsed = parse_model_output(assistant_response_1)\n",
        "            except:\n",
        "                print(f\"\\t\\tError parsing response, skipping head ({layer}, {head})\")\n",
        "                continue\n",
        "            feedback = \"invalid_output\"\n",
        "\n",
        "            # print(f\"\\t\\t{parsed}\")\n",
        "            print(f\"\\t\\tHypothesis: {parsed['hypothesis']}. Program successfully parsed.\")\n",
        "\n",
        "            candidate_path = f\"{folder}/candidate/layer{layer}_head{head}_code.py\"\n",
        "            with open(candidate_path, \"w\") as f: f.write(parsed[\"program\"].rstrip(\"}\"))\n",
        "\n",
        "            feedback = validate_program(candidate_path, model, tokenizer, layer, head, sentences)\n",
        "            if isinstance(feedback, np.float64) and not np.isnan(feedback):\n",
        "                candidate_scores.append(feedback)\n",
        "                candidate_paths.append(candidate_path)\n",
        "                print(f\"\\t\\tProgram valid with score {feedback:.4f}.\")\n",
        "                continue\n",
        "                \n",
        "            else:\n",
        "                candidate_scores.append(100)\n",
        "                candidate_paths.append(\"\")\n",
        "                # print(f\"\\t\\tInitial program invalid w/ error: ({feedback}), skipping.\")\n",
        "                continue\n",
        "        \n",
        "        except Exception as e:\n",
        "            candidate_scores.append(100)\n",
        "            candidate_paths.append(\"\")\n",
        "            # print(f\"\\t\\tError processing Layer {layer}, Head {head}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # select and save best candidate program to master list\n",
        "    best_candidate = candidate_paths[np.argmin(candidate_scores)]\n",
        "    best_score = np.min(candidate_scores)\n",
        "    print(f\"Best candidate program {new_program.__name__} added | solo score {best_score:.4f}.\")\n",
        "\n",
        "    if best_score < 0.6:\n",
        "        spec = importlib.util.spec_from_file_location(\"loaded_program\", best_candidate)\n",
        "        module = importlib.util.module_from_spec(spec)\n",
        "        module.__dict__['np'] = np\n",
        "        spec.loader.exec_module(module)\n",
        "\n",
        "        for attr_name in dir(module):\n",
        "            attr = getattr(module, attr_name)\n",
        "            if isinstance(attr, types.FunctionType):\n",
        "                new_program = attr\n",
        "\n",
        "        new_pattern_name = new_program.__name__\n",
        "        with open(os.path.join(folder, \"master_list\", f\"{new_pattern_name}.py\"), \"w\") as f:\n",
        "            f.write(inspect.getsource(new_program))\n",
        "\n",
        "        patterns.append(new_program)\n",
        "        new_patterns += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5396e98",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONDUCT MODEL LEVEL ANALYSIS / GET SUMMARY SCORE FOR WHOLE MODEL\n",
        "\n",
        "def classify_model(method, sentences, torch_model, torch_tokenizer):\n",
        "    if method == \"linear_fit\":\n",
        "        patterns = [next_attention, previous_attention, same_attention, punctuation_attention, repeated_attention, pos_alignment]\n",
        "    elif method == \"best_fit\":\n",
        "        saved_file = pd.read_csv('data/best_fit_t5.csv')\n",
        "    elif method == \"automation\":\n",
        "        from automation_helper import generate_prompt, parse_llm_idea, validate_program\n",
        "\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    all_scores = []\n",
        "    final_scores = []\n",
        "    \n",
        "    if method != \"automation\":\n",
        "        for idx, sentence in enumerate(sentences):\n",
        "            scores = np.zeros((num_layers, num_heads))\n",
        "            for i in range(num_layers):\n",
        "                for j in range(num_heads):\n",
        "                    layer, head = i, j\n",
        "                    inputs = torch_tokenizer(sentence, return_tensors=\"pt\")\n",
        "                    len_seq = len(torch_tokenizer([sentence], return_tensors=\"pt\").input_ids[0])\n",
        "\n",
        "                    X = []\n",
        "                    # y =  torch_model(**inputs, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "                    decoder_input_ids = tokens[\"input_ids\"]\n",
        "                    outputs = torch_model(input_ids=inputs[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "                    y = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "                    if method == \"random_baseline\":\n",
        "                        pred_att = np.zeros((len_seq, len_seq))\n",
        "                        pred_att[:, -1] = 1.0\n",
        "\n",
        "                    elif method == \"best_fit\":\n",
        "                        matching_rows = saved_file[(saved_file['i'] == i) & (saved_file['j'] == j)]\n",
        "                        if not matching_rows.empty:\n",
        "                            best_pattern = matching_rows.loc[matching_rows['Score'].idxmax(), 'Pattern']\n",
        "                            func = globals()[best_pattern]\n",
        "                            _, pred_att = func(sentence, tokenizer)\n",
        "                        else:\n",
        "                            out = np.random.rand(len_seq, len_seq)\n",
        "                            pred_att =  out / out.sum(axis=1, keepdims=True)\n",
        "\n",
        "                    elif method == \"linear_fit\":\n",
        "                        for pattern in patterns:\n",
        "                            X.append(pattern(sentence, torch_tokenizer)[1].flatten())\n",
        "                        X_n = np.array(X).T\n",
        "                        y = y.flatten()\n",
        "\n",
        "                        reg = LinearRegression().fit(X_n, y)\n",
        "                        side_length = int(np.sqrt(len(y)))\n",
        "                        y = y.reshape((side_length, side_length))\n",
        "\n",
        "                        pred_att = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
        "                        pred_att = pred_att.reshape((side_length, side_length))\n",
        "\n",
        "                    jensonshannon_distances = []\n",
        "                    for row_att, row_out in zip(y, pred_att):\n",
        "                        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "                    score = np.mean(jensonshannon_distances)\n",
        "                    scores[layer, head] = score\n",
        "                    \n",
        "            all_scores.append(scores)\n",
        "            final_scores.append(np.sum(scores))\n",
        "            print(f\"Processed sentence #{idx}/{len(sentences)}: Score: {np.sum(scores):.2f}\\n\\t->'{sentence}'\")\n",
        "        \n",
        "    elif method == \"automation\":\n",
        "        \n",
        "        prompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.025)\n",
        "        hypothesis, program = parse_llm_idea(prompt, config=config, verbalize=False)\n",
        "        python_path = f\"{program_path}/{head}_output.py\"\n",
        "        feedback = validate_program(python_path, model, tokenizer, layer, head, sentences)\n",
        "        scores[layer, head] = feedback\n",
        "            \n",
        "\n",
        "    print(f\"Final Score: {sum(final_scores) / len(final_scores)}\")\n",
        "    return all_scores, final_scores\n",
        "\n",
        "methods = [\"random_baseline\", \"best_fit\", \"linear_fit\"]\n",
        "final_scores = {\"random_baseline\": 0, \"best_fit\": 0, \"linear_fit\": 0}\n",
        "\n",
        "for method in methods:\n",
        "    print(f\"\\nAnalyzing method: {method}\")\n",
        "    _, scores = classify_model(method, sentences[:10], model, tokenizer)\n",
        "    final_scores[method] = np.mean(scores)\n",
        "\n",
        "print(\"\\n\",final_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0989da5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PLOT DIFFERENT SUMMARY SCORES FOR THE MODEL\n",
        "\n",
        "max_score = model.config.num_hidden_layers * model.config.num_attention_heads\n",
        "raw_scores = [111, 92, 62, 65, 56]\n",
        "labels = ['Random \\nToken Baseline', 'Automatic\\nPrograms', 'K=1Refined\\nPrograms', 'Best Fit\\nPrograms', 'Linear Weight\\nPrograms']\n",
        "colors = ['darkred', 'darkblue', '#6aa84f', '#800080']\n",
        "\n",
        "# Normalize scores: lower scores become higher bars\n",
        "scores = [(score / max_score) for score in raw_scores]\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "bars = plt.bar(labels, scores, color=colors, width=0.6)\n",
        "\n",
        "# Add text labels on top of bars\n",
        "for bar, raw, norm in zip(bars, raw_scores, scores):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
        "             f'{norm:.2f}\\n[ {int(raw)} / {max_score} ]', ha='center', va='bottom', fontsize=14)\n",
        "ax.set_facecolor('#F5F5F5')\n",
        "\n",
        "plt.ylim(0, 1.0)\n",
        "# plt.title('Normalized Error (1 - Score / Max Score)')\n",
        "plt.xticks(fontsize=14)\n",
        "plt.text(plt.xlim()[0]-0.7, plt.ylim()[1]+0.05, '[bad hypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
        "plt.text(plt.xlim()[0]-0.7, plt.ylim()[0]-0.13, '[well-fitting\\nhypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
        "plt.ylabel('Normalized Model Scores', fontsize=16, labelpad=20)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fdf02a",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Program Automation Efforts\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5510c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# AUTOMATICALLY GENERATE PATTERNS FOR A HEAD & GENERATE LLM PROMPT\n",
        "\n",
        "example_program_one = \"\"\"\n",
        "def dependencies(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
        "    len_seq = len(toks.input_ids[0]) /\n",
        "    out = np.zeros((len_seq, len_seq)) /\n",
        "    words = sentence.split() /\n",
        "    doc = nlp(\" \".join(words)) /\n",
        "    for stok in doc: /\n",
        "        parent_index = stok.i /\n",
        "        for child_stok in stok.children: /\n",
        "            child_index = child_stok.i /\n",
        "            out[parent_index+1, child_index+1] = 1 /\n",
        "            out[child_index+1, parent_index+1] = 1 /\n",
        "    out[0, 0] = 1 /\n",
        "    out[-1, 0] = 1 /\n",
        "    out += 1e-4 /\n",
        "    out = out / out.sum(axis=1, keepdims=True) /\n",
        "    return \"Dependency Parsing Pattern\", out /\n",
        "\"\"\"\n",
        "example_program_two = \"\"\"\n",
        "def same_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Same Token Pattern\", out\n",
        "\"\"\"\n",
        "example_program_three = \"\"\"\n",
        "def pos_alignment(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
        "    len_seq = len(toks.input_ids[0]) /\n",
        "    out = np.zeros((len_seq, len_seq)) /\n",
        "    # assign toks, input_ids, word_ids, len_seq, out, doc /\n",
        "    # use spacey to get pos_tags for tokens in docs [token.pos_ for token in doc] /\n",
        "    # for token in pos_tags: /\n",
        "    # loop through pos_tags and increment out[i,j] when pos_tags match /\n",
        "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention /\n",
        "    # Normalize out matrix by row (results in uniform attention) and return out /\n",
        "    # return 'Part of Speech Implementation 1', out /\n",
        "\"\"\"\n",
        "\n",
        "def generate_prompt(sentences, model, tokenizer, head_loc, top_k_ratio=0.1):\n",
        "    layer, head = head_loc\n",
        "    data = {\n",
        "        \"layer\": layer,\n",
        "        \"head\": head,\n",
        "        \"model\": model.config.architectures[0],\n",
        "        \"examples\": []\n",
        "    }\n",
        "\n",
        "    def handle_score(score):\n",
        "        # convert to percentage with 0 decimal places\n",
        "        return \"{:.0f}\".format(score * 100)\n",
        "        \n",
        "    def scrape_head(att, tokens, top_k_ratio, ignore_special=True):\n",
        "        seq_len = att.shape[0]\n",
        "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
        "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
        "        att_scores = []\n",
        "        for i in keep_indices:\n",
        "            for j in keep_indices:\n",
        "                att_scores.append((i, j, att[i, j]))\n",
        "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
        "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
        "        top_activations = []\n",
        "        for i, j, score in top_att:\n",
        "            top_activations.append(f\"[{str(tokens[i])}|{str(tokens[j])}:{handle_score(score)}]\")\n",
        "        top_activations_str = \" \".join(top_activations).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "        return top_activations_str\n",
        "    \n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "            att = outputs.attentions[layer][0, head]\n",
        "        att = att.detach().cpu().numpy()\n",
        "        top_activations = scrape_head(att, tokens, top_k_ratio=top_k_ratio)\n",
        "        item = {f\"sentence {idx}\": \" \".join(tokens), \"sentence attention\": top_activations}\n",
        "        data[\"examples\"].append(item)\n",
        "\n",
        "    data = json.dumps(data, indent=2)\n",
        "    prompt = f\"\"\"\n",
        "    Using the following pieces of data based on {len(sentences)} sentences, generate three hypothesises about the linguistic role the following head is responsible for based on patterns\n",
        "    in the {model.config.architectures[0]} activations.  Then, choose the most fitting hypothesis for the head function using examples from the data. Finally, using the linguistic hypothesis you determine, \n",
        "    write a python function which takes in a sentence and tokenizer as parameters and outputs the name of the pattern you hypothesize along with a predicted_matrix (size: token_len * token_len), which is the \n",
        "    rule encoded matrix mirroring attention patterns you'd predict for any given sentence for Layer {layer}, Head {head}. Feel free to encode complex functions but write the simplest algorithm that captures your \n",
        "    observed pattern. You must respond to this prompt in JSON in the form \"{{\"hypothesis\": \"...\", \"program\": \"...\"}} with your chosen hypothesis. Think carefully before generating any code.\n",
        "    The first portion of your response has key \"hypothesis\" with the title of the hypothesis and the second portion of your response with key \"program\" should have valid python code starting with ```python and including imports. These patterns can be simple or \n",
        "    complex.  For uniformity, the first three lines of your function should be 'toks = tokenizer([sentence], return_tensors=\"pt\") len_seq = len(toks.input_ids[0]) out = np.zeros((len_seq, len_seq))'.\n",
        "    Make sure the token sequences from your tokenizer and spaCy (if you must use spaCy) are aligned via a dictionary if necessary, because they split text differently. Make sure you generalize your hypothesis pattern to any sentence. Functions can almost \n",
        "    always be expressed in fewer than 50 lines of code. As examples, it has been discovered one head is responsible for the complex task of dependency parsing. It's simplistic predicted pseudocode looks like: \n",
        "    {example_program_one}. Example 2: '''{example_program_two}''' Example 3: '''{example_program_three}'''. DATA: {data}\"\"\"\n",
        "    return ' '.join(prompt.strip().split())\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "layer, head = 5, 7\n",
        "prompt = generate_prompt(generic_sentences[:25], model, tokenizer, (layer, head), 0.025)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5704e094",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using Gemini, GPT-4o, Claude, Deepseek\n",
        "# API needs long contexts and free access\n",
        "# Source to get API keys is \"usage\" key\n",
        "\n",
        "load_dotenv()\n",
        "API_CONFIGS = {\n",
        "    \"gemini\": {\n",
        "        \"model\": \"gemini\",\n",
        "        \"url\": \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\",\n",
        "        \"key\": os.getenv(\"GEMINI\"),\n",
        "        \"headers_fn\": lambda key: {\"Content-Type\": \"application/json\", \"X-goog-api-key\": key},\n",
        "        \"payload_fn\": lambda prompt: {\n",
        "            \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "            \"generationConfig\": {\"response_mime_type\": \"application/json\"}\n",
        "        },\n",
        "        \"usage\": \"https://aistudio.google.com/apikey\"\n",
        "    },\n",
        "    \"openai\": {\n",
        "        \"model\": \"openai\",\n",
        "        \"url\": \"https://api.openai.com/v1/responses\",\n",
        "        \"key\": os.getenv(\"OPENAI\"),\n",
        "        \"headers_fn\": lambda key: {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\": \"gpt-4.1\", \"input\": prompt},\n",
        "        \"usage\": \"https://platform.openai.com/account/api-keys\"\n",
        "    },\n",
        "    \"claude\": {\n",
        "        \"model\": \"claude\",\n",
        "        \"url\": \"https://api.anthropic.com/v1/messages\",\n",
        "        \"key\": os.getenv(\"CLAUDE\"),\n",
        "        \"headers_fn\": lambda key: {\"x-api-key\": key, \"Content-Type\": \"application/json\", \"Anthropic-Version\":\"2023-06-01\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\":\"claude-sonnet-4-20250514\", \"messages\":[{\"role\":\"user\",\"content\":prompt}]},\n",
        "        \"usage\": \"https://platform.claude.com/api_keys\"\n",
        "    },\n",
        "    \"deepseek\": {\n",
        "        \"model\": \"deepseek\",\n",
        "        \"url\": \"https://api.deepseek.com/chat/completions\",\n",
        "        \"key\": os.getenv(\"DEEPSEEK\"),\n",
        "        \"headers_fn\": lambda key: {\"Authorization\": f\"Bearer {key}\", \"Content-Type\": \"application/json\"},\n",
        "        \"payload_fn\": lambda prompt: {\"model\": \"deepseek-chat\", \"input\": prompt, \"max_tokens\": 1000},\n",
        "        \"usage\": \"https://platform.deepseek.com/api_keys\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2112fba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAVE AUTOMATED HYPOTHESIS + VALIDATE GENERATED PROGRAM SYNTHESIS CODE\n",
        "\n",
        "def parse_llm_idea(prompt, config=\"YOUR_API_CONFIG\", verbalize=True):\n",
        "    def make_request():\n",
        "        headers = config[\"headers_fn\"](config[\"key\"])\n",
        "        payload = config[\"payload_fn\"](prompt)\n",
        "        response = requests.post(config[\"url\"], headers=headers, data=json.dumps(payload))\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if config[\"model\"] == \"gemini\":\n",
        "            data = response.json()\n",
        "            output = data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        if config[\"model\"] == \"openai\":\n",
        "            pass\n",
        "        if config[\"model\"] == \"claude\":\n",
        "            data = response.json()\n",
        "            output = data[\"content\"][\"text\"]\n",
        "        if config[\"model\"] == \"deepseek\":\n",
        "            pass\n",
        "\n",
        "        return output\n",
        "    \n",
        "    output = make_request()\n",
        "\n",
        "    try:\n",
        "        result = json.loads(output)\n",
        "\n",
        "        if type(result) is list: result = result[0]\n",
        "        hypothesis = result.get(\"hypothesis\", \"\")\n",
        "        program = result.get(\"program\", \"\")\n",
        "\n",
        "        if program.startswith(\"```python\"): program = program[9:]\n",
        "        if program.endswith(\"```\"): program = program[:-3]\n",
        "        program = program.strip()\n",
        "\n",
        "        if verbalize: print(\"Hypothesis, Explanation & Program successfully parsed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Parsing API failed: {str(e)}\")\n",
        "        return str(e)\n",
        "\n",
        "    return hypothesis, program\n",
        "\n",
        "config = API_CONFIGS[\"gemini\"] \n",
        "parse_llm_idea(prompt, config=config, verbalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be7aad1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SIMPLE AUTOMATION OF PIPELINE FOR ANALYZING ALL HEADS & WRITING/SAVING PROGRAMS\n",
        "\n",
        "def automation_pipeline(model, tokenizer, sentences, API_KEY, save_data=True, evaluate=False):\n",
        "    heads = model.config.num_attention_heads\n",
        "    layers = model.config.num_hidden_layers\n",
        "    prompts, programs = [], []\n",
        "\n",
        "    for layer in range(layers):\n",
        "        # if layer == 0: continue\n",
        "        if save_data:\n",
        "            # save prompts:\n",
        "            prompt_path = f\"automation_2/prompts/{layer}/\"\n",
        "            os.makedirs(prompt_path, exist_ok=True)\n",
        "\n",
        "            # save programs:\n",
        "            program_path = f\"automation_2/llm_code/code_layer_{layer}/\"\n",
        "            os.makedirs(program_path, exist_ok=True)\n",
        "\n",
        "            # save scores:\n",
        "            if evaluate:\n",
        "                score_path = f\"automation_2/scores/{layer}/\"\n",
        "                os.makedirs(score_path, exist_ok=True)\n",
        "\n",
        "        for head in range(heads):\n",
        "            # if head < 9: continue\n",
        "            if (layer, head) not in failed_programs:\n",
        "                continue\n",
        "            prompt = generate_prompt(sentences, model, tokenizer, (layer, head), top_k_ratio=0.1)\n",
        "            hypothesis, explanation, program = parse_llm_idea(prompt, API_KEY, output=False)\n",
        "            print(f\"Analyzed Layer {layer}, Head {head} | Hypothesis ~ {hypothesis} \")\n",
        "\n",
        "            prompts.append(prompt)\n",
        "            programs.append(program)\n",
        "\n",
        "            if save_data:\n",
        "                with open(f\"{prompt_path}/{layer}_{head}_prompt.txt\", \"w\") as f: f.write(prompt)\n",
        "                with open(f\"{program_path}/{head}_output.py\", \"w\") as f: f.write(program)\n",
        "\n",
        "        if evaluate: \n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "automation_pipeline(model, tokenizer, generic_sentences[:10], API_KEY=API_KEY, save_data=True, evaluate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e9c1ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CALCULATE AND SAVE SCORES FOR AUTOMATICALLY GENERATED PROGRAMS\n",
        "\n",
        "import importlib.util\n",
        "import types\n",
        "\n",
        "scores = []\n",
        "failed_programs = []\n",
        "for layer in range(12):\n",
        "    # if layer != 11: continue\n",
        "    code_path = f\"automation_2/llm_code/code_layer_{layer}/\"\n",
        "    for j in range(12):\n",
        "        # if j != 11: continue\n",
        "        filename = f\"{j}_output.py\"\n",
        "        program_path = os.path.join(code_path, filename)\n",
        "        if not os.path.exists(program_path): continue\n",
        "        score_path = f\"automation_2/scores/{layer}_{j}_score.txt\"\n",
        "        os.makedirs(os.path.dirname(score_path), exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_j{j}\", program_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            spec.loader.exec_module(module)\n",
        "        except Exception as e:\n",
        "            print(f\"Layer {layer}, Head {j} - Error: {e}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"Error loading module: {e}\")\n",
        "            failed_programs.append((layer, j))\n",
        "            scores.append(-1)\n",
        "            continue\n",
        "\n",
        "        for attr_name in dir(module):\n",
        "            attr = getattr(module, attr_name)\n",
        "            if isinstance(attr, types.FunctionType):\n",
        "                program_to_test = attr\n",
        "                break\n",
        "\n",
        "        try:\n",
        "            score = score_prediction(model, tokenizer, (layer, j), program_to_test, generic_sentences[0], distance=\"jsd\", output=False)\n",
        "            print(f\"Layer {layer}, Head {j} - Score: {score:.2f}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"{score:.2f}\")\n",
        "            scores.append(score)\n",
        "        except Exception as e:\n",
        "            print(f\"Layer {layer}, Head {j} - Error: {e}\")\n",
        "            with open(score_path, \"w\") as f: f.write(f\"Error during scoring: {e}\")\n",
        "            failed_programs.append((layer, j))\n",
        "            scores.append(-1)\n",
        "            continue\n",
        "\n",
        "num_scored = len([s for s in scores if s != -1 and not np.isnan(s)])\n",
        "print(f\"Number of Successfully Scored Heads: {num_scored} out of {len(scores)}\")\n",
        "\n",
        "avg_score = sum([s for s in scores if s != -1 and not np.isnan(s)]) / len([s for s in scores if s != -1 and not np.isnan(s)])\n",
        "print(f\"Average Score (excluding errors): {avg_score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48c76a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# make scores into a matrix (12,12) with -1 for failed programs\n",
        "\n",
        "sq_score = np.full((12, 12), -1.0)\n",
        "for idx, score in enumerate(scores):\n",
        "    layer = idx // 12\n",
        "    head = idx % 12\n",
        "    sq_score[layer, head] = score\n",
        "\n",
        "sq_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a61ba7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# loop through bert and bert2 scores, take best score and build sq_Score matrix\n",
        "\n",
        "scores = np.array([])\n",
        "\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        score1_path = f\"automation_bert/scores/layer{layer}_head{head}_score.txt\"\n",
        "        score2_path = f\"automation_bert2/scores/layer{layer}_head{head}_score.txt\"\n",
        "        score1, score2 = -1, -1\n",
        "        if os.path.exists(score1_path):\n",
        "            with open(score1_path, \"r\") as f:\n",
        "                try:\n",
        "                    score1 = float(f.read().strip())\n",
        "                except:\n",
        "                    score1 = -1\n",
        "        if os.path.exists(score2_path):\n",
        "            with open(score2_path, \"r\") as f:\n",
        "                try:\n",
        "                    score2 = float(f.read().strip())\n",
        "                except:\n",
        "                    score2 = -1\n",
        "        best_score = min(score1, score2)\n",
        "        scores = np.append(scores, best_score)\n",
        "\n",
        "sq_scores = scores.reshape((12, 12))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa549158",
      "metadata": {},
      "outputs": [],
      "source": [
        "sq_score = np.reshape(scores, (12, 12))\n",
        "\n",
        "colors = \"Grays\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "im2 = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im2.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im2, ax=ax)\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
        "ax.set_yticklabels([i for i in range(12)])\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "# put a space element in between automation and scores in text\n",
        "title = (\n",
        "    r'$\\mathbf{Automation \\ Scores}$'  # \\mathbf makes the text bold\n",
        "    '\\n\\nMethod: No Refinement'\n",
        "    f'\\nScores | {model.config.architectures[0]}'  # Example: replaced model.config...\n",
        ")\n",
        "plt.title(f\"{title}\\n\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257999d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "colors = \"Grays_r\"\n",
        "plt.rcParams['text.usetex'] = False\n",
        "\n",
        "score_threshold = 0.4\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "highlighted_sq = np.where(sq_score < score_threshold, sq_score, np.nan)\n",
        "# make all non-highlighted values white (1.0)\n",
        "highlighted_sq = np.where(np.isnan(highlighted_sq), 1.0, highlighted_sq)\n",
        "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
        "cmap = plt.cm.get_cmap(colors).copy()\n",
        "cmap.set_bad(color='gray')\n",
        "im2 = ax.imshow(highlighted_sq, cmap=cmap, aspect='auto', norm=norm)\n",
        "im2.set_clim(vmin=0, vmax=1)\n",
        "cbar = plt.colorbar(im2, ax=ax)\n",
        "ax.set_xticks(range(12))\n",
        "ax.set_yticks(range(12))\n",
        "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
        "ax.set_yticklabels([i for i in range(12)])\n",
        "import matplotlib\n",
        "print(\"usetex:\", matplotlib.rcParams['text.usetex'])\n",
        "plt.rcParams['text.usetex'] = False\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "# make automation bold\n",
        "# get number of highlighted scores\n",
        "num_highlighted = np.sum(sq_score < score_threshold)\n",
        "title = (\n",
        "    r'$\\mathbf{Highlighted\\ Scores}$'\n",
        "    '\\n\\nMethod: No Refinement'\n",
        "    f'\\n {num_highlighted} scores < {score_threshold} ({num_highlighted/(len(sq_score)**2)*100:.0f}%) | {model.config.architectures[0]}\\n')\n",
        "# title = \"Automation Scores\\n\"\n",
        "plt.title(title)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3a0037",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center; font-size: 2em;\">\n",
        "  <hr>\n",
        "    Hypothesis Structure Experiments\n",
        "  <hr>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c820d5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from programs import *\n",
        "\n",
        "programs = [next_attention, previous_attention, same_attention, punctuation_attention, dependencies,\n",
        "            last_token_attention, uniform_attention, cls_attention, eos_attention, pos_alignment,\n",
        "            special_token_attention, repeated_attention, noun_modifier_attention, pronoun_attention,\n",
        "            single_token_attention, root_cluster_attention]\n",
        "\n",
        "sentence_data = sentences[:25]\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def program_similarity(att_one, att_two):\n",
        "    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "        p = np.clip(p, 1e-12, 1.0)\n",
        "        q = np.clip(q, 1e-12, 1.0)\n",
        "        p /= p.sum()\n",
        "        q /= q.sum()\n",
        "        m = 0.5 * (p + q)\n",
        "        return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
        "\n",
        "    jensonshannon_distances = []\n",
        "    for row_att, row_out in zip(att_one, att_two):\n",
        "        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "    score = np.mean(jensonshannon_distances)\n",
        "    return score\n",
        "\n",
        "x = len(programs)\n",
        "S = np.zeros((x, x))\n",
        "for i in range(x):\n",
        "    print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
        "    for j in range(x):\n",
        "        if i < j:\n",
        "            similarities = []\n",
        "            program_one = programs[i]\n",
        "            program_two = programs[j]\n",
        "\n",
        "            for sentence in sentence_data:\n",
        "                h1, activations_one = program_one(sentence, tokenizer)\n",
        "                h2, activations_two = program_two(sentence, tokenizer)\n",
        "                similarities.append(program_similarity(activations_one, activations_two))\n",
        "            \n",
        "            S[i, j] = np.mean(similarities)\n",
        "            S[j, i] = S[i, j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1a3b62",
      "metadata": {},
      "outputs": [],
      "source": [
        "S = np.load('data/similarity_matrix_auto.npy')\n",
        "\n",
        "def group_similar_programs(programs, S, threshold=0.6):\n",
        "    groups, used = [], set()\n",
        "    for i in range(len(programs)):\n",
        "        if i in used: continue\n",
        "        group = [i]\n",
        "        used.add(i)\n",
        "        \n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for group_member in group:\n",
        "                for j in range(len(programs)):\n",
        "                    if j not in used and S[group_member, j] < threshold:\n",
        "                        group.append(j)\n",
        "                        used.add(j)\n",
        "                        changed = True\n",
        "        groups.append([programs[idx].__name__ for idx in group])\n",
        "    \n",
        "    return groups\n",
        "\n",
        "groups = group_similar_programs(programs, S, threshold=0.6)\n",
        "for i, group in enumerate(groups):\n",
        "    print(f\"Group {i+1}: {group}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4789011f",
      "metadata": {},
      "outputs": [],
      "source": [
        "folder = \"automation_results_bert\"\n",
        "programs = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        code_path = os.path.join(folder, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
        "        if os.path.exists(code_path):\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_head{head}\", code_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            try:\n",
        "                spec.loader.exec_module(module)\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        programs.append(attr)\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading program for Layer {layer}, Head {head}: {e}\")\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8f46ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8219481",
      "metadata": {},
      "outputs": [],
      "source": [
        "S = (S + S.T) / 2\n",
        "\n",
        "def group_similar_programs(programs, S, threshold=0.6):\n",
        "    groups, used = [], set()\n",
        "    for i in range(len(programs)):\n",
        "        if i in used: continue\n",
        "        group = [i]\n",
        "        used.add(i)\n",
        "        \n",
        "        changed = True\n",
        "        while changed:\n",
        "            changed = False\n",
        "            for group_member in group:\n",
        "                for j in range(len(programs)):\n",
        "                    if j not in used and S[group_member, j] < threshold:\n",
        "                        group.append(j)\n",
        "                        used.add(j)\n",
        "                        changed = True\n",
        "        groups.append([programs[idx].__name__ for idx in group])\n",
        "    \n",
        "    return groups\n",
        "\n",
        "groups = group_similar_programs(programs, S, threshold=0.25)\n",
        "for i, group in enumerate(groups):\n",
        "    if len(group) >= 1:\n",
        "        print(f\"Group {i+1}: {group}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ed1f38",
      "metadata": {},
      "outputs": [],
      "source": [
        "for program in programs:\n",
        "    print(program.__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b464ed4",
      "metadata": {},
      "outputs": [],
      "source": [
        "name_to_idx = {fn.__name__: i for i, fn in enumerate(programs)}\n",
        "new_order = [name_to_idx[name] for group in groups for name in group]\n",
        "S_grouped = S[np.ix_(new_order, new_order)]\n",
        "# S_grouped = np.load(\"data/similarity_matrix_auto.npy\")\n",
        "colors = \"Blues_r\"\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
        "im2 = ax.imshow(S_grouped, cmap=colors, aspect='auto')\n",
        "# ax.set_axis_off()\n",
        "# ax.set_xticks(range(len(programs)))\n",
        "# ax.set_yticks(range(len(programs)))\n",
        "# ax.set_xticklabels([p.__name__ for p in programs], rotation=90)\n",
        "# ax.set_yticklabels([p.__name__ for p in programs])\n",
        "# axis off\n",
        "ax.set_axis_off()\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.title(\"Automated Programs | Similarity Matrix\\n\", weight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b0e864",
      "metadata": {},
      "outputs": [],
      "source": [
        "# automated_program_similarity_analysis\n",
        "\n",
        "import os\n",
        "import importlib.util\n",
        "import types\n",
        "from AutoTokenizer import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "folder = \"automation_results_bert\"\n",
        "programs = []\n",
        "for layer in range(12):\n",
        "    for head in range(12):\n",
        "        code_path = os.path.join(folder, \"llm_code\", f\"programs-layer_{layer}\", f\"{head}_output.py\")\n",
        "        if os.path.exists(code_path):\n",
        "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_head{head}\", code_path)\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            module.__dict__['np'] = np\n",
        "            try:\n",
        "                spec.loader.exec_module(module)\n",
        "                for attr_name in dir(module):\n",
        "                    attr = getattr(module, attr_name)\n",
        "                    if isinstance(attr, types.FunctionType):\n",
        "                        programs.append(attr)\n",
        "                        break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading program for Layer {layer}, Head {head}: {e}\")\n",
        "                continue\n",
        "\n",
        "sentence_data = sentences[:10]\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def program_similarity(att_one, att_two):\n",
        "    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
        "        p = np.clip(p, 1e-12, 1.0)\n",
        "        q = np.clip(q, 1e-12, 1.0)\n",
        "        p /= p.sum()\n",
        "        q /= q.sum()\n",
        "        m = 0.5 * (p + q)\n",
        "        return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
        "\n",
        "    jensonshannon_distances = []\n",
        "    for row_att, row_out in zip(att_one, att_two):\n",
        "        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "    score = np.mean(jensonshannon_distances)\n",
        "    return score\n",
        "\n",
        "x = len(programs)\n",
        "S = np.zeros((x, x))\n",
        "for i in range(x):\n",
        "    print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
        "    for j in range(x):\n",
        "        if j % 24 == 0: print(f\"  inner loop {j}/{x}\")\n",
        "        if i != j:\n",
        "            similarities = []\n",
        "            program_one = programs[i]\n",
        "            program_two = programs[j]\n",
        "\n",
        "            for sentence in sentence_data:\n",
        "                h1, activations_one = program_one(sentence, tokenizer)\n",
        "                h2, activations_two = program_two(sentence, tokenizer)\n",
        "                similarities.append(program_similarity(activations_one, activations_two))\n",
        "            \n",
        "            S[i, j] = np.mean(similarities)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
