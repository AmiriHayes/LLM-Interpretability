{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23302cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automated_program_similarity_analysis\n",
    "\n",
    "import os\n",
    "import importlib.util\n",
    "import types\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "file = 'data/small_text.csv'\n",
    "df = pd.read_csv(file)\n",
    "sentences = []\n",
    "for paragraph in df['text']:\n",
    "    sentences.extend(sent_tokenize(paragraph))\n",
    "sentences = sentences[:10]\n",
    "\n",
    "folder = \"automation_results_bert\"\n",
    "programs = []\n",
    "for layer in range(12):\n",
    "    print(f\"Loading programs for Layer {layer}...\")\n",
    "    for head in range(12):\n",
    "        # print(f\"Loading program for Layer {layer}, Head {head}...\")\n",
    "        # code is in llm_code subfolder (e.g. automation_results_bert\\llm_code\\layer0_head0_code.py)\n",
    "        code_path = os.path.join(folder, \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
    "        # code_path = os.path.join(folder, \"llm_code\", f\"programs-layer_{layer}\", f\"{head}_output.py\")\n",
    "        if os.path.exists(code_path):\n",
    "            spec = importlib.util.spec_from_file_location(f\"layer{layer}_head{head}\", code_path)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            module.__dict__['np'] = np\n",
    "            try:\n",
    "                spec.loader.exec_module(module)\n",
    "                for attr_name in dir(module):\n",
    "                    attr = getattr(module, attr_name)\n",
    "                    if isinstance(attr, types.FunctionType):\n",
    "                        programs.append(attr)\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading program for Layer {layer}, Head {head}: {e}\")\n",
    "                continue\n",
    "print(f\"Loaded {len(programs)} programs.\")\n",
    "\n",
    "            \n",
    "            S[i, j] = np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_data = sentences[:10]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def program_similarity(att_one, att_two):\n",
    "    def js_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "        p = np.clip(p, 1e-12, 1.0)\n",
    "        q = np.clip(q, 1e-12, 1.0)\n",
    "        p /= p.sum()\n",
    "        q /= q.sum()\n",
    "        m = 0.5 * (p + q)\n",
    "        return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m))) \n",
    "\n",
    "    jensonshannon_distances = []\n",
    "    for row_att, row_out in zip(att_one, att_two):\n",
    "        jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
    "    score = np.mean(jensonshannon_distances)\n",
    "    return score\n",
    "\n",
    "x = len(programs)\n",
    "S = np.zeros((x, x))\n",
    "for i in range(1, x):\n",
    "    if i in [106, 128]:\n",
    "        S[i, :] = 0.8\n",
    "        continue\n",
    "    print(f\"calculating hypothesis similarities [{i}]: {programs[i].__name__}\")\n",
    "    for j in range(x):\n",
    "        if j % 36 == 0: print(f\"\\tinner loop {j/x:.0%}\")\n",
    "        if j in [106, 128]:\n",
    "            S[i, j] = 0.8\n",
    "            continue\n",
    "\n",
    "        if i != j:\n",
    "            similarities = []\n",
    "            program_one = programs[i]\n",
    "            program_two = programs[j]\n",
    "\n",
    "            for sentence in sentence_data:\n",
    "                h1, activations_one = program_one(sentence, tokenizer)\n",
    "                try:\n",
    "                    h2, activations_two = program_two(sentence, tokenizer)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sentence with programs {i} and {j}: {e}\")\n",
    "                    continue\n",
    "                similarities.append(program_similarity(activations_one, activations_two))\n",
    "        \n",
    "            S[i, j] = np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797d22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_similar_programs(programs, S, threshold=0.6):\n",
    "    groups, used = [], set()\n",
    "    for i in range(len(programs)):\n",
    "        if i in used: continue\n",
    "        group = [i]\n",
    "        used.add(i)\n",
    "        \n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            for group_member in group:\n",
    "                for j in range(len(programs)):\n",
    "                    if j not in used and S[group_member, j] < threshold:\n",
    "                        group.append(j)\n",
    "                        used.add(j)\n",
    "                        changed = True\n",
    "        groups.append([programs[idx].__name__ for idx in group])\n",
    "    \n",
    "    return groups\n",
    "\n",
    "groups = group_similar_programs(programs, S, threshold=0.2)\n",
    "for i, group in enumerate(groups):\n",
    "    print(f\"Group {i+1}: {group}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make S a symmetric matrix\n",
    "S = (S + S.T) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aae9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2195e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_similar_programs(programs, S, threshold=0.6):\n",
    "    groups, used = [], set()\n",
    "    for i in range(len(programs)):\n",
    "        if i in used:\n",
    "            continue\n",
    "        \n",
    "        group = [i]\n",
    "        used.add(i)\n",
    "        \n",
    "        for j in range(len(programs)):\n",
    "            if j not in used and S[i, j] < threshold:\n",
    "                group.append(j)\n",
    "                used.add(j)\n",
    "        \n",
    "        groups.append([programs[idx].__name__ for idx in group])\n",
    "    \n",
    "    return groups\n",
    "\n",
    "groups = group_similar_programs(programs, S, threshold=0.1)\n",
    "for i, group in enumerate(groups):\n",
    "    print(f\"Group {i+1}: {group}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through bert and bert2 scores, take best score and build sq_Score matrix\n",
    "\n",
    "scores = np.array([])\n",
    "\n",
    "for layer in range(12):\n",
    "    for head in range(12):\n",
    "        score1_path = f\"automation_bert/scores/layer{layer}_head{head}_score.txt\"\n",
    "        score2_path = f\"automation_bert_2/scores/layer{layer}_head{head}_score.txt\"\n",
    "        score1, score2 = -1, -1\n",
    "        if os.path.exists(score1_path):\n",
    "            with open(score1_path, \"r\") as f:\n",
    "                try:\n",
    "                    score1 = float(f.read().strip())\n",
    "                except:\n",
    "                    score1 = 1\n",
    "        if os.path.exists(score2_path):\n",
    "            with open(score2_path, \"r\") as f:\n",
    "                try:\n",
    "                    score2 = float(f.read().strip())\n",
    "                except:\n",
    "                    score2 = 1\n",
    "        best_score = min(score1, score2)\n",
    "        scores = np.append(scores, best_score)\n",
    "\n",
    "sq_scores = scores.reshape((12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6f3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_score = np.reshape(scores, (12, 12))\n",
    "\n",
    "colors = \"Grays\"\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
    "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
    "cmap = plt.cm.get_cmap(colors).copy()\n",
    "cmap.set_bad(color='gray')\n",
    "im2 = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
    "im2.set_clim(vmin=0, vmax=1)\n",
    "cbar = plt.colorbar(im2, ax=ax)\n",
    "ax.set_xticks(range(12))\n",
    "ax.set_yticks(range(12))\n",
    "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
    "ax.set_yticklabels([i for i in range(12)])\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# put a space element in between automation and scores in text\n",
    "title = (\n",
    "    r'$\\mathbf{Automation \\ Scores}$'  # \\mathbf makes the text bold\n",
    "    '\\n\\nMethod: Greedy Refinement'\n",
    "    f'\\nScores | {model.config.architectures[0]}'  # Example: replaced model.config...\n",
    ")\n",
    "plt.title(f\"{title}\\n\")\n",
    "plt.show()\n",
    "plt.savefig('automation_scores_greedy_refinement_k1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a322a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the (layer, head) associated with the 10 highest scores from automation_results_bert, print the function name from the python file too\n",
    "\n",
    "import os\n",
    "folder = \"automation_results_bert/scores\"\n",
    "scores = []\n",
    "for layer in range(12):\n",
    "    for head in range(12):\n",
    "        score_path = os.path.join(folder, f\"layer{layer}_head{head}_score.txt\")\n",
    "        if os.path.exists(score_path):\n",
    "            with open(score_path, \"r\") as f:\n",
    "                score = float(f.read().strip())\n",
    "                scores.append((layer, head, score))\n",
    "\n",
    "names = []\n",
    "for layer, head, _ in scores:\n",
    "    code_path = os.path.join(\"automation_results_bert\", \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
    "    if os.path.exists(code_path):\n",
    "        with open(code_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip().startswith(\"def \"):\n",
    "                    func_name = line.strip().split()[1].split('(')[0]\n",
    "                    names.append(func_name)\n",
    "                    break\n",
    "    else:\n",
    "        names.append(\"N/A\")\n",
    "\n",
    "\n",
    "highest_scores = sorted(scores, key=lambda x: x[2])[:10]\n",
    "print(\"Top 10 highest scores (layer, head, score):\")\n",
    "for (layer, head, score), func_name in zip(highest_scores, names):\n",
    "    print(f\"Layer {layer}, Head {head}: {score} | Function: {func_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find \"coreferent_entity_focus\" function which (layer, head) is it\n",
    "\n",
    "for layer in range(12):\n",
    "    for head in range(12):\n",
    "        code_path = os.path.join(\"automation_results_bert\", \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
    "        if os.path.exists(code_path):\n",
    "            with open(code_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    if line.strip().startswith(\"def coreferent_entity_focus\"):\n",
    "                        print(f\"'coreferent_entity_focus' found in Layer {layer}, Head {head}\")\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d8239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the (layer, head) associated with the following python functions: ['punctuation_conjunction_attention', 'punctuation_conjunction_dependency', 'conjunction_coherence', 'coordination_alignment'], also print score\n",
    "\n",
    "import os\n",
    "folder = \"automation_results_bert/scores\"\n",
    "functions = ['punctuation_conjunction_attention', 'punctuation_conjunction_dependency', 'conjunction_coherence', 'coordination_alignment']\n",
    "folders = []\n",
    "\n",
    "for layer in range(12):\n",
    "    for head in range(12):\n",
    "        code_path = os.path.join(\"automation_results_bert\", \"llm_code\", f\"layer{layer}_head{head}_code.py\")\n",
    "        if os.path.exists(code_path):\n",
    "            with open(code_path, \"r\") as f:\n",
    "                code = f.read()\n",
    "                for func in functions:\n",
    "                    if f\"def {func}(\" in code:\n",
    "                        folders.append((layer, head, func)) \n",
    "\n",
    "print(\"Functions found in (layer, head, function):\")\n",
    "for layer, head, func in folders:\n",
    "    print(f\"Layer {layer}, Head {head}: {func}\")\n",
    "\n",
    "print(\"Scores found in (layer, head, function):\\n\")\n",
    "for layer, head, func in folders:\n",
    "    score_path = os.path.join(folder, f\"layer{layer}_head{head}_score.txt\")\n",
    "    if os.path.exists(score_path):\n",
    "        with open(score_path, \"r\") as f:\n",
    "            score = f.read().strip()\n",
    "            print(f\"Layer {layer}, Head {head}: {func} - Score: {float(score):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de61a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall matplotlib\n",
    "# !pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "name_to_idx = {fn.__name__: i for i, fn in enumerate(programs)}\n",
    "new_order = [name_to_idx[name] for group in groups for name in group]\n",
    "S_grouped = S[np.ix_(new_order, new_order)]\n",
    "colors = \"Purples_r\"\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "im2 = ax.imshow(S_grouped, cmap=colors, aspect='auto')\n",
    "# ax.set_axis_off()\n",
    "ax.set_xticks(range(len(programs)))\n",
    "ax.set_yticks(range(len(programs)))\n",
    "ax.set_xticklabels([p.__name__ for p in programs], rotation=90)\n",
    "ax.set_yticklabels([p.__name__ for p in programs])\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.title(\"Similarity Matrix\\n\", weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8091a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum scores in roberta/scores txt\n",
    "\n",
    "import os\n",
    "folder = \"automation_results_roberta/scores\"\n",
    "scores = []\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder, filename), 'r') as f:\n",
    "            content = f.read().strip()\n",
    "            try:\n",
    "                content = float(content)\n",
    "                if content != content:  # Check for NaN\n",
    "                    content = 1.0\n",
    "            except ValueError:\n",
    "                content = 1.0\n",
    "            scores.append(content)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install regex\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all programs from automation_refinement/master_list and load these python functions as patterns = [executable functions]\n",
    "\n",
    "import os\n",
    "import importlib.util\n",
    "import types\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import PreTrainedTokenizerBase, PreTrainedModel\n",
    "from typing import Optional, Tuple, Callable\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "folder = \"automation_refinement_gpt2/master_list\"\n",
    "patterns = []\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if filename.endswith(\".py\"):\n",
    "        code_path = os.path.join(folder, filename)\n",
    "        spec = importlib.util.spec_from_file_location(f\"module_{filename[:-3]}\", code_path)\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        module.__dict__['np'] = np\n",
    "        # get pretrainedtokenizerbase, from typing import Optional, Tuple, Callable\n",
    "        module.__dict__['PreTrainedTokenizerBase'] = PreTrainedTokenizerBase\n",
    "        module.__dict__['Optional'] = Optional\n",
    "        module.__dict__['Tuple'] = Tuple\n",
    "        module.__dict__['Callable'] = Callable\n",
    "        module.__dict__['spacy'] = spacy\n",
    "        \n",
    "\n",
    "        try:\n",
    "            spec.loader.exec_module(module)\n",
    "            for attr_name in dir(module):\n",
    "                attr = getattr(module, attr_name)\n",
    "                if isinstance(attr, types.FunctionType):\n",
    "                    patterns.append(attr)\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading program from {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Loaded {len(patterns)} patterns.\")\n",
    "for i, prog in enumerate(patterns):\n",
    "    print(f\"{i}: {prog.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5429c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91659e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make patterns_strong excluding [\"adverbial_modulation\", \"conjunction_based_grouping\", \"dependencies\", \"pos_alignment\", \"semantics_comma_separation\"]:\n",
    "patterns_strong = [p for p in patterns if p.__name__ not in [\"adverbial_modulation\", \"conjunction_based_grouping\", \"dependencies\", \"pos_alignment\", \"semantics_comma_separation\"]]\n",
    "len(patterns_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855df463",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = sentences[135:137]\n",
    "scores = np.zeros((model.config.num_hidden_layers, model.config.num_attention_heads))\n",
    "top_k = 1\n",
    "\n",
    "for layer in range(num_layers):\n",
    "    for head in range(num_heads):\n",
    "        print(f\"Analyzing Layer {layer}, Head {head}...\")\n",
    "        sentence_scores = []\n",
    "        for sentence in test_sentences:\n",
    "            inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "            attention = outputs.attentions[layer][0, head].detach().numpy()\n",
    "            y = attention.flatten()\n",
    "\n",
    "            X = []\n",
    "            for pattern in patterns_strong:\n",
    "                X.append(pattern(sentence, torch_tokenizer)[1].flatten())\n",
    "            X_n = np.array(X).T\n",
    "            y = y.flatten()\n",
    "\n",
    "            # avoid ValueError: Input X contains NaN.\n",
    "            X_n = np.nan_to_num(X_n)\n",
    "            y = np.nan_to_num(y)\n",
    "\n",
    "            reg = LinearRegression().fit(X_n, y)\n",
    "            side_length = int(np.sqrt(len(y)))\n",
    "            y = y.reshape((side_length, side_length))\n",
    "\n",
    "            # pred_att = reg.intercept_ + sum(coef * mat for coef, mat in zip(reg.coef_, X))\n",
    "            # pred att should be the intercept + sum of top_k patterns based on magnitude of coef\n",
    "            top_indices = np.argsort(np.abs(reg.coef_))[-top_k:]\n",
    "            pred_att = reg.intercept_ + sum(reg.coef_[i] * X[i] for i in top_indices)\n",
    "            pred_att = pred_att.reshape((side_length, side_length))\n",
    "\n",
    "            if top_k == 1:\n",
    "                #pred_att is just the single pattern with highest coef, it isn't equal to a sum at all\n",
    "                fn_highest_coeff = patterns_strong[np.argmax(np.abs(reg.coef_))]\n",
    "                pred_att = fn_highest_coeff(sentence, torch_tokenizer)[1]\n",
    "                print(f\"Using pattern: {fn_highest_coeff.__name__}\")\n",
    "\n",
    "            jensonshannon_distances = []\n",
    "            for row_att, row_out in zip(y, pred_att):\n",
    "                jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
    "            score = np.mean(jensonshannon_distances)\n",
    "            sentence_scores.append(score)\n",
    "        \n",
    "        scores[layer, head] = np.mean(sentence_scores)\n",
    "        # print(f\"Score for Layer {layer}, Head {head}: {scores[layer, head]}\")\n",
    "\n",
    "# for each head: do linear interpolation on patterns, set pred_att to sum with top_k hypotheses based on parameter magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44699705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT DIFFERENT SUMMARY SCORES FOR THE MODEL\n",
    "\n",
    "max_score = model.config.num_hidden_layers * model.config.num_attention_heads\n",
    "raw_scores = [111, 92, 62, 65, 56]\n",
    "labels = ['Random \\nToken Baseline', 'Automatic\\nPrograms', 'Auto, K=1\\nPrograms', 'Best Fit\\nPrograms', 'Linear Weight\\nPrograms']\n",
    "colors = ['darkred', 'darkblue', '#6aa84f', '#800080', \"darkorange\"]\n",
    "\n",
    "# Normalize scores: lower scores become higher bars\n",
    "scores = [(score / max_score) for score in raw_scores]\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = plt.bar(labels, scores, color=colors, width=0.6)\n",
    "\n",
    "# Add text labels on top of bars\n",
    "for bar, raw, norm in zip(bars, raw_scores, scores):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
    "             f'{norm:.2f}\\n[ {int(raw)} / {max_score} ]', ha='center', va='bottom', fontsize=14)\n",
    "ax.set_facecolor('#F5F5F5')\n",
    "\n",
    "plt.ylim(0, 1.0)\n",
    "# plt.title('Normalized Error (1 - Score / Max Score)')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.text(plt.xlim()[0]-0.7, plt.ylim()[1]+0.05, '[bad hypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
    "plt.text(plt.xlim()[0]-0.7, plt.ylim()[0]-0.13, '[well-fitting\\nhypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
    "plt.ylabel('Normalized Model Scores', fontsize=16, labelpad=20)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"bert_head_greedy_scores_2.npy\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all nan values to mean in scores\n",
    "\n",
    "mean_score = np.nanmean(scores)\n",
    "scores = np.where(np.isnan(scores), mean_score, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982377bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import PowerNorm\n",
    "sq_score = scores\n",
    "\n",
    "colors = \"Grays\"\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "# masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
    "# masked_sq converts na to mean\n",
    "masked_sq = np.ma.masked_invalid(sq_score)\n",
    "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
    "cmap = plt.cm.get_cmap(colors).copy()\n",
    "cmap.set_bad(color='gray')\n",
    "im2 = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
    "im2.set_clim(vmin=0, vmax=1)\n",
    "cbar = plt.colorbar(im2, ax=ax)\n",
    "ax.set_xticks(range(12))\n",
    "ax.set_yticks(range(12))\n",
    "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
    "ax.set_yticklabels([i for i in range(12)])\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# put a space element in between automation and scores in text\n",
    "title = (\n",
    "    r'$\\mathbf{Automation \\ Scores}$'  # \\mathbf makes the text bold\n",
    "    '\\n\\nMethod: Refinement w/ K=1'\n",
    "    f'\\nScores | {model.config.architectures[0]}'  # Example: replaced model.config...\n",
    ")\n",
    "plt.title(f\"{title}\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928266b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f47a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/bert_head_greedy_scores.npy\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9ff964",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_score = np.reshape(scores, (12, 12))\n",
    "\n",
    "colors = \"Grays\"\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "masked_sq = np.ma.masked_where(sq_score == -1, sq_score)\n",
    "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
    "cmap = plt.cm.get_cmap(colors).copy()\n",
    "cmap.set_bad(color='gray')\n",
    "im2 = ax.imshow(masked_sq, cmap=cmap, aspect='auto', norm=norm)\n",
    "im2.set_clim(vmin=0, vmax=1)\n",
    "cbar = plt.colorbar(im2, ax=ax)\n",
    "ax.set_xticks(range(12))\n",
    "ax.set_yticks(range(12))\n",
    "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
    "ax.set_yticklabels([i for i in range(12)])\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# put a space element in between automation and scores in text\n",
    "title = (\n",
    "    r'$\\mathbf{Automation \\ Scores}$'  # \\mathbf makes the text bold\n",
    "    '\\n\\nMethod: Greedy Refinement'\n",
    "    f'\\nScores | {model.config.architectures[0]}'  # Example: replaced model.config...\n",
    ")\n",
    "plt.title(f\"{title}\\n\")\n",
    "plt.show()\n",
    "plt.savefig('automation_scores_refinement_k1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(data):\n",
    "    data = np.array(data)\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - (1.5 * IQR)\n",
    "    upper_bound = Q3 + (1.5 * IQR)\n",
    "\n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound, Q1, Q3, IQR\n",
    "\n",
    "outliers, lower_bound, upper_bound, Q1, Q3, IQR = find_outliers(scores)\n",
    "\n",
    "def plot_scores_boxplot(scores):\n",
    "    plt.figure(figsize=(3.2,8))\n",
    "    plt.boxplot(\n",
    "        scores,\n",
    "        positions=[0.75], \n",
    "        vert=True,\n",
    "        patch_artist=True,\n",
    "        medianprops={'color': 'black', 'linewidth': 3},\n",
    "        boxprops={'facecolor': 'gray', 'edgecolor': 'black'},\n",
    "        flierprops={'marker': 'D', 'markerfacecolor': 'black', 'markersize': 3, 'linestyle': 'none'}\n",
    "    )\n",
    "\n",
    "    plt.title('Auto, K=1 | BERT', fontsize=14, weight='bold')\n",
    "    plt.ylabel('Automation Scores', fontsize=12)\n",
    "    plt.xticks([])\n",
    "    plt.ylim(0, 1.05)\n",
    "\n",
    "    #insert the text 'WIP' in center of plot\n",
    "    # plt.text(0.75, 0.5, 'WIP', fontsize=12, ha='center', va='center')\n",
    "\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    x = np.ones_like(scores)\n",
    "    plt.scatter(\n",
    "        x,\n",
    "        scores,\n",
    "        color='gray',\n",
    "        edgecolor='black',\n",
    "        s=30,\n",
    "        alpha=0.9,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "flattened_scores = scores.flatten()\n",
    "plot_scores_boxplot(flattened_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41721ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "colors = \"Grays_r\"\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "score_threshold = 0.4\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "highlighted_sq = np.where(sq_score < score_threshold, sq_score, np.nan)\n",
    "# make all non-highlighted values white (1.0)\n",
    "highlighted_sq = np.where(np.isnan(highlighted_sq), 1.0, highlighted_sq)\n",
    "norm = PowerNorm(gamma=1.8, vmin=sq_score.min(), vmax=sq_score.max())\n",
    "cmap = plt.cm.get_cmap(colors).copy()\n",
    "cmap.set_bad(color='gray')\n",
    "im2 = ax.imshow(highlighted_sq, cmap=cmap, aspect='auto', norm=norm)\n",
    "im2.set_clim(vmin=0, vmax=1)\n",
    "cbar = plt.colorbar(im2, ax=ax)\n",
    "ax.set_xticks(range(12))\n",
    "ax.set_yticks(range(12))\n",
    "ax.set_xticklabels([i for i in range(12)], rotation=90)\n",
    "ax.set_yticklabels([i for i in range(12)])\n",
    "# import matplotlib\n",
    "# print(\"usetex:\", matplotlib.rcParams['text.usetex'])\n",
    "# plt.rcParams['text.usetex'] = False\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# make automation bold\n",
    "# get number of highlighted scores\n",
    "num_highlighted = np.sum(sq_score < score_threshold)\n",
    "title = (\n",
    "    r'$\\mathbf{Highlighted\\ Scores}$'\n",
    "    '\\n\\nMethod: Refinement w/ K=1'\n",
    "    f'\\n {num_highlighted} scores < {score_threshold} ({num_highlighted/(len(sq_score)**2)*100:.0f}%) | {model.config.architectures[0]}\\n')\n",
    "# title = \"Automation Scores\\n\"\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import csv\n",
    "\n",
    "# use bert\n",
    "torch_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "torch_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "num_layers = torch_model.config.num_hidden_layers\n",
    "num_heads = torch_model.config.num_attention_heads\n",
    "activations = {} \n",
    "\n",
    "file = 'data/small_text.csv'\n",
    "df = pd.read_csv(file)\n",
    "sentences = []\n",
    "for paragraph in df['text']:\n",
    "    sentences.extend(sent_tokenize(paragraph))\n",
    "sentences = sentences[:10_000]\n",
    "\n",
    "short = sentences[:8]\n",
    "csv_file_name = \"data/bert_refinement.csv\"\n",
    "with open(csv_file_name, 'a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for pattern in patterns:\n",
    "        if pattern.__name__ in [\"adverbial_modulation\", \"conjunction_based_grouping\", \"dependencies\", \"pos_alignment\", \"semantics_comma_separation\"]: continue\n",
    "        print(f\"\\nCurrently Analyzing pattern: {pattern.__name__}\")\n",
    "        avg_score = []\n",
    "        for idx, sentence in enumerate(short):\n",
    "            print(f\"\\tProcessing sentence {idx}/{len(short)}\")\n",
    "            for i in range(num_layers):\n",
    "                for j in range(num_heads):\n",
    "                    if i != 3 or j != 9: continue\n",
    "                    nlp = spacy.load(\"en_core_web_sm\")\n",
    "                    score = score_prediction(torch_model, torch_tokenizer, (i, j), pattern, sentence, distance=\"jsd\", output=False)\n",
    "                    if score < 0.55:\n",
    "                        avg_score.append((idx, pattern.__name__, i, j, score))\n",
    "        \n",
    "        score_dict = {}\n",
    "        for idx, pattern_name, i, j, score in avg_score:\n",
    "            score_dict.setdefault((i, j), []).append((pattern_name, score))\n",
    "        for (i, j), values in score_dict.items():\n",
    "            scores = [score for _, score in values]\n",
    "            avg_score_val = sum(scores) / len(scores)\n",
    "            pattern_name = values[0][0]\n",
    "            activations[(i, j)] = (pattern_name, avg_score_val)\n",
    "            print(f\"Layer {i}, Head {j} - Score: {avg_score_val:.2f}\")\n",
    "            writer.writerow([i, j, pattern.__name__, avg_score_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all programs from automation_refinement/master_list and load these python functions as patterns = [executable functions]\n",
    "# loop over heads and using three sentences get an average score for each head for each pattern, save the name and best score of best fitting pattern\n",
    "# build a matrix (layers * heads) with the best fitting pattern_name and best_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83baa4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT DIFFERENT SUMMARY SCORES FOR THE MODEL\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_score = 144\n",
    "raw_scores = [114, 92.05, 56, 77.9, 67]\n",
    "labels = ['Random Token\\n Baseline', 'Automatic\\nPrograms', 'Automatic\\nw/ Refinement', 'Best Fit\\nPrograms', 'Linear Weight\\nPrograms']\n",
    "colors = ['darkred', 'darkblue', '#FF8C00', '#6aa84f', '#800080']\n",
    "\n",
    "# Normalize scores: lower scores become higher bars\n",
    "scores = [(score / max_score) for score in raw_scores]\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = plt.bar(labels, scores, color=colors, width=0.6)\n",
    "\n",
    "# Add text labels on top of bars\n",
    "for bar, raw, norm in zip(bars, raw_scores, scores):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02,\n",
    "             f'{norm:.2f}', ha='center', va='bottom', fontsize=14)\n",
    "ax.set_facecolor('#F5F5F5')\n",
    "\n",
    "plt.ylim(0, 1.0)\n",
    "# plt.title('Normalized Error (1 - Score / Max Score)')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.text(plt.xlim()[0]-0.7, plt.ylim()[1]+0.05, '[bad hypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
    "plt.text(plt.xlim()[0]-0.7, plt.ylim()[0]-0.13, '[well-fitting\\nhypotheses]', ha='left', va='bottom', fontsize=12, color='gray')\n",
    "plt.ylabel('Normalized Model Scores', fontsize=16, labelpad=20)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f9601",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x, edge_index,n_id, feature_vec):\n",
    "        transformed_feature = self.feature_transform(feature_vec)\n",
    "\n",
    "        text = [self.texts[i] for i in n_id.cpu().numpy()]\n",
    "        tokens = self.tokenizer(text, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "        tokens = tokens.to(edge_index.device)\n",
    "        input_embeddings = self.text_model.get_input_embeddings()(tokens['input_ids'])\n",
    "        \n",
    "        if self.soft == False:\n",
    "            outputs = self.text_model(inputs_embeds=input_embeddings)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            text_embedding = hidden_states[:, 0, :]\n",
    "\n",
    "        graph_embedding = transformed_feature\n",
    "        count = 0\n",
    "        \n",
    "        for gcn_layer in self.gcn_layers:\n",
    "            edge_index = edge_index.long()\n",
    "            \n",
    "            if self.soft:\n",
    "                graph_embedding = graph_embedding.unsqueeze(1)\n",
    "                modified_embeddings = torch.cat((graph_embedding, input_embeddings), dim=1)\n",
    "                attention_mask = tokens['attention_mask']\n",
    "                batch_size = attention_mask.shape[0]\n",
    "                new_token_mask = torch.ones((batch_size, 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
    "                attention_mask = torch.cat([new_token_mask, attention_mask], dim=1)\n",
    "                outputs = self.text_model(inputs_embeds=modified_embeddings, attention_mask=attention_mask)\n",
    "                hidden_states = outputs.last_hidden_state\n",
    "                text_embedding = hidden_states[:, 0, :]\n",
    "\n",
    "            graph_embedding_for_attention = graph_embedding.squeeze(1).unsqueeze(0)  # [1, batch_size, embedding_dim]\n",
    "            text_embedding_for_attention = text_embedding.unsqueeze(0)  # [1, batch_size, embedding_dim]\n",
    "            text_to_graph_attention, _ = self.cross_attention(graph_embedding_for_attention, \n",
    "                                                              text_embedding_for_attention,text_embedding_for_attention)\n",
    "            text_to_graph_attention = text_to_graph_attention.squeeze(0)  # [batch_size, embedding_dim]\n",
    "\n",
    "            combined_embedding = (text_embedding + text_to_graph_attention) / 2\n",
    "            graph_embedding = gcn_layer(combined_embedding, edge_index)\n",
    "\n",
    "        return self.classifier(graph_embedding),graph_embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
