{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6443588e",
      "metadata": {
        "id": "6443588e"
      },
      "source": [
        "Author: Amiri Hayes \\\n",
        "Date Updated: 7/7/25 \\\n",
        "Title: ViewLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3e36de",
      "metadata": {
        "id": "fb3e36de"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbe22fd",
      "metadata": {
        "id": "7cbe22fd"
      },
      "outputs": [],
      "source": [
        "# POSITIONAL FILTERING PATTERNS:\n",
        "\n",
        "def next_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i+1] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Next Head Attention Pattern\", out\n",
        "\n",
        "def previous_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i-1] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Previous Head Attention Pattern\", out\n",
        "\n",
        "def same_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    for i in range(1, len_seq-1):\n",
        "        out[i, i] = 1\n",
        "    out[0,0] = 1\n",
        "    out[-1,0] = 1\n",
        "    return \"Same Head Attention Pattern\", out\n",
        "\n",
        "def punctuation_attention(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = tokenizer.convert_ids_to_tokens(toks.input_ids[0])\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "    punctuation_indices = [i for i, tok in enumerate(words) if any(p in tok for p in punctuation_set)]\n",
        "    for i in range(len_seq):\n",
        "        future_punct = [j for j in punctuation_indices if j > i]\n",
        "        if future_punct:\n",
        "            for j in future_punct:\n",
        "                out[i, j] = 1.0\n",
        "            out[i] /= out[i].sum()\n",
        "        else:\n",
        "            out[i, i] = 1.0\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Punctuation Pattern\", out\n",
        "\n",
        "def repeated_attention(sentence, tokenizer):\n",
        "    return \"\", 0\n",
        "\n",
        "# LINGUISTIC ROLE ALIGNMENT PATTERNS:\n",
        "\n",
        "def pos_alignment(sentence, tokenizer):\n",
        "  return \"\", 0\n",
        "\n",
        "def dependencies(sentence, tokenizer):\n",
        "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
        "    len_seq = len(toks.input_ids[0])\n",
        "    out = np.zeros((len_seq, len_seq))\n",
        "    words = sentence.split()\n",
        "    doc = nlp(\" \".join(words))\n",
        "    check_errors = False\n",
        "    if check_errors:\n",
        "        if len(doc) == 0: print(\"problem, doc empty\")\n",
        "        if len(doc) != (len_seq-2): print(\"problem, doc length mismatch\", len(doc), len(toks)-2)\n",
        "    for stok in doc:\n",
        "        parent_index = stok.i\n",
        "        for child_stok in stok.children:\n",
        "            child_index = child_stok.i\n",
        "            out[parent_index+1, child_index+1] = 1\n",
        "            out[child_index+1, parent_index+1] = 1\n",
        "    out[0, 0] = 1\n",
        "    out[-1, 0] = 1\n",
        "    out += 1e-4\n",
        "    out = out / out.sum(axis=1, keepdims=True)\n",
        "    return \"Dependency Parsing Pattern\", out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9c0f33",
      "metadata": {
        "id": "bc9c0f33"
      },
      "outputs": [],
      "source": [
        "def repeated_attention(sentence, model, tokenizer, head, layer, output=False):\n",
        "\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad(): outputs = model(**inputs)\n",
        "    attention = outputs.attentions[layer]\n",
        "\n",
        "    attn_matrix = attention[0, head]\n",
        "    seq_len = attn_matrix.shape[0]\n",
        "\n",
        "    if output:\n",
        "        for i, row in enumerate(attn_matrix):\n",
        "            probs = []\n",
        "            for val in row: probs.append(f\"{val.item():.2f}\")\n",
        "            print(f\"Token {i}: {probs}\")\n",
        "\n",
        "    token_counts = Counter(sentence.split())\n",
        "    print(f\"Token counts: {token_counts}\")\n",
        "    repeated_tokens = {tok for tok, count in token_counts.items() if count > 1}\n",
        "\n",
        "    if not repeated_tokens:\n",
        "        return 0\n",
        "\n",
        "    repeated_indices = [i for i, tok in enumerate(inputs) if tok in repeated_tokens]\n",
        "\n",
        "    for i in repeated_indices:\n",
        "        attn_to_repeats = sum(attn_matrix[i][j] for j in repeated_indices)\n",
        "        if attn_to_repeats > 0.5:\n",
        "            if output: print(f\"Same token pattern detected\")\n",
        "            return 1\n",
        "\n",
        "    if output: print(f\"No repeated attention pattern detected\")\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d51a2fa",
      "metadata": {
        "id": "9d51a2fa"
      },
      "outputs": [],
      "source": [
        "# GENERATE & VISUALIZE ATTENTION PATTERN SCORES\n",
        "\n",
        "def js_divergence(p, q):\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    q = np.clip(q, 1e-12, 1.0)\n",
        "    p /= p.sum()\n",
        "    q /= q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (np.sum(p * np.log(p / m)) + np.sum(q * np.log(q / m)))\n",
        "\n",
        "def score_prediction(sentence, torch_model, torch_tokenizer, head_loc, pattern, distance=\"jsd\", output=False):\n",
        "    layer, head = head_loc\n",
        "    tokens = torch_tokenizer(sentence, return_tensors=\"pt\")\n",
        "    if torch_model.config.architectures[0] == 'T5ForConditionalGeneration':\n",
        "        decoder_input_ids = tokens[\"input_ids\"]\n",
        "        outputs = torch_model(input_ids=tokens[\"input_ids\"], decoder_input_ids=decoder_input_ids, output_attentions=True)\n",
        "        att = outputs.encoder_attentions[layer][0, head].detach().numpy()\n",
        "    else:\n",
        "        att = torch_model(**tokens, output_attentions=True).attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "    name, pred_att = pattern(sentence, torch_tokenizer)\n",
        "\n",
        "    if distance == \"raw\":\n",
        "        score = np.abs(att - pred_att).sum()\n",
        "    elif distance == \"jsd\":\n",
        "      jensonshannon_distances = []\n",
        "      for row_att, row_out in zip(att, pred_att):\n",
        "          jensonshannon_distances.append(np.sqrt(js_divergence(row_att, row_out)))\n",
        "      score = np.mean(jensonshannon_distances)\n",
        "\n",
        "    if output:\n",
        "        colors=\"inferno\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        im1 = axes[0].imshow(att, cmap=colors, aspect='auto')\n",
        "        axes[0].set_title(\"Actual Head Attention\")\n",
        "        fig.colorbar(im1, ax=axes[0])\n",
        "        im2 = axes[1].imshow(pred_att, cmap=colors, aspect='auto')\n",
        "        axes[1].set_title(\"Optimal Head Attention for Pattern\")\n",
        "        fig.colorbar(im2, ax=axes[1])\n",
        "\n",
        "        underlined_name_unicode = \"\".join([char + '\\u0332' for char in name])\n",
        "        plt.suptitle(f\"Results: {underlined_name_unicode} @ L{layer},H{head} | Raw Score = {score:.2f}\\n\\nSentence: \\\"{sentence}\\\"\", fontsize=16)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOWNLOAD ADDITIONAL TEXT DATA:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/small_text.csv')\n",
        "\n",
        "sentences = []\n",
        "for paragraph in df['text']:\n",
        "    sentences.extend(sent_tokenize(paragraph))\n",
        "sentences = sentences[:10_000]\n",
        "\n",
        "print(\"Sentences from Tiny Stories Dataset:\")\n",
        "print(sentences[20:30])"
      ],
      "metadata": {
        "id": "n899pxRzSWRe"
      },
      "id": "n899pxRzSWRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff49f93",
      "metadata": {
        "id": "1ff49f93"
      },
      "outputs": [],
      "source": [
        "# ANALYZE PATTERN AT LAYER AND HEAD\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "\n",
        "models = [\"bert-base-uncased\", \"openai-community/gpt2\", \"google-t5/t5-small\"]\n",
        "model_name = models[0]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
        "model.eval()\n",
        "layer, head = 0, 10\n",
        "\n",
        "score_prediction(sentences[0], model, tokenizer, (layer, head), next_attention, distance=\"jsd\", output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd29726",
      "metadata": {
        "id": "9fd29726"
      },
      "outputs": [],
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (1 SENTENCE)\n",
        "\n",
        "def visualize_full_model(sentence, torch_model, torch_tokenizer, pattern, title, bias_towards_best=0.9):\n",
        "    num_layers = model.config.num_hidden_layers\n",
        "    num_heads = model.config.num_attention_heads\n",
        "    model_viz = np.zeros((num_layers, num_heads))\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        for j in range(num_heads):\n",
        "            score = score_prediction(sentence, torch_model, torch_tokenizer, (i, j), pattern, distance=\"jsd\")\n",
        "            model_viz[i, j] = score\n",
        "    print(f\"Best Score: {min(map(min, model_viz)):.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=model_viz.min(), vmax=model_viz.max())\n",
        "    plt.imshow(model_viz, cmap='bone', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_three = list(zip(*np.unravel_index(np.argsort(model_viz, axis=None)[:3], model_viz.shape)))\n",
        "    for tuple_val in top_three:\n",
        "        layer, head = tuple_val\n",
        "        score = model_viz[layer, head]\n",
        "        print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return model_viz, top_three\n",
        "\n",
        "from matplotlib.colors import PowerNorm\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "sentence = \"Hi. How are you? I'm fine! Thanks. Bye, see you tomorrow.\"\n",
        "pattern_name = \"Punctuation Attention Pattern\"\n",
        "underlined_name_unicode = \"\".join([char + '\\u0332' for char in pattern_name])\n",
        "visualize_full_model(sentence, model, tokenizer, punctuation_attention, title=f\"Top Heads: {underlined_name_unicode} | {model_name}\\nSentence: \\\"{sentence}\\\"\\n\", bias_towards_best=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ANALYZE ALL HEADS FOR A PATTERN (MULTIPLE SENTENCES)\n",
        "\n",
        "def visualize_full_model(sentences, torch_model, torch_tokenizer, pattern, title, bias_towards_best=0.9):\n",
        "    num_layers = torch_model.config.num_hidden_layers\n",
        "    num_heads = torch_model.config.num_attention_heads\n",
        "\n",
        "    average_score = np.zeros((num_layers, num_heads))\n",
        "    for sentence in tqdm(sentences):\n",
        "        model_score = np.zeros((num_layers, num_heads))\n",
        "        for i in range(num_layers):\n",
        "            for j in range(num_heads):\n",
        "                score = score_prediction(sentence, torch_model, torch_tokenizer, (i, j), pattern)\n",
        "                model_score[i, j] = score\n",
        "        average_score += model_score\n",
        "    average_score /= len(sentences)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    norm = PowerNorm(gamma=1-bias_towards_best, vmin=average_score.min(), vmax=average_score.max())\n",
        "    plt.imshow(average_score, cmap='bone', aspect='auto', norm=norm)\n",
        "    plt.colorbar()\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel(\"Attention Heads\")\n",
        "    plt.ylabel(\"Transformer Layers\")\n",
        "    plt.xticks(ticks=np.arange(num_heads), labels=[f'H{h}' for h in range(num_heads)])\n",
        "    plt.yticks(ticks=np.arange(num_layers), labels=[f'L{l}' for l in range(num_layers)])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_three = list(zip(*np.unravel_index(np.argsort(average_score, axis=None)[3:], average_score.shape)))\n",
        "    for tuple_val in top_three:\n",
        "            layer, head = tuple_val\n",
        "            score = average_score[layer, head]\n",
        "            print(f\"Layer {layer}, Head {head} - Score: {score:.2f}\")\n",
        "    return average_score, top_three\n",
        "\n",
        "from matplotlib.colors import PowerNorm\n",
        "model_name = f\"Model = {model.config.architectures[0]}\\n\"\n",
        "visualize_full_model(sentences, model, tokenizer, punctuation_attention, title=\"Top Heads: Punctuation Attention Pattern [AVERAGE]\\n\"+model_name, bias_towards_best=0.7)"
      ],
      "metadata": {
        "id": "G90Rgk6fVKIZ"
      },
      "id": "G90Rgk6fVKIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANALYZE HEAD PATTERN ON ALL SENTENCES\n",
        "\n",
        "def visualize_highest_head(sentences, top_n, torch_model, torch_tokenizer, head_loc, pattern):\n",
        "    layer, head = head_loc\n",
        "    scores = []\n",
        "\n",
        "    punctuation_set = set('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "\n",
        "    for sentence in tqdm(sentences):\n",
        "        # if len(sentence.split()) < 10:\n",
        "        #   scores.append(100)\n",
        "        #   continue\n",
        "\n",
        "        punctuation_count = sum(1 for char in sentence if char in punctuation_set)\n",
        "        if punctuation_count <= 3:\n",
        "            scores.append(100)\n",
        "            continue\n",
        "\n",
        "        score = score_prediction(sentence, torch_model, torch_tokenizer, (layer, head), pattern, distance=\"jsd\")\n",
        "        scores.append(score)\n",
        "\n",
        "    indexed_scores = list(enumerate(scores))\n",
        "    sorted_scores = sorted(indexed_scores, key= lambda x: x[1])\n",
        "    top_scores = sorted_scores[:top_n]\n",
        "\n",
        "    for idx, score in top_scores:\n",
        "        print(f\"Sentence #{idx} Score: {score}\")\n",
        "        sentence = sentences[idx]\n",
        "        score_prediction(sentence, torch_model, torch_tokenizer, (layer, head), pattern, output=True)\n",
        "    return 0\n",
        "\n",
        "layer, head = 7, 8\n",
        "visualize_highest_head(sentences, 8, model, tokenizer, (layer, head), punctuation_attention)"
      ],
      "metadata": {
        "id": "mR56LNrRd37k"
      },
      "id": "mR56LNrRd37k",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}