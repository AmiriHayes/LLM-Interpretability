{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf67858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of generating prompts for an LLM to use for hypothesis program generation\n",
    "\n",
    "example_program_one = \"\"\"\n",
    "def dependencies(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
    "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
    "    len_seq = len(toks.input_ids[0]) /\n",
    "    out = np.zeros((len_seq, len_seq)) /\n",
    "    words = sentence.split() /\n",
    "    doc = nlp(\" \".join(words)) /\n",
    "    for stok in doc: /\n",
    "        parent_index = stok.i /\n",
    "        for child_stok in stok.children: /\n",
    "            child_index = child_stok.i /\n",
    "            out[parent_index+1, child_index+1] = 1 /\n",
    "            out[child_index+1, parent_index+1] = 1 /\n",
    "    out[0, 0] = 1 /\n",
    "    out[-1, 0] = 1 /\n",
    "    out += 1e-4 /\n",
    "    out = out / out.sum(axis=1, keepdims=True) /\n",
    "    return \"Dependency Parsing Pattern\", out /\n",
    "\"\"\"\n",
    "example_program_two = \"\"\"\n",
    "def same_attention(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]: /\n",
    "    toks = tokenizer([sentence], return_tensors=\"pt\")\n",
    "    len_seq = len(toks.input_ids[0])\n",
    "    out = np.zeros((len_seq, len_seq))\n",
    "    for i in range(1, len_seq-1):\n",
    "        out[i, i] = 1\n",
    "    out[0,0] = 1\n",
    "    out[-1,0] = 1\n",
    "    return \"Same Token Pattern\", out\n",
    "\"\"\"\n",
    "example_program_three = \"\"\"\n",
    "def pos_alignment(sentence: str, tokenizer: PreTrainedTokenizerBase) -> Tuple[str, np.ndarray]:\n",
    "    toks = tokenizer([sentence], return_tensors=\"pt\") /\n",
    "    len_seq = len(toks.input_ids[0]) /\n",
    "    out = np.zeros((len_seq, len_seq)) /\n",
    "    # assign toks, input_ids, word_ids, len_seq, out, doc /\n",
    "    # use spacey to get pos_tags for tokens in docs [token.pos_ for token in doc] /\n",
    "    # for token in pos_tags: /\n",
    "    # loop through pos_tags and increment out[i,j] when pos_tags match /\n",
    "    # assign cls (out[0, 0] = 1) and eos (out[-1, 0] = 1) to have self_attention /\n",
    "    # Normalize out matrix by row (results in uniform attention) and return out /\n",
    "    # return 'Part of Speech Implementation 1', out /\n",
    "\"\"\"\n",
    "\n",
    "def generate_prompt(sentences, model, tokenizer, head_loc, top_k_ratio=0.1):\n",
    "    layer, head = head_loc\n",
    "    data = {\n",
    "        \"layer\": layer,\n",
    "        \"head\": head,\n",
    "        \"model\": model.config.architectures[0],\n",
    "        \"examples\": []\n",
    "    }\n",
    "\n",
    "    def handle_score(score):\n",
    "        # convert to percentage with 0 decimal places\n",
    "        return \"{:.0f}\".format(score * 100)\n",
    "        \n",
    "    def scrape_head(att, tokens, top_k_ratio, ignore_special=True):\n",
    "        seq_len = att.shape[0]\n",
    "        ignore_indices = {i for i, tok in enumerate(tokens) if ignore_special and tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\")}\n",
    "        keep_indices = [i for i in range(seq_len) if i not in ignore_indices]\n",
    "        att_scores = []\n",
    "        for i in keep_indices:\n",
    "            for j in keep_indices:\n",
    "                att_scores.append((i, j, att[i, j]))\n",
    "        top_k = max(1, int(len(att_scores) * top_k_ratio))\n",
    "        top_att = sorted(att_scores, key=lambda x: x[2], reverse=True)[:top_k]\n",
    "        top_activations = []\n",
    "        for i, j, score in top_att:\n",
    "            top_activations.append(f\"[{str(tokens[i])}|{str(tokens[j])}:{handle_score(score)}]\")\n",
    "        top_activations_str = \" \".join(top_activations).replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "        return top_activations_str\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "            att = outputs.attentions[layer][0, head]\n",
    "        att = att.detach().cpu().numpy()\n",
    "        top_activations = scrape_head(att, tokens, top_k_ratio=top_k_ratio)\n",
    "        item = {f\"sentence {idx}\": \" \".join(tokens), \"sentence attention\": top_activations}\n",
    "        data[\"examples\"].append(item)\n",
    "\n",
    "    data = json.dumps(data, indent=2)\n",
    "    prompt = f\"\"\"\n",
    "    Using the following pieces of data based on {len(sentences)} sentences, generate three hypothesises about the linguistic role the following head is responsible for based on patterns\n",
    "    in the activations.  Then, choose the most fitting hypothesis for the head function using examples from the data. Finally, using the linguistic hypothesis you determine, \n",
    "    write a python function which takes in a sentence and tokenizer as parameters and outputs the name of the pattern you hypothesize along with a predicted_matrix (size: token_len * token_len), which is the \n",
    "    rule encoded matrix mirroring attention patterns you'd predict for any given sentence for Layer {layer}, Head {head}. Feel free to encode complex functions but write the simplest algorithm that captures your \n",
    "    observed pattern. You must respond to this prompt in JSON in the form \"{{\"hypothesis\": \"...\", \"program\": \"...\"}} with your chosen hypothesis. Think carefully before generating any code.\n",
    "    The first portion of your response has key \"hypothesis\" with the title of the hypothesis and the second portion of your response with key \"program\" should have valid python code starting with ```python and including imports. These patterns can be simple or \n",
    "    complex.  For uniformity, the first three lines of your function should be 'toks = tokenizer([sentence], return_tensors=\"pt\") len_seq = len(toks.input_ids[0]) out = np.zeros((len_seq, len_seq))'.\n",
    "    Make sure the token sequences from your tokenizer and spaCy (if you must use spaCy) are aligned via a dictionary if necessary, because they split text differently. Make sure you generalize your hypothesis pattern to any sentence. Functions can almost \n",
    "    always be expressed in fewer than 50 lines of code. As examples, it has been discovered one head is responsible for the complex task of dependency parsing. It's simplistic predicted pseudocode looks like: \n",
    "    {example_program_one}. Example 2: '''{example_program_two}''' Example 3: '''{example_program_three}'''. DATA: {data}\"\"\"\n",
    "    return ' '.join(prompt.strip().split())\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "layer, head = 5, 7\n",
    "prompt = generate_prompt(generic_sentences[:25], model, tokenizer, (layer, head), 0.025)\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
